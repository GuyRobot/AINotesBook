{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextStoriesGenerate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN75yaTrG492+WYfCzWjQ61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuyRobot/AINotesBook/blob/main/TextStoriesGenerate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19w7NLMYsCeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731f00cc-751b-423d-acb6-e2c627aa199d"
      },
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN0_Vk_GsFHe"
      },
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import collections\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import time\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpZX_h15s8hd",
        "outputId": "e9c0b7c3-117c-4d6d-dd7a-af98630015b8"
      },
      "source": [
        "\n",
        "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
        "\n",
        "dir_name = 'stories'\n",
        "if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)\n",
        "\n",
        "def download_stories(filename):\n",
        "    print(\"Downloading file: \", dir_name + os.sep + filename)\n",
        "    if not os.path.exists(os.path.join(dir_name, filename)):\n",
        "        filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
        "    else:\n",
        "        print(\"File %s already exits\" % filename)\n",
        "    return filename\n",
        "\n",
        "\n",
        "filenames = [format(i, '03d') + '.txt' for i in range(1, 101)]\n",
        "\n",
        "for fn in filenames:\n",
        "    download_stories(fn)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading file:  stories/001.txt\n",
            "Downloading file:  stories/002.txt\n",
            "Downloading file:  stories/003.txt\n",
            "Downloading file:  stories/004.txt\n",
            "Downloading file:  stories/005.txt\n",
            "Downloading file:  stories/006.txt\n",
            "Downloading file:  stories/007.txt\n",
            "Downloading file:  stories/008.txt\n",
            "Downloading file:  stories/009.txt\n",
            "Downloading file:  stories/010.txt\n",
            "Downloading file:  stories/011.txt\n",
            "Downloading file:  stories/012.txt\n",
            "Downloading file:  stories/013.txt\n",
            "Downloading file:  stories/014.txt\n",
            "Downloading file:  stories/015.txt\n",
            "Downloading file:  stories/016.txt\n",
            "Downloading file:  stories/017.txt\n",
            "Downloading file:  stories/018.txt\n",
            "Downloading file:  stories/019.txt\n",
            "Downloading file:  stories/020.txt\n",
            "Downloading file:  stories/021.txt\n",
            "Downloading file:  stories/022.txt\n",
            "Downloading file:  stories/023.txt\n",
            "Downloading file:  stories/024.txt\n",
            "Downloading file:  stories/025.txt\n",
            "Downloading file:  stories/026.txt\n",
            "Downloading file:  stories/027.txt\n",
            "Downloading file:  stories/028.txt\n",
            "Downloading file:  stories/029.txt\n",
            "Downloading file:  stories/030.txt\n",
            "Downloading file:  stories/031.txt\n",
            "Downloading file:  stories/032.txt\n",
            "Downloading file:  stories/033.txt\n",
            "Downloading file:  stories/034.txt\n",
            "Downloading file:  stories/035.txt\n",
            "Downloading file:  stories/036.txt\n",
            "Downloading file:  stories/037.txt\n",
            "Downloading file:  stories/038.txt\n",
            "Downloading file:  stories/039.txt\n",
            "Downloading file:  stories/040.txt\n",
            "Downloading file:  stories/041.txt\n",
            "Downloading file:  stories/042.txt\n",
            "Downloading file:  stories/043.txt\n",
            "Downloading file:  stories/044.txt\n",
            "Downloading file:  stories/045.txt\n",
            "Downloading file:  stories/046.txt\n",
            "Downloading file:  stories/047.txt\n",
            "Downloading file:  stories/048.txt\n",
            "Downloading file:  stories/049.txt\n",
            "Downloading file:  stories/050.txt\n",
            "Downloading file:  stories/051.txt\n",
            "Downloading file:  stories/052.txt\n",
            "Downloading file:  stories/053.txt\n",
            "Downloading file:  stories/054.txt\n",
            "Downloading file:  stories/055.txt\n",
            "Downloading file:  stories/056.txt\n",
            "Downloading file:  stories/057.txt\n",
            "Downloading file:  stories/058.txt\n",
            "Downloading file:  stories/059.txt\n",
            "Downloading file:  stories/060.txt\n",
            "Downloading file:  stories/061.txt\n",
            "Downloading file:  stories/062.txt\n",
            "Downloading file:  stories/063.txt\n",
            "Downloading file:  stories/064.txt\n",
            "Downloading file:  stories/065.txt\n",
            "Downloading file:  stories/066.txt\n",
            "Downloading file:  stories/067.txt\n",
            "Downloading file:  stories/068.txt\n",
            "Downloading file:  stories/069.txt\n",
            "Downloading file:  stories/070.txt\n",
            "Downloading file:  stories/071.txt\n",
            "Downloading file:  stories/072.txt\n",
            "Downloading file:  stories/073.txt\n",
            "Downloading file:  stories/074.txt\n",
            "Downloading file:  stories/075.txt\n",
            "Downloading file:  stories/076.txt\n",
            "Downloading file:  stories/077.txt\n",
            "Downloading file:  stories/078.txt\n",
            "Downloading file:  stories/079.txt\n",
            "Downloading file:  stories/080.txt\n",
            "Downloading file:  stories/081.txt\n",
            "Downloading file:  stories/082.txt\n",
            "Downloading file:  stories/083.txt\n",
            "Downloading file:  stories/084.txt\n",
            "Downloading file:  stories/085.txt\n",
            "Downloading file:  stories/086.txt\n",
            "Downloading file:  stories/087.txt\n",
            "Downloading file:  stories/088.txt\n",
            "Downloading file:  stories/089.txt\n",
            "Downloading file:  stories/090.txt\n",
            "Downloading file:  stories/091.txt\n",
            "Downloading file:  stories/092.txt\n",
            "Downloading file:  stories/093.txt\n",
            "Downloading file:  stories/094.txt\n",
            "Downloading file:  stories/095.txt\n",
            "Downloading file:  stories/096.txt\n",
            "Downloading file:  stories/097.txt\n",
            "Downloading file:  stories/098.txt\n",
            "Downloading file:  stories/099.txt\n",
            "Downloading file:  stories/100.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-r2S6tIsF0K",
        "outputId": "4fcc0d7c-3810-4e01-919e-478f0c12e3cf"
      },
      "source": [
        "\n",
        "filenames = [format(i, '03d') + '.txt' for i in range(1, 101)]\n",
        "dir_name = 'stories'\n",
        "\n",
        "def read_data(filename):\n",
        "    with open(filename) as f:\n",
        "        data =  tf.compat.as_str(f.read())\n",
        "        data = data.lower()\n",
        "        data = list(data)\n",
        "    return data\n",
        "\n",
        "global documents\n",
        "documents = []\n",
        "num_files = 100\n",
        "for i in range(num_files):\n",
        "    print(\"processing file %s\" % os.path.join(dir_name, filenames[i]))\n",
        "    chars = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    # break into bigrams\n",
        "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
        "    # Create document\n",
        "    documents.append(two_grams)\n",
        "    print(\"Data size (chars) (document %d) %d\" % (i, len(two_grams)))\n",
        "    print(\"Sample string %s\\n\" % (two_grams[:50]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing file stories/001.txt\n",
            "Data size (chars) (document 0) 3667\n",
            "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' w', 'he', 'n ', 'wi', 'sh', 'in', 'g ', 'st', 'il', 'l ', 'he', 'lp', 'ed', ' o', 'ne', ', ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', '\\nw', 'ho', 'se', ' d', 'au', 'gh', 'te', 'rs', ' w', 'er', 'e ', 'al', 'l ', 'be', 'au', 'ti', 'fu', 'l,']\n",
            "\n",
            "processing file stories/002.txt\n",
            "Data size (chars) (document 1) 4928\n",
            "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ', ', 'wh', 'o ', 'ha', 'd ', 'an', '\\no', 'nl', 'y ', 'ch', 'il', 'd,', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' t', 'hr', 'ee']\n",
            "\n",
            "processing file stories/003.txt\n",
            "Data size (chars) (document 2) 9745\n",
            "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'fa', 'th', 'er', ' h', 'ad', ' t', 'wo', ' s', 'on', 's,', ' t', 'he', ' e', 'ld', 'er', ' o', 'f ', 'wh', 'om', ' w', 'as', ' s', 'ma', 'rt', ' a', 'nd', '\\ns', 'en', 'si', 'bl', 'e,', ' a', 'nd', ' c', 'ou', 'ld', ' d', 'o ', 'ev', 'er', 'yt', 'hi', 'ng', ', ', 'bu']\n",
            "\n",
            "processing file stories/004.txt\n",
            "Data size (chars) (document 3) 2852\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'go', 'at', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' l', 'it', 'tl', 'e ', 'ki', 'ds', ', ', 'an', 'd\\n', 'lo', 've', 'd ', 'th', 'em', ' w', 'it', 'h ', 'al', 'l ', 'th', 'e ', 'lo', 've', ' o']\n",
            "\n",
            "processing file stories/005.txt\n",
            "Data size (chars) (document 4) 8189\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' i', 'll', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' t', 'o\\n', 'hi', 'ms', 'el', 'f ', \"'i\", ' a', 'm ', 'ly', 'in', 'g ', 'on', ' w', 'ha', 't ', 'mu', 'st', ' b']\n",
            "\n",
            "processing file stories/006.txt\n",
            "Data size (chars) (document 5) 4369\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'ea', 'sa', 'nt', ' w', 'ho', ' h', 'ad', ' d', 'ri', 've', 'n ', 'hi', 's ', 'co', 'w ', 'to', ' t', 'he', ' f', 'ai', 'r,', ' a', 'nd', ' s', 'ol', 'd\\n', 'he', 'r ', 'fo', 'r ', 'se', 've', 'n ', 'ta', 'le', 'rs', '. ', ' o', 'n ', 'th', 'e ']\n",
            "\n",
            "processing file stories/007.txt\n",
            "Data size (chars) (document 6) 5216\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' a', ' q', 'ue', 'en', ' w', 'ho', ' l', 'iv', 'ed', '\\nh', 'ap', 'pi', 'ly', ' t', 'og', 'et', 'he', 'r ', 'an', 'd ', 'ha', 'd ', 'tw', 'el', 've', ' c', 'hi', 'ld', 're', 'n,', ' b']\n",
            "\n",
            "processing file stories/008.txt\n",
            "Data size (chars) (document 7) 6097\n",
            "Sample string ['li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' t', 'oo', 'k ', 'hi', 's ', 'li', 'tt', 'le', ' s', 'is', 'te', 'r ', 'by', ' t', 'he', ' h', 'an', 'd ', 'an', 'd ', 'sa', 'id', ', ', 'si', 'nc', 'e\\n', 'ou', 'r ', 'mo', 'th', 'er', ' d', 'ie', 'd ', 'we', ' h', 'av', 'e ', 'ha', 'd ', 'no', ' h', 'ap']\n",
            "\n",
            "processing file stories/009.txt\n",
            "Data size (chars) (document 8) 3699\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'lo', 'ng', ' i', 'n ', 'va', 'in', '\\nw', 'is', 'he', 'd ', 'fo', 'r ', 'a ', 'ch', 'il', 'd.', '  ', 'at', ' l', 'en', 'gt', 'h ', 'th', 'e ', 'wo', 'ma', 'n ', 'ho', 'pe']\n",
            "\n",
            "processing file stories/010.txt\n",
            "Data size (chars) (document 9) 5268\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', 'se', ' w', 'if', 'e ', 'di', 'ed', ', ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd\\n', 'di', 'ed', ', ', 'an', 'd ', 'th', 'e ', 'ma', 'n ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ', ', 'an']\n",
            "\n",
            "processing file stories/011.txt\n",
            "Data size (chars) (document 10) 2377\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'id', 'le', ' a', 'nd', ' w', 'ou', 'ld', ' n', 'ot', ' s', 'pi', 'n,', ' a', 'nd', '\\nl', 'et', ' h', 'er', ' m', 'ot', 'he', 'r ', 'sa', 'y ', 'wh', 'at', ' s', 'he', ' w', 'ou', 'ld', ', ', 'sh', 'e ', 'co']\n",
            "\n",
            "processing file stories/012.txt\n",
            "Data size (chars) (document 11) 7695\n",
            "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'wo', 'od', '-c', 'ut', 'te', 'r ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'an', 'd ', 'hi', 's ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' b', 'oy', ' w', 'as', ' c', 'al']\n",
            "\n",
            "processing file stories/013.txt\n",
            "Data size (chars) (document 12) 3665\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'ma', 'n,', ' w', 'ho', ' c', 'ou', 'ld', ' n', 'o ', 'lo', 'ng', 'er', '\\ns', 'up', 'po', 'rt', ' h', 'is', ' o', 'nl', 'y ', 'so', 'n.', '  ', 'th', 'en', ' s', 'ai', 'd ', 'th', 'e ', 'so', 'n,', ' d']\n",
            "\n",
            "processing file stories/014.txt\n",
            "Data size (chars) (document 13) 4178\n",
            "Sample string ['a ', 'lo', 'ng', ' t', 'im', 'e ', 'ag', 'o ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' f', 'am', 'ed', ' f', 'or', ' h', 'is', ' w', 'is', 'do', 'm\\n', 'th', 'ro', 'ug', 'h ', 'al', 'l ', 'th', 'e ', 'la', 'nd', '. ', ' n', 'ot', 'hi', 'ng', ' w', 'as', ' h']\n",
            "\n",
            "processing file stories/015.txt\n",
            "Data size (chars) (document 14) 8674\n",
            "Sample string ['on', 'e ', 'su', 'mm', 'er', \"'s\", ' m', 'or', 'ni', 'ng', ' a', ' l', 'it', 'tl', 'e ', 'ta', 'il', 'or', ' w', 'as', ' s', 'it', 'ti', 'ng', ' o', 'n ', 'hi', 's ', 'ta', 'bl', 'e\\n', 'by', ' t', 'he', ' w', 'in', 'do', 'w,', ' h', 'e ', 'wa', 's ', 'in', ' g', 'oo', 'd ', 'sp', 'ir', 'it', 's,']\n",
            "\n",
            "processing file stories/016.txt\n",
            "Data size (chars) (document 15) 7018\n",
            "Sample string ['\\tc', 'in', 'de', 're', 'll', 'a\\n', 'th', 'e ', 'wi', 'fe', ' o', 'f ', 'a ', 'ri', 'ch', ' m', 'an', ' f', 'el', 'l ', 'si', 'ck', ', ', 'an', 'd ', 'as', ' s', 'he', ' f', 'el', 't ', 'th', 'at', ' h', 'er', ' e', 'nd', '\\nw', 'as', ' d', 'ra', 'wi', 'ng', ' n', 'ea', 'r,', ' s', 'he', ' c', 'al']\n",
            "\n",
            "processing file stories/017.txt\n",
            "Data size (chars) (document 16) 3039\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'se', 'iz', 'ed', ' w', 'it', 'h ', 'a ', 'de', 'si', 're', ' t', 'o ', 'tr', 'av', 'el', '\\na', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'to', 'ok', ' n', 'o ', 'on', 'e ']\n",
            "\n",
            "processing file stories/018.txt\n",
            "Data size (chars) (document 17) 3020\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'id', 'ow', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' d', 'au', 'gh', 'te', 'rs', ' -', ' o', 'ne', ' o', 'f\\n', 'wh', 'om', ' w', 'as', ' p', 're', 'tt', 'y ', 'an', 'd ', 'in', 'du', 'st', 'ri', 'ou', 's,', ' w', 'hi', 'ls', 't ', 'th', 'e ', 'ot']\n",
            "\n",
            "processing file stories/019.txt\n",
            "Data size (chars) (document 18) 2465\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' s', 'on', 's,', ' a', 'nd', ' s', 'ti', 'll', ' h', 'e ', 'ha', 'd\\n', 'no', ' d', 'au', 'gh', 'te', 'r,', ' h', 'ow', 'ev', 'er', ' m', 'uc', 'h ', 'he', ' w', 'is', 'he', 'd ', 'fo', 'r ', 'on']\n",
            "\n",
            "processing file stories/020.txt\n",
            "Data size (chars) (document 19) 3703\n",
            "Sample string ['\\tl', 'it', 'tl', 'e ', 're', 'd-', 'ca', 'p\\n', '\\no', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'de', 'ar', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' l', 'ov', 'ed', '\\nb', 'y ', 'ev', 'er', 'y ', 'on', 'e ', 'wh', 'o ', 'lo', 'ok', 'ed']\n",
            "\n",
            "processing file stories/021.txt\n",
            "Data size (chars) (document 20) 1924\n",
            "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' c', 'ou', 'nt', 'ry', ' t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'gr', 'ea', 't ', 'la', 'me', 'nt', 'at', 'io', 'n ', 'ov', 'er', ' a', '\\nw', 'il', 'd ', 'bo', 'ar', ' t', 'ha', 't ', 'la', 'id', ' w', 'as', 'te', ' t', 'he', ' f', 'ar', 'me', \"r'\", 's ']\n",
            "\n",
            "processing file stories/022.txt\n",
            "Data size (chars) (document 21) 6561\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ga', 've', ' b', 'ir', 'th', ' t', 'o ', 'a ', 'li', 'tt', 'le', ' s', 'on', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'ca', 'me', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'wi', 'th', ' a', ' c', 'au']\n",
            "\n",
            "processing file stories/023.txt\n",
            "Data size (chars) (document 22) 5956\n",
            "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'mi', 'll', 'er', ' h', 'ad', ' l', 'it', 'tl', 'e ', 'by', ' l', 'it', 'tl', 'e ', 'fa', 'll', 'en', ' i', 'nt', 'o ', 'po', 've', 'rt', 'y,', ' a', 'nd', '\\nh', 'ad', ' n', 'ot', 'hi', 'ng', ' l', 'ef', 't ', 'bu', 't ', 'hi', 's ', 'mi', 'll', ' a', 'nd', ' a', ' l']\n",
            "\n",
            "processing file stories/024.txt\n",
            "Data size (chars) (document 23) 2529\n",
            "Sample string ['th', 'e ', 'mo', 'th', 'er', ' o', 'f ', 'ha', 'ns', ' s', 'ai', 'd,', ' w', 'hi', 'th', 'er', ' a', 'wa', 'y,', ' h', 'an', 's.', '  ', 'ha', 'ns', ' a', 'ns', 'we', 're', 'd,', ' t', 'o\\n', 'gr', 'et', 'el', '. ', ' b', 'eh', 'av', 'e ', 'we', 'll', ', ', 'ha', 'ns', '. ', ' o', 'h,', ' i', \"'l\"]\n",
            "\n",
            "processing file stories/025.txt\n",
            "Data size (chars) (document 24) 2416\n",
            "Sample string ['an', ' a', 'ge', 'd ', 'co', 'un', 't ', 'on', 'ce', ' l', 'iv', 'ed', ' i', 'n ', 'sw', 'it', 'ze', 'rl', 'an', 'd,', ' w', 'ho', ' h', 'ad', ' a', 'n ', 'on', 'ly', ' s', 'on', ',\\n', 'bu', 't ', 'he', ' w', 'as', ' s', 'tu', 'pi', 'd,', ' a', 'nd', ' c', 'ou', 'ld', ' l', 'ea', 'rn', ' n', 'ot']\n",
            "\n",
            "processing file stories/026.txt\n",
            "Data size (chars) (document 25) 3369\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'ca', 'll', 'ed', ' c', 'le', 've', 'r\\n', 'el', 'si', 'e.', '  ', 'an', 'd ', 'wh', 'en', ' s', 'he', ' h', 'ad', ' g', 'ro', 'wn', ' u', 'p ', 'he', 'r ']\n",
            "\n",
            "processing file stories/027.txt\n",
            "Data size (chars) (document 26) 10013\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' t', 'ai', 'lo', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'an', 'd\\n', 'on', 'ly', ' o', 'ne', ' g', 'oa', 't.', '  ', 'bu', 't ', 'as', ' t', 'he', ' g', 'oa', 't ', 'su', 'pp', 'or', 'te']\n",
            "\n",
            "processing file stories/028.txt\n",
            "Data size (chars) (document 27) 5788\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'sa', 't ', 'in', ' t', 'he', ' e', 've', 'ni', 'ng', ' b', 'y ', 'th', 'e\\n', 'he', 'ar', 'th', ' a', 'nd', ' p', 'ok', 'ed', ' t', 'he', ' f', 'ir', 'e,', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ']\n",
            "\n",
            "processing file stories/029.txt\n",
            "Data size (chars) (document 28) 1335\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'se', 'rv', 'an', 't-', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' i', 'nd', 'us', 'tr', 'io', 'us', ' a', 'nd', ' c', 'le', 'an', 'ly', '\\na', 'nd', ' s', 'we', 'pt', ' t', 'he', ' h', 'ou', 'se', ' e', 've', 'ry', ' d', 'ay', ', ', 'an']\n",
            "\n",
            "processing file stories/030.txt\n",
            "Data size (chars) (document 29) 3591\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r,', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', '\\nd', 'au', 'gh', 'te', 'r,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 's ', 'gr', 'ow', 'n ', 'up', ', ', 'he', ' w', 'is', 'he']\n",
            "\n",
            "processing file stories/031.txt\n",
            "Data size (chars) (document 30) 1624\n",
            "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' s', 'o ', 'ma', 'ny', ' c', 'hi', 'ld', 're', 'n ', 'th', 'at', ' h', 'e ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'as', 'ke', 'd\\n', 'ev', 'er', 'yo', 'ne', ' i', 'n ', 'th', 'e ', 'wo', 'rl', 'd ', 'to', ' b', 'e ', 'go', 'df', 'at', 'he', 'r,', ' a', 'nd']\n",
            "\n",
            "processing file stories/032.txt\n",
            "Data size (chars) (document 31) 758\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' o', 'bs', 'ti', 'na', 'te', ' a', 'nd', ' i', 'nq', 'ui', 'si', 'ti', 've', ',\\n', 'an', 'd ', 'wh', 'en', ' h', 'er', ' p', 'ar', 'en', 'ts', ' t', 'ol', 'd ', 'he', 'r ', 'to', ' d', 'o ']\n",
            "\n",
            "processing file stories/033.txt\n",
            "Data size (chars) (document 32) 3121\n",
            "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' t', 'we', 'lv', 'e ', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' f', 'or', 'ce', 'd ', 'to', ' w', 'or', 'k ', 'ni', 'gh', 't ', 'an', 'd\\n', 'da', 'y ', 'to', ' g', 'iv', 'e ', 'th', 'em', ' e', 've', 'n ', 'br', 'ea', 'd.', '  ', 'wh', 'en', ' t']\n",
            "\n",
            "processing file stories/034.txt\n",
            "Data size (chars) (document 33) 4192\n",
            "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' h', 'ad', ' a', ' s', 'on', ', ', 'wh', 'o ', 'ha', 'pp', 'en', 'ed', ' t', 'o ', 'be', ' s', 'ma', 'll', ', ', 'an', 'd\\n', 'no', ' b', 'ig', 'ge', 'r ', 'th', 'an', ' a', ' t', 'hu', 'mb', ', ', 'an', 'd ', 'on', ' t', 'hi', 's ', 'ac', 'co', 'un']\n",
            "\n",
            "processing file stories/035.txt\n",
            "Data size (chars) (document 34) 3650\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'iz', 'ar', 'd ', 'wh', 'o ', 'us', 'ed', ' t', 'o ', 'ta', 'ke', ' t', 'he', ' f', 'or', 'm ', 'of', ' a', ' p', 'oo', 'r\\n', 'ma', 'n,', ' a', 'nd', ' w', 'en', 't ', 'to', ' h', 'ou', 'se', 's ', 'an', 'd ', 'be', 'gg', 'ed', ', ', 'an', 'd ']\n",
            "\n",
            "processing file stories/036.txt\n",
            "Data size (chars) (document 35) 8219\n",
            "Sample string ['it', ' i', 's ', 'no', 'w ', 'lo', 'ng', ' a', 'go', ', ', 'qu', 'it', 'e ', 'tw', 'o ', 'th', 'ou', 'sa', 'nd', ' y', 'ea', 'rs', ', ', 'si', 'nc', 'e ', 'th', 'er', 'e ', 'wa', 's\\n', 'a ', 'ri', 'ch', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', ' a', 'nd', ' p', 'io']\n",
            "\n",
            "processing file stories/037.txt\n",
            "Data size (chars) (document 36) 2151\n",
            "Sample string ['a ', 'fa', 'rm', 'er', ' o', 'nc', 'e ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' d', 'og', ' c', 'al', 'le', 'd ', 'su', 'lt', 'an', ', ', 'wh', 'o ', 'ha', 'd ', 'gr', 'ow', 'n\\n', 'ol', 'd,', ' a', 'nd', ' l', 'os', 't ', 'al', 'l ', 'hi', 's ', 'te', 'et', 'h,', ' s', 'o ', 'th', 'at', ' h']\n",
            "\n",
            "processing file stories/038.txt\n",
            "Data size (chars) (document 37) 5129\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ', ', 'a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' w', 'as', ' h', 'un', 'ti', 'ng', ' i', 'n ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'he', ' c', 'ha', 'se', 'd ', 'a ', 'wi', 'ld', ' b', 'ea', 'st', ' s', 'o ', 'ea', 'ge', 'rl']\n",
            "\n",
            "processing file stories/039.txt\n",
            "Data size (chars) (document 38) 3472\n",
            "Sample string ['\\tb', 'ri', 'ar', '-r', 'os', 'e\\n', '\\na', ' l', 'on', 'g ', 'ti', 'me', ' a', 'go', ' t', 'he', 're', ' w', 'er', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' q', 'ue', 'en', ' w', 'ho', ' s', 'ai', 'd ', 'ev', 'er', 'y\\n', 'da', 'y,', ' a', 'h,', ' i', 'f ', 'on', 'ly', ' w', 'e ', 'ha', 'd ', 'a ', 'ch']\n",
            "\n",
            "processing file stories/040.txt\n",
            "Data size (chars) (document 39) 2490\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' f', 'or', 'es', 'te', 'r ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'hu', 'nt', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'en', 'te', 're', 'd ', 'it', ' h', 'e ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'of']\n",
            "\n",
            "processing file stories/041.txt\n",
            "Data size (chars) (document 40) 4273\n",
            "Sample string ['a ', 'ki', 'ng', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'be', 'au', 'ti', 'fu', 'l ', 'be', 'yo', 'nd', ' a', 'll', ' m', 'ea', 'su', 're', ',\\n', 'bu', 't ', 'so', ' p', 'ro', 'ud', ' a', 'nd', ' h', 'au', 'gh', 'ty', ' w', 'it', 'ha', 'l ', 'th', 'at', ' n', 'o ']\n",
            "\n",
            "processing file stories/042.txt\n",
            "Data size (chars) (document 41) 8327\n",
            "Sample string ['\\ts', 'no', 'w ', 'wh', 'it', 'e ', 'an', 'd ', 'th', 'e ', 'se', 've', 'n ', 'dw', 'ar', 'fs', '\\n\\n', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' i', 'n ', 'th', 'e ', 'mi', 'dd', 'le', ' o', 'f ', 'wi', 'nt', 'er', ', ', 'wh', 'en', ' t', 'he', ' f', 'la', 'ke', 's ', 'of', '\\ns', 'no', 'w ']\n",
            "\n",
            "processing file stories/043.txt\n",
            "Data size (chars) (document 42) 6128\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' h', 'ad', ' f', 'al', 'le', 'n ', 'de', 'ep', 'er', ' a', 'nd', ' d', 'ee', 'pe', 'r ', 'in', 'to', '\\np', 'ov', 'er', 'ty', ', ', 'an', 'd ', 'at', ' l', 'as', 't ', 'th', 'ei', 'r ', 'ne', 'ed']\n",
            "\n",
            "processing file stories/044.txt\n",
            "Data size (chars) (document 43) 2819\n",
            "Sample string ['\\tr', 'um', 'pe', 'ls', 'ti', 'lt', 'sk', 'in', '\\n\\n', 'on', 'ce', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'wa', 's ', 'po', 'or', ', ', 'bu', 't ', 'wh', 'o ', 'ha', 'd ', 'a ', 'be', 'au', 'ti', 'fu', 'l\\n', 'da', 'ug', 'ht', 'er', '. ', ' n', 'ow', ' i', 't ', 'ha']\n",
            "\n",
            "processing file stories/045.txt\n",
            "Data size (chars) (document 44) 3822\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' w', 'om', 'an', ' w', 'ho', ' w', 'as', ' a', ' r', 'ea', 'l ', 'wi', 'tc', 'h ', 'an', 'd ', 'ha', 'd ', 'tw', 'o\\n', 'da', 'ug', 'ht', 'er', 's,', ' o', 'ne', ' u', 'gl', 'y ', 'an', 'd ', 'wi', 'ck', 'ed', ', ']\n",
            "\n",
            "processing file stories/046.txt\n",
            "Data size (chars) (document 45) 7772\n",
            "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' w', 'as', ' a', ' k', 'in', 'g,', ' w', 'ho', ' h', 'ad', ' b', 'eh', 'in', 'd ', 'hi', 's ', 'pa', 'la', 'ce', ' a', '\\nb', 'ea', 'ut', 'if', 'ul', ' p', 'le', 'as', 'ur', 'e-', 'ga', 'rd', 'en', ' i', 'n ', 'wh', 'ic', 'h ', 'th', 'er']\n",
            "\n",
            "processing file stories/047.txt\n",
            "Data size (chars) (document 46) 22158\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'on', 'e ', 'ri', 'ch', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', '\\np', 'oo', 'r.', '  ', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ' w', 'as', ' a', ' g', 'ol', 'ds', 'mi', 'th']\n",
            "\n",
            "processing file stories/048.txt\n",
            "Data size (chars) (document 47) 2169\n",
            "Sample string ['tw', 'o ', 'ki', 'ng', \"s'\", ' s', 'on', 's ', 'on', 'ce', ' w', 'en', 't ', 'ou', 't ', 'in', ' s', 'ea', 'rc', 'h ', 'of', ' a', 'dv', 'en', 'tu', 're', 's,', ' a', 'nd', ' f', 'el', 'l ', 'in', 'to', '\\na', ' w', 'il', 'd,', ' d', 'is', 'or', 'de', 'rl', 'y ', 'wa', 'y ', 'of', ' l', 'iv', 'in']\n",
            "\n",
            "processing file stories/049.txt\n",
            "Data size (chars) (document 48) 2822\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'of', ' w', 'ho', 'm ', 'tw', 'o\\n', 'we', 're', ' c', 'le', 've', 'r ', 'an', 'd ', 'wi', 'se', ', ', 'bu', 't ', 'th', 'e ', 'th', 'ir']\n",
            "\n",
            "processing file stories/050.txt\n",
            "Data size (chars) (document 49) 4034\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'th', 'e ', 'yo', 'un', 'ge', 'st', ' o', 'f ', 'wh', 'om', ' w', 'as', ' c', 'al', 'le', 'd\\n', 'du', 'mm', 'li', 'ng', ', ', 'an', 'd ', 'wa', 's ', 'de', 'sp', 'is', 'ed', ', ', 'mo', 'ck']\n",
            "\n",
            "processing file stories/051.txt\n",
            "Data size (chars) (document 50) 5608\n",
            "Sample string ['\\ta', 'll', 'er', 'le', 'ir', 'au', 'h\\n', '\\nt', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' w', 'if', 'e ', 'wi', 'th', ' g', 'ol', 'de', 'n ', 'ha', 'ir', ',\\n', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' b', 'ea']\n",
            "\n",
            "processing file stories/052.txt\n",
            "Data size (chars) (document 51) 1287\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' a', 'nd', ' h', 'er', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', '\\np', 're', 'tt', 'y ', 'ga', 'rd', 'en', ' w', 'it', 'h ', 'ca', 'bb', 'ag', 'es', '. ', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'ha']\n",
            "\n",
            "processing file stories/053.txt\n",
            "Data size (chars) (document 52) 2841\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'br', 'id', 'e ', 'wh', 'om', ' h', 'e ', 'lo', 've', 'd ', 've', 'ry', ' m', 'uc', 'h.', '\\na', 'nd', ' w', 'he', 'n ', 'he', ' w', 'as', ' s', 'it', 'ti', 'ng', ' b', 'es', 'id', 'e ']\n",
            "\n",
            "processing file stories/054.txt\n",
            "Data size (chars) (document 53) 1922\n",
            "Sample string ['ha', 'ns', ' w', 'is', 'he', 'd ', 'to', ' p', 'ut', ' h', 'is', ' s', 'on', ' t', 'o ', 'le', 'ar', 'n ', 'a ', 'tr', 'ad', 'e,', ' s', 'o ', 'he', ' w', 'en', 't ', 'in', 'to', ' t', 'he', '\\nc', 'hu', 'rc', 'h ', 'an', 'd ', 'pr', 'ay', 'ed', ' t', 'o ', 'ou', 'r ', 'lo', 'rd', ' g', 'od', ' t']\n",
            "\n",
            "processing file stories/055.txt\n",
            "Data size (chars) (document 54) 2573\n",
            "Sample string ['a ', 'fa', 'th', 'er', ' o', 'nc', 'e ', 'ca', 'll', 'ed', ' h', 'is', ' t', 'hr', 'ee', ' s', 'on', 's ', 'be', 'fo', 're', ' h', 'im', ', ', 'an', 'd ', 'he', ' g', 'av', 'e ', 'to', ' t', 'he', '\\nf', 'ir', 'st', ' a', ' c', 'oc', 'k,', ' t', 'o ', 'th', 'e ', 'se', 'co', 'nd', ' a', ' s', 'cy']\n",
            "\n",
            "processing file stories/056.txt\n",
            "Data size (chars) (document 55) 5285\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' u', 'nd', 'er', 'st', 'oo', 'd ', 'al', 'l ', 'ki', 'nd', 's ', 'of', ' a', 'rt', 's.', '  ', 'he', ' s', 'er', 've', 'd ', 'in', '\\nw', 'ar', ', ', 'an', 'd ', 'be', 'ha', 've', 'd ', 'we', 'll', ' a', 'nd', ' b', 'ra', 've']\n",
            "\n",
            "processing file stories/057.txt\n",
            "Data size (chars) (document 56) 971\n",
            "Sample string ['th', 'e ', 'sh', 'e-', 'wo', 'lf', ' b', 'ro', 'ug', 'ht', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'a ', 'yo', 'un', 'g ', 'on', 'e,', ' a', 'nd', ' i', 'nv', 'it', 'ed', ' t', 'he', ' f', 'ox', '\\nt', 'o ', 'be', ' g', 'od', 'fa', 'th', 'er', '. ', ' a', 'ft', 'er', ' a', 'll', ', ', 'he']\n",
            "\n",
            "processing file stories/058.txt\n",
            "Data size (chars) (document 57) 4538\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' t', 'o ', 'wh', 'om', ' g', 'od', ' h', 'ad', ' g', 'iv', 'en', ' n', 'o ', 'ch', 'il', 'dr', 'en', '.\\n', 'ev', 'er', 'y ', 'mo', 'rn', 'in', 'g ', 'sh', 'e ', 'we', 'nt', ' i', 'nt', 'o ', 'th']\n",
            "\n",
            "processing file stories/059.txt\n",
            "Data size (chars) (document 58) 636\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'ma', 'n,', ' w', 'ho', 'se', ' e', 'ye', 's ', 'ha', 'd ', 'be', 'co', 'me', ' d', 'im', ', ', 'hi', 's ', 'ea', 'rs', '\\nd', 'ul', 'l ', 'of', ' h', 'ea', 'ri', 'ng', ', ', 'hi', 's ', 'kn', 'ee', 's ', 'tr', 'em', 'bl']\n",
            "\n",
            "processing file stories/060.txt\n",
            "Data size (chars) (document 59) 786\n",
            "Sample string ['a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' s', 'is', 'te', 'r ', 'we', 're', ' o', 'nc', 'e ', 'pl', 'ay', 'in', 'g ', 'by', ' a', ' w', 'el', 'l,', ' a', 'nd', ' w', 'hi', 'le', '\\nt', 'he', 'y ', 'we', 're', ' t', 'hu', 's ', 'pl', 'ay', 'in', 'g,', ' t', 'he', 'y ', 'bo', 'th']\n",
            "\n",
            "processing file stories/061.txt\n",
            "Data size (chars) (document 60) 10687\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' w', 'he', 'n ', 'it', ' c', 'am', 'e ', 'to', ' a', 'n ', 'en', 'd,', '\\nm', 'an', 'y ', 'so', 'ld', 'ie', 'rs', ' w', 'er', 'e ', 'di', 'sc', 'ha', 'rg', 'ed', '. ', ' t']\n",
            "\n",
            "processing file stories/062.txt\n",
            "Data size (chars) (document 61) 5105\n",
            "Sample string ['ha', 'ns', ' h', 'ad', ' s', 'er', 've', 'd ', 'hi', 's ', 'ma', 'st', 'er', ' f', 'or', ' s', 'ev', 'en', ' y', 'ea', 'rs', ', ', 'so', ' h', 'e ', 'sa', 'id', ' t', 'o ', 'hi', 'm,', '\\nm', 'as', 'te', 'r,', ' m', 'y ', 'ti', 'me', ' i', 's ', 'up', ', ', 'no', 'w ', 'i ', 'sh', 'ou', 'ld', ' b']\n",
            "\n",
            "processing file stories/063.txt\n",
            "Data size (chars) (document 62) 1127\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' y', 'ou', 'ng', ' p', 'ea', 'sa', 'nt', ' n', 'am', 'ed', ' h', 'an', 's,', ' w', 'ho', 'se', ' u', 'nc', 'le', '\\nw', 'an', 'te', 'd ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'a ', 'ri', 'ch', ' w', 'if', 'e.', '  ']\n",
            "\n",
            "processing file stories/064.txt\n",
            "Data size (chars) (document 63) 4981\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'an', 'd ', 'a ', 'po', 'or', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' a', '\\nl', 'it', 'tl', 'e ', 'co', 'tt', 'ag', 'e,', ' a', 'nd', ' w', 'ho', ' e', 'ar', 'ne', 'd ', 'th', 'ei']\n",
            "\n",
            "processing file stories/065.txt\n",
            "Data size (chars) (document 64) 6006\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'se', 't ', 'ou', 't ', 'on', ' a', ' l', 'on', 'g\\n', 'jo', 'ur', 'ne', 'y,', ' a', 'nd', ' o', 'n ', 'pa', 'rt', 'in', 'g ', 'he', ' a', 'sk', 'ed']\n",
            "\n",
            "processing file stories/066.txt\n",
            "Data size (chars) (document 65) 5900\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'qu', 'ee', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd ', 'ha', 'd ', 'be', 'en', ' d', 'ea', 'd\\n', 'fo', 'r ', 'ma', 'ny', ' y', 'ea', 'rs', ', ', 'an', 'd ', 'sh', 'e ', 'ha', 'd ', 'a ', 'be']\n",
            "\n",
            "processing file stories/067.txt\n",
            "Data size (chars) (document 66) 7837\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' c', 'ou', 'nt', 'ry', 'ma', 'n ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'as', ' b', 'ig', ' a', 's ', 'a ', 'th', 'um', 'b,', '\\na', 'nd', ' d', 'id', ' n', 'ot', ' b', 'ec', 'om', 'e ', 'an', 'y ', 'bi', 'gg', 'er', ', ', 'an']\n",
            "\n",
            "processing file stories/068.txt\n",
            "Data size (chars) (document 67) 4717\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' r', 'ic', 'h ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'wh', 'o\\n', 'da', 'il', 'y ', 'we', 'nt', ' t', 'o ', 'wa', 'lk', ' i', 'n ', 'th', 'e ', 'pa', 'la', 'ce']\n",
            "\n",
            "processing file stories/069.txt\n",
            "Data size (chars) (document 68) 6233\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'me', 'rc', 'ha', 'nt', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' c', 'hi', 'ld', 're', 'n,', ' a', ' b', 'oy', ' a', 'nd', ' a', ' g', 'ir', 'l,', '\\nt', 'he', 'y ', 'we', 're', ' b', 'ot', 'h ', 'yo', 'un', 'g,', ' a', 'nd', ' c', 'ou', 'ld']\n",
            "\n",
            "processing file stories/070.txt\n",
            "Data size (chars) (document 69) 5664\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' w', 'ho', ' h', 'ad', ' a', ' l', 'it', 'tl', 'e ', 'da', 'ug', 'ht', 'er', ' w', 'ho', '\\nw', 'as', ' s', 'ti', 'll', ' s', 'o ', 'yo', 'un', 'g ', 'th', 'at', ' s', 'he', ' h', 'ad', ' t', 'o ']\n",
            "\n",
            "processing file stories/071.txt\n",
            "Data size (chars) (document 70) 3569\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'ha', 'd ', 'no', ' l', 'an', 'd,', ' b', 'ut', ' o', 'nl', 'y ', 'a ', 'sm', 'al', 'l\\n', 'ho', 'us', 'e,', ' a', 'nd', ' o', 'ne', ' d', 'au', 'gh', 'te', 'r.', '  ', 'th', 'en', ' s', 'ai', 'd ']\n",
            "\n",
            "processing file stories/072.txt\n",
            "Data size (chars) (document 71) 3793\n",
            "Sample string ['ab', 'ou', 't ', 'a ', 'th', 'ou', 'sa', 'nd', ' o', 'r ', 'mo', 're', ' y', 'ea', 'rs', ' a', 'go', ', ', 'th', 'er', 'e ', 'we', 're', ' i', 'n ', 'th', 'is', '\\nc', 'ou', 'nt', 'ry', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' s', 'ma', 'll', ' k', 'in', 'gs', ', ', 'an', 'd ', 'on', 'e ', 'of', ' t']\n",
            "\n",
            "processing file stories/073.txt\n",
            "Data size (chars) (document 72) 5980\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'an', ' i', 'll', 'ne', 'ss', ', ', 'an', 'd ', 'no', ' o', 'ne', ' b', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'e\\n', 'wo', 'ul', 'd ', 'co', 'me', ' o', 'ut', ' o', 'f ', 'it', ' w', 'it', 'h ', 'hi', 's ']\n",
            "\n",
            "processing file stories/074.txt\n",
            "Data size (chars) (document 73) 4518\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'od', 'cu', 'tt', 'er', ' w', 'ho', ' t', 'oi', 'le', 'd ', 'fr', 'om', ' e', 'ar', 'ly', '\\nm', 'or', 'ni', 'ng', ' t', 'il', 'l ', 'la', 'te', ' a', 't ', 'ni', 'gh', 't.', '  ', 'wh', 'en', ' a', 't ', 'la', 'st', ' h', 'e ']\n",
            "\n",
            "processing file stories/075.txt\n",
            "Data size (chars) (document 74) 3247\n",
            "Sample string ['a ', 'di', 'sc', 'ha', 'rg', 'ed', ' s', 'ol', 'di', 'er', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'li', 've', ' o', 'n,', ' a', 'nd', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'ho', 'w ', 'to', '\\nm', 'ak', 'e ', 'hi', 's ', 'wa', 'y.', '  ', 'so', ' h', 'e ', 'we', 'nt', ' o', 'ut', ' i']\n",
            "\n",
            "processing file stories/076.txt\n",
            "Data size (chars) (document 75) 5130\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'en', 'li', 'st', 'ed', ' a', 's ', 'a ', 'so', 'ld', 'ie', 'r,', ' c', 'on', 'du', 'ct', 'ed', '\\nh', 'im', 'se', 'lf', ' b', 'ra', 've', 'ly', ', ', 'an', 'd ', 'wa', 's ', 'al', 'wa', 'ys', ' t']\n",
            "\n",
            "processing file stories/077.txt\n",
            "Data size (chars) (document 76) 2401\n",
            "Sample string ['on', 'ce', ' i', 'n ', 'su', 'mm', 'er', '-t', 'im', 'e ', 'th', 'e ', 'be', 'ar', ' a', 'nd', ' t', 'he', ' w', 'ol', 'f ', 'we', 're', ' w', 'al', 'ki', 'ng', ' i', 'n ', 'th', 'e ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'th', 'e ', 'be', 'ar', ' h', 'ea', 'rd', ' a', ' b', 'ir', 'd ', 'si', 'ng']\n",
            "\n",
            "processing file stories/078.txt\n",
            "Data size (chars) (document 77) 624\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'po', 'or', ' b', 'ut', ' g', 'oo', 'd ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'wh', 'o ', 'li', 've', 'd ', 'al', 'on', 'e ', 'wi', 'th', ' h', 'er', '\\nm', 'ot', 'he', 'r,', ' a', 'nd', ' t', 'he', 'y ', 'no', ' l', 'on', 'ge', 'r ', 'ha', 'd ', 'an', 'yt', 'hi']\n",
            "\n",
            "processing file stories/079.txt\n",
            "Data size (chars) (document 78) 3991\n",
            "Sample string ['on', 'e ', 'da', 'y ', 'a ', 'pe', 'as', 'an', 't ', 'to', 'ok', ' h', 'is', ' g', 'oo', 'd ', 'ha', 'ze', 'l-', 'st', 'ic', 'k ', 'ou', 't ', 'of', ' t', 'he', ' c', 'or', 'ne', 'r\\n', 'an', 'd ', 'sa', 'id', ' t', 'o ', 'hi', 's ', 'wi', 'fe', ', ', 'tr', 'in', 'a,', ' i', ' a', 'm ', 'go', 'in']\n",
            "\n",
            "processing file stories/080.txt\n",
            "Data size (chars) (document 79) 1426\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd ', 'wh', 'os', 'e ', 'mo', 'th', 'er', ' g', 'av', 'e ', 'he', 'r ', 'ev', 'er', 'y\\n', 'af', 'te', 'rn', 'oo', 'n ', 'a ', 'sm', 'al', 'l ', 'bo', 'wl', ' o', 'f ', 'mi', 'lk', ' a', 'nd', ' b', 're', 'ad', ', ']\n",
            "\n",
            "processing file stories/081.txt\n",
            "Data size (chars) (document 80) 3574\n",
            "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' m', 'il', 'l ', 'li', 've', 'd ', 'an', ' o', 'ld', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'ha', 'd ', 'ne', 'it', 'he', 'r ', 'wi', 'fe', ' n', 'or', ' c', 'hi', 'ld', ',\\n', 'an', 'd ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's ', 'se', 'rv', 'ed', ' u']\n",
            "\n",
            "processing file stories/082.txt\n",
            "Data size (chars) (document 81) 10822\n",
            "Sample string ['hi', 'll', ' a', 'nd', ' v', 'al', 'e ', 'do', ' n', 'ot', ' m', 'ee', 't,', ' b', 'ut', ' t', 'he', ' c', 'hi', 'ld', 're', 'n ', 'of', ' m', 'en', ' d', 'o,', ' g', 'oo', 'd ', 'an', 'd ', 'ba', 'd.', '\\ni', 'n ', 'th', 'is', ' w', 'ay', ' a', ' s', 'ho', 'em', 'ak', 'er', ' a', 'nd', ' a', ' t']\n",
            "\n",
            "processing file stories/083.txt\n",
            "Data size (chars) (document 82) 5480\n",
            "Sample string ['\\th', 'an', 's ', 'th', 'e ', 'he', 'dg', 'eh', 'og', '\\n\\n', 'th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'ou', 'nt', 'ry', ' m', 'an', ' w', 'ho', ' h', 'ad', ' m', 'on', 'ey', ' a', 'nd', ' l', 'an', 'd ', 'in', ' p', 'le', 'nt', 'y,', ' b', 'ut', '\\nh', 'ow', 'ev', 'er', ' r', 'ic', 'h ']\n",
            "\n",
            "processing file stories/084.txt\n",
            "Data size (chars) (document 83) 658\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' o', 'f ', 'se', 've', 'n ', 'ye', 'ar', 's ', 'ol', 'd,', ' w', 'ho', '\\nw', 'as', ' s', 'o ', 'ha', 'nd', 'so', 'me', ' a', 'nd', ' l', 'ov', 'ab', 'le', ' t', 'ha']\n",
            "\n",
            "processing file stories/085.txt\n",
            "Data size (chars) (document 84) 5989\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'ha', 'd ', 'le', 'ar', 'nt', ' t', 'he', ' t', 'ra', 'de', ' o', 'f ', 'lo', 'ck', 'sm', 'it', 'h,', '\\na', 'nd', ' t', 'ol', 'd ', 'hi', 's ', 'fa', 'th', 'er', ' h', 'e ', 'wo', 'ul', 'd ', 'no']\n",
            "\n",
            "processing file stories/086.txt\n",
            "Data size (chars) (document 85) 8758\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' i', 'n ', 'wh', 'os', 'e ', 'st', 'ar', 's\\n', 'it', ' h', 'ad', ' b', 'ee', 'n ', 'fo', 're', 'to', 'ld', ' t', 'ha', 't ', 'he', ' s']\n",
            "\n",
            "processing file stories/087.txt\n",
            "Data size (chars) (document 86) 3109\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's ', 'wh', 'o ', 'wa', 's ', 'ex', 'tr', 'em', 'el', 'y ', 'pr', 'ou', 'd.', ' i', 'f ', 'a\\n', 'wo', 'oe', 'r ', 'ca', 'me', ' s', 'he', ' g', 'av', 'e ', 'hi', 'm ', 'so', 'me', ' r', 'id']\n",
            "\n",
            "processing file stories/088.txt\n",
            "Data size (chars) (document 87) 1365\n",
            "Sample string ['a ', 'ta', 'il', 'or', \"'s\", ' a', 'pp', 're', 'nt', 'ic', 'e ', 'wa', 's ', 'tr', 'av', 'el', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'wo', 'rl', 'd ', 'in', ' s', 'ea', 'rc', 'h ', 'of', '\\nw', 'or', 'k,', ' a', 'nd', ' a', 't ', 'on', 'e ', 'ti', 'me', ' h', 'e ', 'co', 'ul', 'd ', 'fi', 'nd']\n",
            "\n",
            "processing file stories/089.txt\n",
            "Data size (chars) (document 88) 4538\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'ol', 'di', 'er', ' w', 'ho', ' f', 'or', ' m', 'an', 'y ', 'ye', 'ar', 's ', 'ha', 'd ', 'se', 'rv', 'ed', ' t', 'he', '\\nk', 'in', 'g ', 'fa', 'it', 'hf', 'ul', 'ly', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he', ' w']\n",
            "\n",
            "processing file stories/090.txt\n",
            "Data size (chars) (document 89) 345\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' c', 'hi', 'ld', ' w', 'ho', ' w', 'as', ' w', 'il', 'lf', 'ul', ', ', 'an', 'd ', 'wo', 'ul', 'd ', 'no', 't ', 'do', '\\nw', 'ha', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'is', 'he', 'd.', '  ', 'fo', 'r ', 'th']\n",
            "\n",
            "processing file stories/091.txt\n",
            "Data size (chars) (document 90) 5460\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n,', ' w', 'ho', ' w', 'as', ' n', 'o ', 'lo', 'ng', 'er', ' c', 'on', 'te', 'nt', ' t', 'o ', 'st', 'ay', ' a', 't\\n', 'ho', 'me', ' i', 'n ', 'hi', 's ', 'fa', 'th', 'er', \"'s\", ' h', 'ou', 'se', ', ', 'an', 'd ', 'as']\n",
            "\n",
            "processing file stories/092.txt\n",
            "Data size (chars) (document 91) 6854\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' h', 'un', 'ts', 'ma', 'n ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'li', 'e ', 'in', '\\nw', 'ai', 't.', '  ', 'he', ' h', 'ad', ' a', ' f', 're', 'sh', ' a', 'nd', ' j', 'oy', 'ou', 's ']\n",
            "\n",
            "processing file stories/093.txt\n",
            "Data size (chars) (document 92) 2314\n",
            "Sample string ['a ', 'po', 'or', ' s', 'er', 'va', 'nt', '-g', 'ir', 'l ', 'wa', 's ', 'on', 'ce', ' t', 'ra', 've', 'li', 'ng', ' w', 'it', 'h ', 'th', 'e ', 'fa', 'mi', 'ly', ' w', 'it', 'h ', 'wh', 'ic', 'h ', 'sh', 'e\\n', 'wa', 's ', 'in', ' s', 'er', 'vi', 'ce', ', ', 'th', 'ro', 'ug', 'h ', 'a ', 'gr', 'ea']\n",
            "\n",
            "processing file stories/094.txt\n",
            "Data size (chars) (document 93) 1706\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' e', 'ls', 'e ', 'in', ' t', 'he', '\\nw', 'or', 'ld', ' b', 'ut', ' t', 'he', ' h', 'ou', 'se', ' i', 'n ', 'wh', 'ic', 'h ', 'he', ' l', 'iv']\n",
            "\n",
            "processing file stories/095.txt\n",
            "Data size (chars) (document 94) 3229\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' t', 'he', ' k', 'in', 'g ', 'ha', 'd ', 'ma', 'ny', ' s', 'ol', 'di', 'er', 's,', ' b', 'ut', ' g', 'av', 'e ', 'th', 'em', '\\ns', 'ma', 'll', ' p', 'ay', ', ', 'so', ' s', 'ma', 'll', ' t', 'ha', 't ', 'th', 'ey', ' c']\n",
            "\n",
            "processing file stories/096.txt\n",
            "Data size (chars) (document 95) 4954\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' m', 'an', ' a', 'nd', ' a', ' w', 'om', 'an', ' w', 'ho', ' s', 'o ', 'lo', 'ng', ' a', 's ', 'th', 'ey', ' w', 'er', 'e\\n', 'ri', 'ch', ' h', 'ad', ' n', 'o ', 'ch', 'il', 'dr', 'en', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he']\n",
            "\n",
            "processing file stories/097.txt\n",
            "Data size (chars) (document 96) 5732\n",
            "Sample string ['in', ' t', 'he', ' d', 'ay', 's ', 'wh', 'en', ' w', 'is', 'hi', 'ng', ' w', 'as', ' s', 'ti', 'll', ' o', 'f ', 'so', 'me', ' u', 'se', ', ', 'a ', 'ki', 'ng', \"'s\", ' s', 'on', ' w', 'as', '\\nb', 'ew', 'it', 'ch', 'ed', ' b', 'y ', 'an', ' o', 'ld', ' w', 'it', 'ch', ', ', 'an', 'd ', 'sh', 'ut']\n",
            "\n",
            "processing file stories/098.txt\n",
            "Data size (chars) (document 97) 4334\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'fo', 'ur', ' s', 'on', 's,', ' a', 'nd', ' w', 'he', 'n ', 'th', 'ey', ' w', 'er', 'e ', 'gr', 'ow', 'n\\n', 'up', ', ', 'he', ' s', 'ai', 'd ', 'to', ' t', 'he', 'm,', ' \"', 'my', ' d', 'ea', 'r ']\n",
            "\n",
            "processing file stories/099.txt\n",
            "Data size (chars) (document 98) 7090\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'th', 'e ', 'el', 'de', 'st', ' o', 'f ', 'wh', 'om', '\\nw', 'as', ' c', 'al', 'le', 'd ', 'on', 'e-', 'ey', 'e,', ' b', 'ec', 'au', 'se', ' s', 'he', ' h']\n",
            "\n",
            "processing file stories/100.txt\n",
            "Data size (chars) (document 99) 1007\n",
            "Sample string ['\"g', 'oo', 'd-', 'da', 'y,', ' f', 'at', 'he', 'r ', 'ho', 'll', 'en', 'th', 'e.', '\" ', '\"m', 'an', 'y ', 'th', 'an', 'ks', ', ', 'pi', 'f-', 'pa', 'f-', 'po', 'lt', 'ri', 'e.', '\" ', '\"m', 'ay', ' i', '\\nb', 'e ', 'al', 'lo', 'we', 'd ', 'to', ' h', 'av', 'e ', 'yo', 'ur', ' d', 'au', 'gh', 'te']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_1fvfu0sHiw",
        "outputId": "5c0fd26d-a239-423d-e8ea-880613b7ea45"
      },
      "source": [
        "\n",
        "def build_dataset(documents):\n",
        "    chars = []\n",
        "    # list of lists\n",
        "    data_list = []\n",
        "\n",
        "    for d in documents:\n",
        "        chars.extend(d)\n",
        "    print('%d character found.' % len(chars))\n",
        "\n",
        "    count = []\n",
        "    # bigrams sorted by their frequency\n",
        "    count.extend(collections.Counter(chars).most_common())\n",
        "\n",
        "    # Create dict map word to id by given the current length of the dictionary\n",
        "    # UNK is for two rare word\n",
        "    dictionary = dict({'UNK': 0})\n",
        "    for char, c in count:\n",
        "        # Only add if its frequency is more than 10\n",
        "        if c > 10:\n",
        "            dictionary[char] = len(dictionary)\n",
        "    unk_count = 0\n",
        "    # replace word with id of word\n",
        "    for d in documents:\n",
        "        data = list()\n",
        "        for char in d:\n",
        "            # if word in dictionary use the id of word\n",
        "            # otherwise use id of UNK\n",
        "            if char in dictionary:\n",
        "                index = dictionary[char]\n",
        "            else:\n",
        "                index = dictionary['UNK']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        data_list.append(data)\n",
        "\n",
        "    # dict map id to word\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data_list, count, dictionary, reverse_dictionary\n",
        "\n",
        "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Least common words (+UNK)', count[-15:])\n",
        "print('Sample data', data_list[0][:10])\n",
        "print('Sample data', data_list[1][:10])\n",
        "print('Vocabulary: ',len(dictionary))\n",
        "vocabulary_size = len(dictionary)\n",
        "del documents  # To reduce memory.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "449177 character found.\n",
            "Most common words (+UNK) [('e ', 15229), ('he', 15164), (' t', 13443), ('th', 13076), ('d ', 10687)]\n",
            "Least common words (+UNK) [('bj', 1), ('ii', 1), ('i?', 1), ('z ', 1), ('c.', 1), ('\"k', 1), ('pw', 1), ('f?', 1), (' z', 1), ('xq', 1), ('nm', 1), ('m?', 1), ('\\t\"', 1), ('\\tw', 1), ('tz', 1)]\n",
            "Sample data [15, 28, 86, 23, 3, 95, 74, 11, 2, 16]\n",
            "Sample data [22, 156, 25, 37, 82, 185, 43, 9, 90, 19]\n",
            "Vocabulary:  544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_1mhkENsJIE",
        "outputId": "aebb2138-e748-4f7b-90e2-065af4bbc167"
      },
      "source": [
        "\n",
        "all_data = [data for i in data_list for data in i]\n",
        "all_data = tf.convert_to_tensor(all_data, dtype=tf.int64)\n",
        "len(all_data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "449177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ_nRqAjsMyn",
        "outputId": "6c6f8394-6c9c-43e1-eb4d-e4c232e56d12"
      },
      "source": [
        "\n",
        "# datasets = [tf.data.Dataset.from_tensor_slices(data) for data in data_list]\n",
        "# datasets\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_data, )\n",
        "dataset\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFSU8S4BsPCU",
        "outputId": "99c3d698-4fe7-440c-b104-4f7beecdc6b2"
      },
      "source": [
        "\n",
        "for i in dataset.take(5):\n",
        "    print(reverse_dictionary[i.numpy()])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in\n",
            " o\n",
            "ld\n",
            "en\n",
            " t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_tEPFEIsPeC",
        "outputId": "ad471dcf-f8f4-43e3-801f-4474444fea24"
      },
      "source": [
        "\n",
        "\n",
        "seq_length = 64\n",
        "examples_per_epoch = len(all_data) // (seq_length + 1)\n",
        "\n",
        "dataset_sequences  = dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for item in dataset_sequences.take(5):\n",
        "    for c in item.numpy():\n",
        "        print(reverse_dictionary[c], end='')\n",
        "    print(\"\\n\" + \"-\" * 24)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in olden times when wishing still helped one, there lived a king\n",
            "whose daughters were all beautiful, but the youngest was so beaut\n",
            "------------------------\n",
            "iful\n",
            "that the sun itself, which has seen so much, was astonished whenever\n",
            "it shone in her face.  close by the king's castle lay a \n",
            "------------------------\n",
            "great dark\n",
            "forest, and under an old lime-tree in the forest was a well, and when\n",
            "the day was very warm, the king's child went out \n",
            "------------------------\n",
            "into the forest and\n",
            "sat down by the side of the cool fountain, and when she was bored she\n",
            "took a golden ball, and threw it up on h\n",
            "------------------------\n",
            "igh and caught it, and this\n",
            "ball was her favorite plaything.\n",
            "\n",
            "now it so happened that on one occasion the princess's golden ball\n",
            "d\n",
            "------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXL-WqUwsPzg",
        "outputId": "9d90ae74-1340-46a0-9c74-c3c1f9e51f29"
      },
      "source": [
        "\n",
        "# Map text to input and target (both input and target have\n",
        "# the same seq_length but target is shifted to right one character)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1] # take all except the last character\n",
        "    target_text = chunk[1:] # take all except the first character\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = dataset_sequences.map(split_input_target)\n",
        "dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function split_input_target at 0x7fe0a39320d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <function split_input_target at 0x7fe0a39320d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaWiYG4TsQOE",
        "outputId": "624acda0-6cb7-44af-f501-2bca8a2ae950"
      },
      "source": [
        "\n",
        "for input_exp, target_exp in dataset.take(2):\n",
        "    print(\"\\nInput data: \")\n",
        "    for i in input_exp.numpy():\n",
        "        print(reverse_dictionary[i], end='')\n",
        "    print(\"\\nTarget data: \")\n",
        "    for t in target_exp.numpy():\n",
        "        print(reverse_dictionary[t], end='')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input data: \n",
            "in olden times when wishing still helped one, there lived a king\n",
            "whose daughters were all beautiful, but the youngest was so bea\n",
            "Target data: \n",
            " olden times when wishing still helped one, there lived a king\n",
            "whose daughters were all beautiful, but the youngest was so beaut\n",
            "Input data: \n",
            "iful\n",
            "that the sun itself, which has seen so much, was astonished whenever\n",
            "it shone in her face.  close by the king's castle lay \n",
            "Target data: \n",
            "ul\n",
            "that the sun itself, which has seen so much, was astonished whenever\n",
            "it shone in her face.  close by the king's castle lay a "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glQ7C62nsWNE",
        "outputId": "9bf4dd83-0365-4ecc-f92c-134356d7dbdf"
      },
      "source": [
        "\n",
        "BATCH_SIZE = 32\n",
        "embedding_dim = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
        "\n",
        "dataset\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((32, 64), (32, 64)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35bewzjvnjx",
        "outputId": "d58879de-32f7-4ecd-e231-6b60f156132f"
      },
      "source": [
        "for i in dataset.take(1):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: id=67, shape=(32, 64), dtype=int64, numpy=\n",
            "array([[ 15,  28,  86, ..., 100,  25,  43],\n",
            "       [142, 123,  99, ...,   1, 138,  37],\n",
            "       [185,  43,   9, ...,  23,   9,  21],\n",
            "       ...,\n",
            "       [ 16, 229,  44, ...,  80, 204,  58],\n",
            "       [ 29, 102,   2, ...,   7,  10,   5],\n",
            "       [ 55,  17,  27, ...,  36,  42,  40]])>, <tf.Tensor: id=68, shape=(32, 64), dtype=int64, numpy=\n",
            "array([[ 28,  86,  23, ...,  25,  43,  60],\n",
            "       [123,  99,  22, ..., 138,  37,  82],\n",
            "       [ 43,   9, 199, ...,   9,  21,   9],\n",
            "       ...,\n",
            "       [229,  44,  36, ..., 204,  58,   5],\n",
            "       [102,   2,  12, ...,  10,   5,  46],\n",
            "       [ 17,  27,  38, ...,  42,  40,  87]])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X80Z7RKZsYe-",
        "outputId": "aa09a431-715a-489f-de4a-07f098f0c4f4"
      },
      "source": [
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, None)),\n",
        "        tf.keras.layers.GRU(units=rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (32, None, 64)            34816     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (32, None, 1024)          3345408   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (32, None, 544)           557600    \n",
            "=================================================================\n",
            "Total params: 3,937,824\n",
            "Trainable params: 3,937,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgRNnLdNsaNZ",
        "outputId": "42c7bcaa-da20-4f69-c591-57ba1e0fd2a1"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "    For each character the model looks up the\n",
        "    embedding, runs the GRU one timestep with\n",
        "    the embedding as input, and applies the dense\n",
        "    layer to generate logits predicting the log-likelihood of the next character:\n",
        "\"\"\"\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# my_model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 64, 544) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBfEocYnscbh",
        "outputId": "031f8c85-b2bc-4947-8260-99fbd1ad57ad"
      },
      "source": [
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([471, 227, 434, 432, 147, 181, 186, 540,  20, 385, 161, 294, 300,\n",
              "       302, 226,   6,   8,  64, 145, 450, 267,  83,  27, 150,  79, 333,\n",
              "         1, 203, 214, 429, 497,  21,  35, 432, 476, 149, 306, 157, 237,\n",
              "       509, 537, 536, 500, 373,  14,  15, 255, 492, 121, 367, 153, 379,\n",
              "       540, 282, 230, 269, 228, 437, 443, 328,  80,  84, 208, 438])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmJR2yk_sdwj",
        "outputId": "dcfcd59c-5b61-4d6e-daf6-44bdf0d12eba"
      },
      "source": [
        "\n",
        "def loss_sparse(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss_sparse(target_example_batch, example_batch_predictions)\n",
        "example_batch_loss.numpy().mean()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.299143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS6NxfPlsfI9"
      },
      "source": [
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "def train_step(inp, target):\n",
        "    with tf.GradientTape() as g:\n",
        "        predictions = model(inp)\n",
        "        loss = tf.reduce_mean(loss_sparse(target, predictions))\n",
        "\n",
        "    gradients = g.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTm3XrY4sj9r"
      },
      "source": [
        "\n",
        "# EPOCHS = 10\n",
        "# checkpoint_dir = './stories_checkpoint'\n",
        "\n",
        "# checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=checkpoint_predix,\n",
        "#     save_weights_only=True\n",
        "# )\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     start = time.time()\n",
        "#     loss = 0\n",
        "#     # reset hidden state\n",
        "#     model.reset_states()\n",
        "\n",
        "#     for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "#         loss = train_step(inp, target)\n",
        "\n",
        "#         if batch_n % 100 == 0:\n",
        "#             print('Epoch {} Batch {} Loss {}'.format(epoch + 1, batch_n, loss))\n",
        "\n",
        "#     if (epoch + 1) % 5 == 0:\n",
        "#         model.save_weights(checkpoint_predix.format(epoch=epoch))\n",
        "\n",
        "#     print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
        "#     print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "\n",
        "# model.save_weights(checkpoint_predix.format(epoch=epoch))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH1c-pTOtbtz"
      },
      "source": [
        "\n",
        "model.compile(\n",
        "    optimizer = tf.train.AdamOptimizer(),\n",
        "    loss = loss_sparse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTpKTrlXtj2c"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './stories_test_checkpoint'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5u_4SJ4tmBz",
        "outputId": "5396d31f-aca1-4754-da65-86e18036f481"
      },
      "source": [
        "\n",
        "EPOCHS = 64\n",
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n",
            "Epoch 1/64\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "215/215 [==============================] - 30s 139ms/step - loss: 4.4428\n",
            "Epoch 2/64\n",
            "215/215 [==============================] - 29s 134ms/step - loss: 3.2972\n",
            "Epoch 3/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 2.9668\n",
            "Epoch 4/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 2.7475\n",
            "Epoch 5/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 2.5655\n",
            "Epoch 6/64\n",
            "215/215 [==============================] - 28s 130ms/step - loss: 2.3906\n",
            "Epoch 7/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 2.2062\n",
            "Epoch 8/64\n",
            "215/215 [==============================] - 29s 135ms/step - loss: 2.0235\n",
            "Epoch 9/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 1.8838\n",
            "Epoch 10/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 1.7673\n",
            "Epoch 11/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 1.6580\n",
            "Epoch 12/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 1.5581\n",
            "Epoch 13/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 1.4694\n",
            "Epoch 14/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 1.4001\n",
            "Epoch 15/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 1.3387\n",
            "Epoch 16/64\n",
            "215/215 [==============================] - 29s 134ms/step - loss: 1.2844\n",
            "Epoch 17/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 1.2387\n",
            "Epoch 18/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 1.1985\n",
            "Epoch 19/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 1.1560\n",
            "Epoch 20/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 1.1139\n",
            "Epoch 21/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 1.0816\n",
            "Epoch 22/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 1.0403\n",
            "Epoch 23/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 1.0111\n",
            "Epoch 24/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.9741\n",
            "Epoch 25/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.9477\n",
            "Epoch 26/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.9287\n",
            "Epoch 27/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.9052\n",
            "Epoch 28/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.8837\n",
            "Epoch 29/64\n",
            "215/215 [==============================] - 29s 134ms/step - loss: 0.8613\n",
            "Epoch 30/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 0.8366\n",
            "Epoch 31/64\n",
            "215/215 [==============================] - 28s 130ms/step - loss: 0.8182\n",
            "Epoch 32/64\n",
            "215/215 [==============================] - 28s 130ms/step - loss: 0.7973\n",
            "Epoch 33/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.7860\n",
            "Epoch 34/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.7681\n",
            "Epoch 35/64\n",
            "215/215 [==============================] - 29s 134ms/step - loss: 0.7445\n",
            "Epoch 36/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.7313\n",
            "Epoch 37/64\n",
            "215/215 [==============================] - 29s 134ms/step - loss: 0.7181\n",
            "Epoch 38/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.7040\n",
            "Epoch 39/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.6942\n",
            "Epoch 40/64\n",
            "215/215 [==============================] - 29s 135ms/step - loss: 0.6787\n",
            "Epoch 41/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 0.6728\n",
            "Epoch 42/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.6754\n",
            "Epoch 43/64\n",
            "215/215 [==============================] - 29s 136ms/step - loss: 0.6792\n",
            "Epoch 44/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.6693\n",
            "Epoch 45/64\n",
            "215/215 [==============================] - 29s 135ms/step - loss: 0.6589\n",
            "Epoch 46/64\n",
            "215/215 [==============================] - 28s 129ms/step - loss: 0.6407\n",
            "Epoch 47/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 0.6320\n",
            "Epoch 48/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 0.6303\n",
            "Epoch 49/64\n",
            "215/215 [==============================] - 28s 130ms/step - loss: 0.6173\n",
            "Epoch 50/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 0.6153\n",
            "Epoch 51/64\n",
            "215/215 [==============================] - 28s 130ms/step - loss: 0.6011\n",
            "Epoch 52/64\n",
            "215/215 [==============================] - 28s 130ms/step - loss: 0.5856\n",
            "Epoch 53/64\n",
            "215/215 [==============================] - 28s 130ms/step - loss: 0.5769\n",
            "Epoch 54/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.5686\n",
            "Epoch 55/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.5691\n",
            "Epoch 56/64\n",
            "215/215 [==============================] - 29s 134ms/step - loss: 0.5847\n",
            "Epoch 57/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.6035\n",
            "Epoch 58/64\n",
            "215/215 [==============================] - 28s 128ms/step - loss: 0.6114\n",
            "Epoch 59/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.5951\n",
            "Epoch 60/64\n",
            "215/215 [==============================] - 28s 132ms/step - loss: 0.5855\n",
            "Epoch 61/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.5710\n",
            "Epoch 62/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.5655\n",
            "Epoch 63/64\n",
            "215/215 [==============================] - 28s 131ms/step - loss: 0.5565\n",
            "Epoch 64/64\n",
            "215/215 [==============================] - 29s 133ms/step - loss: 0.5464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB7piNECyXIX",
        "outputId": "fe146465-972c-484d-ad38-9770f4c10d7b"
      },
      "source": [
        "\n",
        "model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 64)             34816     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3345408   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 544)            557600    \n",
            "=================================================================\n",
            "Total params: 3,937,824\n",
            "Trainable params: 3,937,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clwfKPK1yt6z",
        "outputId": "44731cd7-0b03-4b88-b456-08c9a001cce4"
      },
      "source": [
        "# Vectorize\n",
        "char2idx = {u:i for i, u in zip(dictionary.values(), dictionary.keys())}\n",
        "idx2char = {i:u for i, u in zip(dictionary.values(), dictionary.keys())}\n",
        "\n",
        "# Convert all character to int base on char2idx dict\n",
        "text_as_int = np.array(all_data)\n",
        "len(idx2char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYPJpl9kuoe7"
      },
      "source": [
        "\n",
        "def generate_text_bigram(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "  two_grams = [''.join(start_string[ch_i:ch_i+2]) for ch_i in range(0, len(start_string)-2, 2)]\n",
        "  two_grams_ids = []\n",
        "  for i in two_grams:\n",
        "    two_grams_ids.append(dictionary[i])\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = tf.convert_to_tensor(two_grams_ids)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(input_eval.shape)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XU9j9o5vSu7",
        "outputId": "840f91a5-99a4-402d-e861-2234680712cb"
      },
      "source": [
        "print(generate_text_bigram(model, start_string=u\"a certain father had two sons, the elder of whom was smart ands\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 31)\n",
            "a certain father had two sons, the elder of whom was smart ands attendants\n",
            "which\n",
            "to into\n",
            "the king's you.  i will\n",
            "travely\n",
            "you, and if i small not be able to give the\n",
            "middle me the\n",
            "tailor died, \"an you sat\n",
            "my good horbs into the right, it would have eaten\n",
            "hy and his time, and believed that,\n",
            "after all, she had only for a keep which they had thought\n",
            "to himself,\n",
            "\"thus it came in.  the king\n",
            "said to him, \"you a\n",
            "come.  i have been asleep.\" but it?\" had a could who he grows sleeps.  the bride bed, jemake them\n",
            "go with\n",
            "them and put a\n",
            "bit off oy\n",
            "out of his sput, that there was no bringing it out,\n",
            "with great fellowed,\" and will give him in a threat and\n",
            "wision to him.  you are,\" said they, and said, \"open that tere but\n",
            "faithful to the king's come, they lived on your beloath with\n",
            "the gole.\"  when the old kinger said,\n",
            "\"when darkness cannot ring-maider.  when they were full of may were alive, when i am a ping.\n",
            "\n",
            "she said\n",
            "up on any had apprentice in the first ston she lay down until her human together, a sortere can go sollier,\n",
            "my carriage, and we have thought of\n",
            "taking there was a stronger to excus.  how i am\n",
            "as sucprome with him and said, \"i must have a threteold of strawberr, under their own no once more fiving the middet of the road in the caff,\" said the king's son. then he has\n",
            "done that, took courage, and\n",
            "the evening when they were making ready, and the piece approached to\n",
            "him, and carried her trouble.  now she will cookent, and when she looked\n",
            "around, the tree\n",
            "was a handsome man, who embraced all the birds here, i will\n",
            "travelon she eight, and with the stork, and threw him into prison.\n",
            "\n",
            "the next morning the king said,\n",
            "\"you have indeed watched all the been in\n",
            "one i must pariever, one of is will bear through\n",
            "till you so akat.\" old just lift, he said, \"i am\n",
            "go upany round about it into the king's bed and said, \"what i will travely very more sugger to\n",
            "ance in might farth from ahe knew, he was turned at all.  the soman fell\n",
            "without her dress.  and when\n",
            "they had traveled up, and said, \"good-bye, hans.  hans\n",
            "comes me had someon and king was astoni\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "GxdoqnUoXMtP",
        "outputId": "fa3f9350-3e21-401f-b5ed-aee46648d228"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('stories_test_checkpoint/ckpt_64.data-00001-of-00002') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c39f7b97-1e32-4911-8f97-d2c69b6758fc\", \"ckpt_64.data-00001-of-00002\", 46836112)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_VysXRwc-dd"
      },
      "source": [
        "import pickle\n",
        "with open('stories_dictionary.pickle', 'wb') as handle:\n",
        "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}