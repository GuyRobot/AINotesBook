{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "TextGenerateModel.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuyRobot/AINotesBook/blob/main/TextGenerateModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SrEAbk1i52Km"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJXx7ojC54xh"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import collections\n",
        "import time\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY2rwBn96Aq1",
        "outputId": "1376bb97-39b0-4955-cc5e-4a9c34c7efd6"
      },
      "source": [
        "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
        "\n",
        "dir_name = 'stories'\n",
        "if not os.path.exists(dir_name):\n",
        "    os.mkdir(dir_name)\n",
        "\n",
        "def download_stories(filename):\n",
        "    print(\"Downloading file: \", dir_name + os.sep + filename)\n",
        "    if not os.path.exists(os.path.join(dir_name, filename)):\n",
        "        filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
        "    else:\n",
        "        print(\"File %s already exits\" % filename)\n",
        "    return filename\n",
        "\n",
        "\n",
        "filenames = [format(i, '03d') + '.txt' for i in range(1, 101)]\n",
        "\n",
        "for fn in filenames:\n",
        "    download_stories(fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading file:  stories/001.txt\n",
            "Downloading file:  stories/002.txt\n",
            "Downloading file:  stories/003.txt\n",
            "Downloading file:  stories/004.txt\n",
            "Downloading file:  stories/005.txt\n",
            "Downloading file:  stories/006.txt\n",
            "Downloading file:  stories/007.txt\n",
            "Downloading file:  stories/008.txt\n",
            "Downloading file:  stories/009.txt\n",
            "Downloading file:  stories/010.txt\n",
            "Downloading file:  stories/011.txt\n",
            "Downloading file:  stories/012.txt\n",
            "Downloading file:  stories/013.txt\n",
            "Downloading file:  stories/014.txt\n",
            "Downloading file:  stories/015.txt\n",
            "Downloading file:  stories/016.txt\n",
            "Downloading file:  stories/017.txt\n",
            "Downloading file:  stories/018.txt\n",
            "Downloading file:  stories/019.txt\n",
            "Downloading file:  stories/020.txt\n",
            "Downloading file:  stories/021.txt\n",
            "Downloading file:  stories/022.txt\n",
            "Downloading file:  stories/023.txt\n",
            "Downloading file:  stories/024.txt\n",
            "Downloading file:  stories/025.txt\n",
            "Downloading file:  stories/026.txt\n",
            "Downloading file:  stories/027.txt\n",
            "Downloading file:  stories/028.txt\n",
            "Downloading file:  stories/029.txt\n",
            "Downloading file:  stories/030.txt\n",
            "Downloading file:  stories/031.txt\n",
            "Downloading file:  stories/032.txt\n",
            "Downloading file:  stories/033.txt\n",
            "Downloading file:  stories/034.txt\n",
            "Downloading file:  stories/035.txt\n",
            "Downloading file:  stories/036.txt\n",
            "Downloading file:  stories/037.txt\n",
            "Downloading file:  stories/038.txt\n",
            "Downloading file:  stories/039.txt\n",
            "Downloading file:  stories/040.txt\n",
            "Downloading file:  stories/041.txt\n",
            "Downloading file:  stories/042.txt\n",
            "Downloading file:  stories/043.txt\n",
            "Downloading file:  stories/044.txt\n",
            "Downloading file:  stories/045.txt\n",
            "Downloading file:  stories/046.txt\n",
            "Downloading file:  stories/047.txt\n",
            "Downloading file:  stories/048.txt\n",
            "Downloading file:  stories/049.txt\n",
            "Downloading file:  stories/050.txt\n",
            "Downloading file:  stories/051.txt\n",
            "Downloading file:  stories/052.txt\n",
            "Downloading file:  stories/053.txt\n",
            "Downloading file:  stories/054.txt\n",
            "Downloading file:  stories/055.txt\n",
            "Downloading file:  stories/056.txt\n",
            "Downloading file:  stories/057.txt\n",
            "Downloading file:  stories/058.txt\n",
            "Downloading file:  stories/059.txt\n",
            "Downloading file:  stories/060.txt\n",
            "Downloading file:  stories/061.txt\n",
            "Downloading file:  stories/062.txt\n",
            "Downloading file:  stories/063.txt\n",
            "Downloading file:  stories/064.txt\n",
            "Downloading file:  stories/065.txt\n",
            "Downloading file:  stories/066.txt\n",
            "Downloading file:  stories/067.txt\n",
            "Downloading file:  stories/068.txt\n",
            "Downloading file:  stories/069.txt\n",
            "Downloading file:  stories/070.txt\n",
            "Downloading file:  stories/071.txt\n",
            "Downloading file:  stories/072.txt\n",
            "Downloading file:  stories/073.txt\n",
            "Downloading file:  stories/074.txt\n",
            "Downloading file:  stories/075.txt\n",
            "Downloading file:  stories/076.txt\n",
            "Downloading file:  stories/077.txt\n",
            "Downloading file:  stories/078.txt\n",
            "Downloading file:  stories/079.txt\n",
            "Downloading file:  stories/080.txt\n",
            "Downloading file:  stories/081.txt\n",
            "Downloading file:  stories/082.txt\n",
            "Downloading file:  stories/083.txt\n",
            "Downloading file:  stories/084.txt\n",
            "Downloading file:  stories/085.txt\n",
            "Downloading file:  stories/086.txt\n",
            "Downloading file:  stories/087.txt\n",
            "Downloading file:  stories/088.txt\n",
            "Downloading file:  stories/089.txt\n",
            "Downloading file:  stories/090.txt\n",
            "Downloading file:  stories/091.txt\n",
            "Downloading file:  stories/092.txt\n",
            "Downloading file:  stories/093.txt\n",
            "Downloading file:  stories/094.txt\n",
            "Downloading file:  stories/095.txt\n",
            "Downloading file:  stories/096.txt\n",
            "Downloading file:  stories/097.txt\n",
            "Downloading file:  stories/098.txt\n",
            "Downloading file:  stories/099.txt\n",
            "Downloading file:  stories/100.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbaMt2Yz52Kr",
        "outputId": "8df6899c-6680-4af9-c000-78843b67e72b"
      },
      "source": [
        "filenames = [format(i, '03d') + '.txt' for i in range(1, 101)]\n",
        "dir_name = 'stories'\n",
        "\n",
        "def read_data(filename):\n",
        "    with open(filename) as f:\n",
        "        data =  tf.compat.as_str(f.read())\n",
        "        data = data.lower()\n",
        "        data = list(data)\n",
        "    return data\n",
        "\n",
        "global documents\n",
        "documents = []\n",
        "num_files = 100\n",
        "for i in range(num_files):\n",
        "    print(\"processing file %s\" % os.path.join(dir_name, filenames[i]))\n",
        "    chars = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    # break into bigrams\n",
        "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0, len(chars)-2, 2)]\n",
        "    # Create document\n",
        "    documents.append(two_grams)\n",
        "    print(\"Data size (chars) (document %d) %d\" % (i, len(two_grams)))\n",
        "    print(\"Sample string %s\\n\" % (two_grams[:50]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing file stories/001.txt\n",
            "Data size (chars) (document 0) 3667\n",
            "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' w', 'he', 'n ', 'wi', 'sh', 'in', 'g ', 'st', 'il', 'l ', 'he', 'lp', 'ed', ' o', 'ne', ', ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', '\\nw', 'ho', 'se', ' d', 'au', 'gh', 'te', 'rs', ' w', 'er', 'e ', 'al', 'l ', 'be', 'au', 'ti', 'fu', 'l,']\n",
            "\n",
            "processing file stories/002.txt\n",
            "Data size (chars) (document 1) 4928\n",
            "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' w', 'oo', 'd-', 'cu', 'tt', 'er', ' w', 'it', 'h ', 'hi', 's ', 'wi', 'fe', ', ', 'wh', 'o ', 'ha', 'd ', 'an', '\\no', 'nl', 'y ', 'ch', 'il', 'd,', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' t', 'hr', 'ee']\n",
            "\n",
            "processing file stories/003.txt\n",
            "Data size (chars) (document 2) 9745\n",
            "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'fa', 'th', 'er', ' h', 'ad', ' t', 'wo', ' s', 'on', 's,', ' t', 'he', ' e', 'ld', 'er', ' o', 'f ', 'wh', 'om', ' w', 'as', ' s', 'ma', 'rt', ' a', 'nd', '\\ns', 'en', 'si', 'bl', 'e,', ' a', 'nd', ' c', 'ou', 'ld', ' d', 'o ', 'ev', 'er', 'yt', 'hi', 'ng', ', ', 'bu']\n",
            "\n",
            "processing file stories/004.txt\n",
            "Data size (chars) (document 3) 2852\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'go', 'at', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' l', 'it', 'tl', 'e ', 'ki', 'ds', ', ', 'an', 'd\\n', 'lo', 've', 'd ', 'th', 'em', ' w', 'it', 'h ', 'al', 'l ', 'th', 'e ', 'lo', 've', ' o']\n",
            "\n",
            "processing file stories/005.txt\n",
            "Data size (chars) (document 4) 8189\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' i', 'll', ' a', 'nd', ' t', 'ho', 'ug', 'ht', ' t', 'o\\n', 'hi', 'ms', 'el', 'f ', \"'i\", ' a', 'm ', 'ly', 'in', 'g ', 'on', ' w', 'ha', 't ', 'mu', 'st', ' b']\n",
            "\n",
            "processing file stories/006.txt\n",
            "Data size (chars) (document 5) 4369\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'ea', 'sa', 'nt', ' w', 'ho', ' h', 'ad', ' d', 'ri', 've', 'n ', 'hi', 's ', 'co', 'w ', 'to', ' t', 'he', ' f', 'ai', 'r,', ' a', 'nd', ' s', 'ol', 'd\\n', 'he', 'r ', 'fo', 'r ', 'se', 've', 'n ', 'ta', 'le', 'rs', '. ', ' o', 'n ', 'th', 'e ']\n",
            "\n",
            "processing file stories/007.txt\n",
            "Data size (chars) (document 6) 5216\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' a', ' q', 'ue', 'en', ' w', 'ho', ' l', 'iv', 'ed', '\\nh', 'ap', 'pi', 'ly', ' t', 'og', 'et', 'he', 'r ', 'an', 'd ', 'ha', 'd ', 'tw', 'el', 've', ' c', 'hi', 'ld', 're', 'n,', ' b']\n",
            "\n",
            "processing file stories/008.txt\n",
            "Data size (chars) (document 7) 6097\n",
            "Sample string ['li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' t', 'oo', 'k ', 'hi', 's ', 'li', 'tt', 'le', ' s', 'is', 'te', 'r ', 'by', ' t', 'he', ' h', 'an', 'd ', 'an', 'd ', 'sa', 'id', ', ', 'si', 'nc', 'e\\n', 'ou', 'r ', 'mo', 'th', 'er', ' d', 'ie', 'd ', 'we', ' h', 'av', 'e ', 'ha', 'd ', 'no', ' h', 'ap']\n",
            "\n",
            "processing file stories/009.txt\n",
            "Data size (chars) (document 8) 3699\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'a ', 'ma', 'n ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'lo', 'ng', ' i', 'n ', 'va', 'in', '\\nw', 'is', 'he', 'd ', 'fo', 'r ', 'a ', 'ch', 'il', 'd.', '  ', 'at', ' l', 'en', 'gt', 'h ', 'th', 'e ', 'wo', 'ma', 'n ', 'ho', 'pe']\n",
            "\n",
            "processing file stories/010.txt\n",
            "Data size (chars) (document 9) 5268\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', 'se', ' w', 'if', 'e ', 'di', 'ed', ', ', 'an', 'd ', 'a ', 'wo', 'ma', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd\\n', 'di', 'ed', ', ', 'an', 'd ', 'th', 'e ', 'ma', 'n ', 'ha', 'd ', 'a ', 'da', 'ug', 'ht', 'er', ', ', 'an']\n",
            "\n",
            "processing file stories/011.txt\n",
            "Data size (chars) (document 10) 2377\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' g', 'ir', 'l ', 'wh', 'o ', 'wa', 's ', 'id', 'le', ' a', 'nd', ' w', 'ou', 'ld', ' n', 'ot', ' s', 'pi', 'n,', ' a', 'nd', '\\nl', 'et', ' h', 'er', ' m', 'ot', 'he', 'r ', 'sa', 'y ', 'wh', 'at', ' s', 'he', ' w', 'ou', 'ld', ', ', 'sh', 'e ', 'co']\n",
            "\n",
            "processing file stories/012.txt\n",
            "Data size (chars) (document 11) 7695\n",
            "Sample string ['ha', 'rd', ' b', 'y ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ' d', 'we', 'lt', ' a', ' p', 'oo', 'r ', 'wo', 'od', '-c', 'ut', 'te', 'r ', 'wi', 'th', ' h', 'is', ' w', 'if', 'e\\n', 'an', 'd ', 'hi', 's ', 'tw', 'o ', 'ch', 'il', 'dr', 'en', '. ', ' t', 'he', ' b', 'oy', ' w', 'as', ' c', 'al']\n",
            "\n",
            "processing file stories/013.txt\n",
            "Data size (chars) (document 12) 3665\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'oo', 'r ', 'ma', 'n,', ' w', 'ho', ' c', 'ou', 'ld', ' n', 'o ', 'lo', 'ng', 'er', '\\ns', 'up', 'po', 'rt', ' h', 'is', ' o', 'nl', 'y ', 'so', 'n.', '  ', 'th', 'en', ' s', 'ai', 'd ', 'th', 'e ', 'so', 'n,', ' d']\n",
            "\n",
            "processing file stories/014.txt\n",
            "Data size (chars) (document 13) 4178\n",
            "Sample string ['a ', 'lo', 'ng', ' t', 'im', 'e ', 'ag', 'o ', 'th', 'er', 'e ', 'li', 've', 'd ', 'a ', 'ki', 'ng', ' w', 'ho', ' w', 'as', ' f', 'am', 'ed', ' f', 'or', ' h', 'is', ' w', 'is', 'do', 'm\\n', 'th', 'ro', 'ug', 'h ', 'al', 'l ', 'th', 'e ', 'la', 'nd', '. ', ' n', 'ot', 'hi', 'ng', ' w', 'as', ' h']\n",
            "\n",
            "processing file stories/015.txt\n",
            "Data size (chars) (document 14) 8674\n",
            "Sample string ['on', 'e ', 'su', 'mm', 'er', \"'s\", ' m', 'or', 'ni', 'ng', ' a', ' l', 'it', 'tl', 'e ', 'ta', 'il', 'or', ' w', 'as', ' s', 'it', 'ti', 'ng', ' o', 'n ', 'hi', 's ', 'ta', 'bl', 'e\\n', 'by', ' t', 'he', ' w', 'in', 'do', 'w,', ' h', 'e ', 'wa', 's ', 'in', ' g', 'oo', 'd ', 'sp', 'ir', 'it', 's,']\n",
            "\n",
            "processing file stories/016.txt\n",
            "Data size (chars) (document 15) 7018\n",
            "Sample string ['\\tc', 'in', 'de', 're', 'll', 'a\\n', 'th', 'e ', 'wi', 'fe', ' o', 'f ', 'a ', 'ri', 'ch', ' m', 'an', ' f', 'el', 'l ', 'si', 'ck', ', ', 'an', 'd ', 'as', ' s', 'he', ' f', 'el', 't ', 'th', 'at', ' h', 'er', ' e', 'nd', '\\nw', 'as', ' d', 'ra', 'wi', 'ng', ' n', 'ea', 'r,', ' s', 'he', ' c', 'al']\n",
            "\n",
            "processing file stories/017.txt\n",
            "Data size (chars) (document 16) 3039\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'se', 'iz', 'ed', ' w', 'it', 'h ', 'a ', 'de', 'si', 're', ' t', 'o ', 'tr', 'av', 'el', '\\na', 'bo', 'ut', ' t', 'he', ' w', 'or', 'ld', ', ', 'an', 'd ', 'to', 'ok', ' n', 'o ', 'on', 'e ']\n",
            "\n",
            "processing file stories/018.txt\n",
            "Data size (chars) (document 17) 3020\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'id', 'ow', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' d', 'au', 'gh', 'te', 'rs', ' -', ' o', 'ne', ' o', 'f\\n', 'wh', 'om', ' w', 'as', ' p', 're', 'tt', 'y ', 'an', 'd ', 'in', 'du', 'st', 'ri', 'ou', 's,', ' w', 'hi', 'ls', 't ', 'th', 'e ', 'ot']\n",
            "\n",
            "processing file stories/019.txt\n",
            "Data size (chars) (document 18) 2465\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' s', 'ev', 'en', ' s', 'on', 's,', ' a', 'nd', ' s', 'ti', 'll', ' h', 'e ', 'ha', 'd\\n', 'no', ' d', 'au', 'gh', 'te', 'r,', ' h', 'ow', 'ev', 'er', ' m', 'uc', 'h ', 'he', ' w', 'is', 'he', 'd ', 'fo', 'r ', 'on']\n",
            "\n",
            "processing file stories/020.txt\n",
            "Data size (chars) (document 19) 3703\n",
            "Sample string ['\\tl', 'it', 'tl', 'e ', 're', 'd-', 'ca', 'p\\n', '\\no', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'th', 'er', 'e ', 'wa', 's ', 'a ', 'de', 'ar', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' l', 'ov', 'ed', '\\nb', 'y ', 'ev', 'er', 'y ', 'on', 'e ', 'wh', 'o ', 'lo', 'ok', 'ed']\n",
            "\n",
            "processing file stories/021.txt\n",
            "Data size (chars) (document 20) 1924\n",
            "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' c', 'ou', 'nt', 'ry', ' t', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'gr', 'ea', 't ', 'la', 'me', 'nt', 'at', 'io', 'n ', 'ov', 'er', ' a', '\\nw', 'il', 'd ', 'bo', 'ar', ' t', 'ha', 't ', 'la', 'id', ' w', 'as', 'te', ' t', 'he', ' f', 'ar', 'me', \"r'\", 's ']\n",
            "\n",
            "processing file stories/022.txt\n",
            "Data size (chars) (document 21) 6561\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'ma', 'n ', 'wh', 'o ', 'ga', 've', ' b', 'ir', 'th', ' t', 'o ', 'a ', 'li', 'tt', 'le', ' s', 'on', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'ca', 'me', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'wi', 'th', ' a', ' c', 'au']\n",
            "\n",
            "processing file stories/023.txt\n",
            "Data size (chars) (document 22) 5956\n",
            "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'mi', 'll', 'er', ' h', 'ad', ' l', 'it', 'tl', 'e ', 'by', ' l', 'it', 'tl', 'e ', 'fa', 'll', 'en', ' i', 'nt', 'o ', 'po', 've', 'rt', 'y,', ' a', 'nd', '\\nh', 'ad', ' n', 'ot', 'hi', 'ng', ' l', 'ef', 't ', 'bu', 't ', 'hi', 's ', 'mi', 'll', ' a', 'nd', ' a', ' l']\n",
            "\n",
            "processing file stories/024.txt\n",
            "Data size (chars) (document 23) 2529\n",
            "Sample string ['th', 'e ', 'mo', 'th', 'er', ' o', 'f ', 'ha', 'ns', ' s', 'ai', 'd,', ' w', 'hi', 'th', 'er', ' a', 'wa', 'y,', ' h', 'an', 's.', '  ', 'ha', 'ns', ' a', 'ns', 'we', 're', 'd,', ' t', 'o\\n', 'gr', 'et', 'el', '. ', ' b', 'eh', 'av', 'e ', 'we', 'll', ', ', 'ha', 'ns', '. ', ' o', 'h,', ' i', \"'l\"]\n",
            "\n",
            "processing file stories/025.txt\n",
            "Data size (chars) (document 24) 2416\n",
            "Sample string ['an', ' a', 'ge', 'd ', 'co', 'un', 't ', 'on', 'ce', ' l', 'iv', 'ed', ' i', 'n ', 'sw', 'it', 'ze', 'rl', 'an', 'd,', ' w', 'ho', ' h', 'ad', ' a', 'n ', 'on', 'ly', ' s', 'on', ',\\n', 'bu', 't ', 'he', ' w', 'as', ' s', 'tu', 'pi', 'd,', ' a', 'nd', ' c', 'ou', 'ld', ' l', 'ea', 'rn', ' n', 'ot']\n",
            "\n",
            "processing file stories/026.txt\n",
            "Data size (chars) (document 25) 3369\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'ca', 'll', 'ed', ' c', 'le', 've', 'r\\n', 'el', 'si', 'e.', '  ', 'an', 'd ', 'wh', 'en', ' s', 'he', ' h', 'ad', ' g', 'ro', 'wn', ' u', 'p ', 'he', 'r ']\n",
            "\n",
            "processing file stories/027.txt\n",
            "Data size (chars) (document 26) 10013\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' t', 'ai', 'lo', 'r ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'an', 'd\\n', 'on', 'ly', ' o', 'ne', ' g', 'oa', 't.', '  ', 'bu', 't ', 'as', ' t', 'he', ' g', 'oa', 't ', 'su', 'pp', 'or', 'te']\n",
            "\n",
            "processing file stories/028.txt\n",
            "Data size (chars) (document 27) 5788\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'sa', 't ', 'in', ' t', 'he', ' e', 've', 'ni', 'ng', ' b', 'y ', 'th', 'e\\n', 'he', 'ar', 'th', ' a', 'nd', ' p', 'ok', 'ed', ' t', 'he', ' f', 'ir', 'e,', ' a', 'nd', ' h', 'is', ' w', 'if', 'e ']\n",
            "\n",
            "processing file stories/029.txt\n",
            "Data size (chars) (document 28) 1335\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'se', 'rv', 'an', 't-', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' i', 'nd', 'us', 'tr', 'io', 'us', ' a', 'nd', ' c', 'le', 'an', 'ly', '\\na', 'nd', ' s', 'we', 'pt', ' t', 'he', ' h', 'ou', 'se', ' e', 've', 'ry', ' d', 'ay', ', ', 'an']\n",
            "\n",
            "processing file stories/030.txt\n",
            "Data size (chars) (document 29) 3591\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'il', 'le', 'r,', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', '\\nd', 'au', 'gh', 'te', 'r,', ' a', 'nd', ' a', 's ', 'sh', 'e ', 'wa', 's ', 'gr', 'ow', 'n ', 'up', ', ', 'he', ' w', 'is', 'he']\n",
            "\n",
            "processing file stories/031.txt\n",
            "Data size (chars) (document 30) 1624\n",
            "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' s', 'o ', 'ma', 'ny', ' c', 'hi', 'ld', 're', 'n ', 'th', 'at', ' h', 'e ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'as', 'ke', 'd\\n', 'ev', 'er', 'yo', 'ne', ' i', 'n ', 'th', 'e ', 'wo', 'rl', 'd ', 'to', ' b', 'e ', 'go', 'df', 'at', 'he', 'r,', ' a', 'nd']\n",
            "\n",
            "processing file stories/032.txt\n",
            "Data size (chars) (document 31) 758\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'gi', 'rl', ' w', 'ho', ' w', 'as', ' o', 'bs', 'ti', 'na', 'te', ' a', 'nd', ' i', 'nq', 'ui', 'si', 'ti', 've', ',\\n', 'an', 'd ', 'wh', 'en', ' h', 'er', ' p', 'ar', 'en', 'ts', ' t', 'ol', 'd ', 'he', 'r ', 'to', ' d', 'o ']\n",
            "\n",
            "processing file stories/033.txt\n",
            "Data size (chars) (document 32) 3121\n",
            "Sample string ['a ', 'po', 'or', ' m', 'an', ' h', 'ad', ' t', 'we', 'lv', 'e ', 'ch', 'il', 'dr', 'en', ' a', 'nd', ' w', 'as', ' f', 'or', 'ce', 'd ', 'to', ' w', 'or', 'k ', 'ni', 'gh', 't ', 'an', 'd\\n', 'da', 'y ', 'to', ' g', 'iv', 'e ', 'th', 'em', ' e', 've', 'n ', 'br', 'ea', 'd.', '  ', 'wh', 'en', ' t']\n",
            "\n",
            "processing file stories/034.txt\n",
            "Data size (chars) (document 33) 4192\n",
            "Sample string ['a ', 'ce', 'rt', 'ai', 'n ', 'ta', 'il', 'or', ' h', 'ad', ' a', ' s', 'on', ', ', 'wh', 'o ', 'ha', 'pp', 'en', 'ed', ' t', 'o ', 'be', ' s', 'ma', 'll', ', ', 'an', 'd\\n', 'no', ' b', 'ig', 'ge', 'r ', 'th', 'an', ' a', ' t', 'hu', 'mb', ', ', 'an', 'd ', 'on', ' t', 'hi', 's ', 'ac', 'co', 'un']\n",
            "\n",
            "processing file stories/035.txt\n",
            "Data size (chars) (document 34) 3650\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'iz', 'ar', 'd ', 'wh', 'o ', 'us', 'ed', ' t', 'o ', 'ta', 'ke', ' t', 'he', ' f', 'or', 'm ', 'of', ' a', ' p', 'oo', 'r\\n', 'ma', 'n,', ' a', 'nd', ' w', 'en', 't ', 'to', ' h', 'ou', 'se', 's ', 'an', 'd ', 'be', 'gg', 'ed', ', ', 'an', 'd ']\n",
            "\n",
            "processing file stories/036.txt\n",
            "Data size (chars) (document 35) 8219\n",
            "Sample string ['it', ' i', 's ', 'no', 'w ', 'lo', 'ng', ' a', 'go', ', ', 'qu', 'it', 'e ', 'tw', 'o ', 'th', 'ou', 'sa', 'nd', ' y', 'ea', 'rs', ', ', 'si', 'nc', 'e ', 'th', 'er', 'e ', 'wa', 's\\n', 'a ', 'ri', 'ch', ' m', 'an', ' w', 'ho', ' h', 'ad', ' a', ' b', 'ea', 'ut', 'if', 'ul', ' a', 'nd', ' p', 'io']\n",
            "\n",
            "processing file stories/037.txt\n",
            "Data size (chars) (document 36) 2151\n",
            "Sample string ['a ', 'fa', 'rm', 'er', ' o', 'nc', 'e ', 'ha', 'd ', 'a ', 'fa', 'it', 'hf', 'ul', ' d', 'og', ' c', 'al', 'le', 'd ', 'su', 'lt', 'an', ', ', 'wh', 'o ', 'ha', 'd ', 'gr', 'ow', 'n\\n', 'ol', 'd,', ' a', 'nd', ' l', 'os', 't ', 'al', 'l ', 'hi', 's ', 'te', 'et', 'h,', ' s', 'o ', 'th', 'at', ' h']\n",
            "\n",
            "processing file stories/038.txt\n",
            "Data size (chars) (document 37) 5129\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ', ', 'a ', 'ce', 'rt', 'ai', 'n ', 'ki', 'ng', ' w', 'as', ' h', 'un', 'ti', 'ng', ' i', 'n ', 'a ', 'gr', 'ea', 't ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'he', ' c', 'ha', 'se', 'd ', 'a ', 'wi', 'ld', ' b', 'ea', 'st', ' s', 'o ', 'ea', 'ge', 'rl']\n",
            "\n",
            "processing file stories/039.txt\n",
            "Data size (chars) (document 38) 3472\n",
            "Sample string ['\\tb', 'ri', 'ar', '-r', 'os', 'e\\n', '\\na', ' l', 'on', 'g ', 'ti', 'me', ' a', 'go', ' t', 'he', 're', ' w', 'er', 'e ', 'a ', 'ki', 'ng', ' a', 'nd', ' q', 'ue', 'en', ' w', 'ho', ' s', 'ai', 'd ', 'ev', 'er', 'y\\n', 'da', 'y,', ' a', 'h,', ' i', 'f ', 'on', 'ly', ' w', 'e ', 'ha', 'd ', 'a ', 'ch']\n",
            "\n",
            "processing file stories/040.txt\n",
            "Data size (chars) (document 39) 2490\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' f', 'or', 'es', 'te', 'r ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'hu', 'nt', ',\\n', 'an', 'd ', 'as', ' h', 'e ', 'en', 'te', 're', 'd ', 'it', ' h', 'e ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'of']\n",
            "\n",
            "processing file stories/041.txt\n",
            "Data size (chars) (document 40) 4273\n",
            "Sample string ['a ', 'ki', 'ng', ' h', 'ad', ' a', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'wa', 's ', 'be', 'au', 'ti', 'fu', 'l ', 'be', 'yo', 'nd', ' a', 'll', ' m', 'ea', 'su', 're', ',\\n', 'bu', 't ', 'so', ' p', 'ro', 'ud', ' a', 'nd', ' h', 'au', 'gh', 'ty', ' w', 'it', 'ha', 'l ', 'th', 'at', ' n', 'o ']\n",
            "\n",
            "processing file stories/042.txt\n",
            "Data size (chars) (document 41) 8327\n",
            "Sample string ['\\ts', 'no', 'w ', 'wh', 'it', 'e ', 'an', 'd ', 'th', 'e ', 'se', 've', 'n ', 'dw', 'ar', 'fs', '\\n\\n', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' i', 'n ', 'th', 'e ', 'mi', 'dd', 'le', ' o', 'f ', 'wi', 'nt', 'er', ', ', 'wh', 'en', ' t', 'he', ' f', 'la', 'ke', 's ', 'of', '\\ns', 'no', 'w ']\n",
            "\n",
            "processing file stories/043.txt\n",
            "Data size (chars) (document 42) 6128\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'th', 're', 'e ', 'br', 'ot', 'he', 'rs', ' w', 'ho', ' h', 'ad', ' f', 'al', 'le', 'n ', 'de', 'ep', 'er', ' a', 'nd', ' d', 'ee', 'pe', 'r ', 'in', 'to', '\\np', 'ov', 'er', 'ty', ', ', 'an', 'd ', 'at', ' l', 'as', 't ', 'th', 'ei', 'r ', 'ne', 'ed']\n",
            "\n",
            "processing file stories/044.txt\n",
            "Data size (chars) (document 43) 2819\n",
            "Sample string ['\\tr', 'um', 'pe', 'ls', 'ti', 'lt', 'sk', 'in', '\\n\\n', 'on', 'ce', ' t', 'he', 're', ' w', 'as', ' a', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'wa', 's ', 'po', 'or', ', ', 'bu', 't ', 'wh', 'o ', 'ha', 'd ', 'a ', 'be', 'au', 'ti', 'fu', 'l\\n', 'da', 'ug', 'ht', 'er', '. ', ' n', 'ow', ' i', 't ', 'ha']\n",
            "\n",
            "processing file stories/045.txt\n",
            "Data size (chars) (document 44) 3822\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' w', 'om', 'an', ' w', 'ho', ' w', 'as', ' a', ' r', 'ea', 'l ', 'wi', 'tc', 'h ', 'an', 'd ', 'ha', 'd ', 'tw', 'o\\n', 'da', 'ug', 'ht', 'er', 's,', ' o', 'ne', ' u', 'gl', 'y ', 'an', 'd ', 'wi', 'ck', 'ed', ', ']\n",
            "\n",
            "processing file stories/046.txt\n",
            "Data size (chars) (document 45) 7772\n",
            "Sample string ['in', ' o', 'ld', 'en', ' t', 'im', 'es', ' t', 'he', 're', ' w', 'as', ' a', ' k', 'in', 'g,', ' w', 'ho', ' h', 'ad', ' b', 'eh', 'in', 'd ', 'hi', 's ', 'pa', 'la', 'ce', ' a', '\\nb', 'ea', 'ut', 'if', 'ul', ' p', 'le', 'as', 'ur', 'e-', 'ga', 'rd', 'en', ' i', 'n ', 'wh', 'ic', 'h ', 'th', 'er']\n",
            "\n",
            "processing file stories/047.txt\n",
            "Data size (chars) (document 46) 22158\n",
            "Sample string ['th', 'er', 'e ', 'we', 're', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'tw', 'o ', 'br', 'ot', 'he', 'rs', ', ', 'on', 'e ', 'ri', 'ch', ' a', 'nd', ' t', 'he', ' o', 'th', 'er', '\\np', 'oo', 'r.', '  ', 'th', 'e ', 'ri', 'ch', ' o', 'ne', ' w', 'as', ' a', ' g', 'ol', 'ds', 'mi', 'th']\n",
            "\n",
            "processing file stories/048.txt\n",
            "Data size (chars) (document 47) 2169\n",
            "Sample string ['tw', 'o ', 'ki', 'ng', \"s'\", ' s', 'on', 's ', 'on', 'ce', ' w', 'en', 't ', 'ou', 't ', 'in', ' s', 'ea', 'rc', 'h ', 'of', ' a', 'dv', 'en', 'tu', 're', 's,', ' a', 'nd', ' f', 'el', 'l ', 'in', 'to', '\\na', ' w', 'il', 'd,', ' d', 'is', 'or', 'de', 'rl', 'y ', 'wa', 'y ', 'of', ' l', 'iv', 'in']\n",
            "\n",
            "processing file stories/049.txt\n",
            "Data size (chars) (document 48) 2822\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'of', ' w', 'ho', 'm ', 'tw', 'o\\n', 'we', 're', ' c', 'le', 've', 'r ', 'an', 'd ', 'wi', 'se', ', ', 'bu', 't ', 'th', 'e ', 'th', 'ir']\n",
            "\n",
            "processing file stories/050.txt\n",
            "Data size (chars) (document 49) 4034\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'th', 're', 'e ', 'so', 'ns', ', ', 'th', 'e ', 'yo', 'un', 'ge', 'st', ' o', 'f ', 'wh', 'om', ' w', 'as', ' c', 'al', 'le', 'd\\n', 'du', 'mm', 'li', 'ng', ', ', 'an', 'd ', 'wa', 's ', 'de', 'sp', 'is', 'ed', ', ', 'mo', 'ck']\n",
            "\n",
            "processing file stories/051.txt\n",
            "Data size (chars) (document 50) 5608\n",
            "Sample string ['\\ta', 'll', 'er', 'le', 'ir', 'au', 'h\\n', '\\nt', 'he', 're', ' w', 'as', ' o', 'nc', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' a', ' w', 'if', 'e ', 'wi', 'th', ' g', 'ol', 'de', 'n ', 'ha', 'ir', ',\\n', 'an', 'd ', 'sh', 'e ', 'wa', 's ', 'so', ' b', 'ea']\n",
            "\n",
            "processing file stories/052.txt\n",
            "Data size (chars) (document 51) 1287\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' a', 'nd', ' h', 'er', ' d', 'au', 'gh', 'te', 'r ', 'wh', 'o ', 'li', 've', 'd ', 'in', ' a', '\\np', 're', 'tt', 'y ', 'ga', 'rd', 'en', ' w', 'it', 'h ', 'ca', 'bb', 'ag', 'es', '. ', ' a', 'nd', ' a', ' l', 'it', 'tl', 'e ', 'ha']\n",
            "\n",
            "processing file stories/053.txt\n",
            "Data size (chars) (document 52) 2841\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n ', 'wh', 'o ', 'ha', 'd ', 'a ', 'br', 'id', 'e ', 'wh', 'om', ' h', 'e ', 'lo', 've', 'd ', 've', 'ry', ' m', 'uc', 'h.', '\\na', 'nd', ' w', 'he', 'n ', 'he', ' w', 'as', ' s', 'it', 'ti', 'ng', ' b', 'es', 'id', 'e ']\n",
            "\n",
            "processing file stories/054.txt\n",
            "Data size (chars) (document 53) 1922\n",
            "Sample string ['ha', 'ns', ' w', 'is', 'he', 'd ', 'to', ' p', 'ut', ' h', 'is', ' s', 'on', ' t', 'o ', 'le', 'ar', 'n ', 'a ', 'tr', 'ad', 'e,', ' s', 'o ', 'he', ' w', 'en', 't ', 'in', 'to', ' t', 'he', '\\nc', 'hu', 'rc', 'h ', 'an', 'd ', 'pr', 'ay', 'ed', ' t', 'o ', 'ou', 'r ', 'lo', 'rd', ' g', 'od', ' t']\n",
            "\n",
            "processing file stories/055.txt\n",
            "Data size (chars) (document 54) 2573\n",
            "Sample string ['a ', 'fa', 'th', 'er', ' o', 'nc', 'e ', 'ca', 'll', 'ed', ' h', 'is', ' t', 'hr', 'ee', ' s', 'on', 's ', 'be', 'fo', 're', ' h', 'im', ', ', 'an', 'd ', 'he', ' g', 'av', 'e ', 'to', ' t', 'he', '\\nf', 'ir', 'st', ' a', ' c', 'oc', 'k,', ' t', 'o ', 'th', 'e ', 'se', 'co', 'nd', ' a', ' s', 'cy']\n",
            "\n",
            "processing file stories/056.txt\n",
            "Data size (chars) (document 55) 5285\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' u', 'nd', 'er', 'st', 'oo', 'd ', 'al', 'l ', 'ki', 'nd', 's ', 'of', ' a', 'rt', 's.', '  ', 'he', ' s', 'er', 've', 'd ', 'in', '\\nw', 'ar', ', ', 'an', 'd ', 'be', 'ha', 've', 'd ', 'we', 'll', ' a', 'nd', ' b', 'ra', 've']\n",
            "\n",
            "processing file stories/057.txt\n",
            "Data size (chars) (document 56) 971\n",
            "Sample string ['th', 'e ', 'sh', 'e-', 'wo', 'lf', ' b', 'ro', 'ug', 'ht', ' i', 'nt', 'o ', 'th', 'e ', 'wo', 'rl', 'd ', 'a ', 'yo', 'un', 'g ', 'on', 'e,', ' a', 'nd', ' i', 'nv', 'it', 'ed', ' t', 'he', ' f', 'ox', '\\nt', 'o ', 'be', ' g', 'od', 'fa', 'th', 'er', '. ', ' a', 'ft', 'er', ' a', 'll', ', ', 'he']\n",
            "\n",
            "processing file stories/058.txt\n",
            "Data size (chars) (document 57) 4538\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' t', 'o ', 'wh', 'om', ' g', 'od', ' h', 'ad', ' g', 'iv', 'en', ' n', 'o ', 'ch', 'il', 'dr', 'en', '.\\n', 'ev', 'er', 'y ', 'mo', 'rn', 'in', 'g ', 'sh', 'e ', 'we', 'nt', ' i', 'nt', 'o ', 'th']\n",
            "\n",
            "processing file stories/059.txt\n",
            "Data size (chars) (document 58) 636\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' v', 'er', 'y ', 'ol', 'd ', 'ma', 'n,', ' w', 'ho', 'se', ' e', 'ye', 's ', 'ha', 'd ', 'be', 'co', 'me', ' d', 'im', ', ', 'hi', 's ', 'ea', 'rs', '\\nd', 'ul', 'l ', 'of', ' h', 'ea', 'ri', 'ng', ', ', 'hi', 's ', 'kn', 'ee', 's ', 'tr', 'em', 'bl']\n",
            "\n",
            "processing file stories/060.txt\n",
            "Data size (chars) (document 59) 786\n",
            "Sample string ['a ', 'li', 'tt', 'le', ' b', 'ro', 'th', 'er', ' a', 'nd', ' s', 'is', 'te', 'r ', 'we', 're', ' o', 'nc', 'e ', 'pl', 'ay', 'in', 'g ', 'by', ' a', ' w', 'el', 'l,', ' a', 'nd', ' w', 'hi', 'le', '\\nt', 'he', 'y ', 'we', 're', ' t', 'hu', 's ', 'pl', 'ay', 'in', 'g,', ' t', 'he', 'y ', 'bo', 'th']\n",
            "\n",
            "processing file stories/061.txt\n",
            "Data size (chars) (document 60) 10687\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'e ', 'up', 'on', ' a', ' t', 'im', 'e ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' w', 'he', 'n ', 'it', ' c', 'am', 'e ', 'to', ' a', 'n ', 'en', 'd,', '\\nm', 'an', 'y ', 'so', 'ld', 'ie', 'rs', ' w', 'er', 'e ', 'di', 'sc', 'ha', 'rg', 'ed', '. ', ' t']\n",
            "\n",
            "processing file stories/062.txt\n",
            "Data size (chars) (document 61) 5105\n",
            "Sample string ['ha', 'ns', ' h', 'ad', ' s', 'er', 've', 'd ', 'hi', 's ', 'ma', 'st', 'er', ' f', 'or', ' s', 'ev', 'en', ' y', 'ea', 'rs', ', ', 'so', ' h', 'e ', 'sa', 'id', ' t', 'o ', 'hi', 'm,', '\\nm', 'as', 'te', 'r,', ' m', 'y ', 'ti', 'me', ' i', 's ', 'up', ', ', 'no', 'w ', 'i ', 'sh', 'ou', 'ld', ' b']\n",
            "\n",
            "processing file stories/063.txt\n",
            "Data size (chars) (document 62) 1127\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' y', 'ou', 'ng', ' p', 'ea', 'sa', 'nt', ' n', 'am', 'ed', ' h', 'an', 's,', ' w', 'ho', 'se', ' u', 'nc', 'le', '\\nw', 'an', 'te', 'd ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'a ', 'ri', 'ch', ' w', 'if', 'e.', '  ']\n",
            "\n",
            "processing file stories/064.txt\n",
            "Data size (chars) (document 63) 4981\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'an', 'd ', 'a ', 'po', 'or', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' a', '\\nl', 'it', 'tl', 'e ', 'co', 'tt', 'ag', 'e,', ' a', 'nd', ' w', 'ho', ' e', 'ar', 'ne', 'd ', 'th', 'ei']\n",
            "\n",
            "processing file stories/065.txt\n",
            "Data size (chars) (document 64) 6006\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' m', 'an', ' w', 'ho', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'se', 't ', 'ou', 't ', 'on', ' a', ' l', 'on', 'g\\n', 'jo', 'ur', 'ne', 'y,', ' a', 'nd', ' o', 'n ', 'pa', 'rt', 'in', 'g ', 'he', ' a', 'sk', 'ed']\n",
            "\n",
            "processing file stories/066.txt\n",
            "Data size (chars) (document 65) 5900\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', 'n ', 'ol', 'd ', 'qu', 'ee', 'n ', 'wh', 'os', 'e ', 'hu', 'sb', 'an', 'd ', 'ha', 'd ', 'be', 'en', ' d', 'ea', 'd\\n', 'fo', 'r ', 'ma', 'ny', ' y', 'ea', 'rs', ', ', 'an', 'd ', 'sh', 'e ', 'ha', 'd ', 'a ', 'be']\n",
            "\n",
            "processing file stories/067.txt\n",
            "Data size (chars) (document 66) 7837\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' c', 'ou', 'nt', 'ry', 'ma', 'n ', 'ha', 'd ', 'a ', 'so', 'n ', 'wh', 'o ', 'wa', 's ', 'as', ' b', 'ig', ' a', 's ', 'a ', 'th', 'um', 'b,', '\\na', 'nd', ' d', 'id', ' n', 'ot', ' b', 'ec', 'om', 'e ', 'an', 'y ', 'bi', 'gg', 'er', ', ', 'an']\n",
            "\n",
            "processing file stories/068.txt\n",
            "Data size (chars) (document 67) 4717\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' r', 'ic', 'h ', 'ki', 'ng', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'wh', 'o\\n', 'da', 'il', 'y ', 'we', 'nt', ' t', 'o ', 'wa', 'lk', ' i', 'n ', 'th', 'e ', 'pa', 'la', 'ce']\n",
            "\n",
            "processing file stories/069.txt\n",
            "Data size (chars) (document 68) 6233\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'ce', 'rt', 'ai', 'n ', 'me', 'rc', 'ha', 'nt', ' w', 'ho', ' h', 'ad', ' t', 'wo', ' c', 'hi', 'ld', 're', 'n,', ' a', ' b', 'oy', ' a', 'nd', ' a', ' g', 'ir', 'l,', '\\nt', 'he', 'y ', 'we', 're', ' b', 'ot', 'h ', 'yo', 'un', 'g,', ' a', 'nd', ' c', 'ou', 'ld']\n",
            "\n",
            "processing file stories/070.txt\n",
            "Data size (chars) (document 69) 5664\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' q', 'ue', 'en', ' w', 'ho', ' h', 'ad', ' a', ' l', 'it', 'tl', 'e ', 'da', 'ug', 'ht', 'er', ' w', 'ho', '\\nw', 'as', ' s', 'ti', 'll', ' s', 'o ', 'yo', 'un', 'g ', 'th', 'at', ' s', 'he', ' h', 'ad', ' t', 'o ']\n",
            "\n",
            "processing file stories/071.txt\n",
            "Data size (chars) (document 70) 3569\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'pe', 'as', 'an', 't ', 'wh', 'o ', 'ha', 'd ', 'no', ' l', 'an', 'd,', ' b', 'ut', ' o', 'nl', 'y ', 'a ', 'sm', 'al', 'l\\n', 'ho', 'us', 'e,', ' a', 'nd', ' o', 'ne', ' d', 'au', 'gh', 'te', 'r.', '  ', 'th', 'en', ' s', 'ai', 'd ']\n",
            "\n",
            "processing file stories/072.txt\n",
            "Data size (chars) (document 71) 3793\n",
            "Sample string ['ab', 'ou', 't ', 'a ', 'th', 'ou', 'sa', 'nd', ' o', 'r ', 'mo', 're', ' y', 'ea', 'rs', ' a', 'go', ', ', 'th', 'er', 'e ', 'we', 're', ' i', 'n ', 'th', 'is', '\\nc', 'ou', 'nt', 'ry', ' n', 'ot', 'hi', 'ng', ' b', 'ut', ' s', 'ma', 'll', ' k', 'in', 'gs', ', ', 'an', 'd ', 'on', 'e ', 'of', ' t']\n",
            "\n",
            "processing file stories/073.txt\n",
            "Data size (chars) (document 72) 5980\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'an', ' i', 'll', 'ne', 'ss', ', ', 'an', 'd ', 'no', ' o', 'ne', ' b', 'el', 'ie', 've', 'd ', 'th', 'at', ' h', 'e\\n', 'wo', 'ul', 'd ', 'co', 'me', ' o', 'ut', ' o', 'f ', 'it', ' w', 'it', 'h ', 'hi', 's ']\n",
            "\n",
            "processing file stories/074.txt\n",
            "Data size (chars) (document 73) 4518\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'wo', 'od', 'cu', 'tt', 'er', ' w', 'ho', ' t', 'oi', 'le', 'd ', 'fr', 'om', ' e', 'ar', 'ly', '\\nm', 'or', 'ni', 'ng', ' t', 'il', 'l ', 'la', 'te', ' a', 't ', 'ni', 'gh', 't.', '  ', 'wh', 'en', ' a', 't ', 'la', 'st', ' h', 'e ']\n",
            "\n",
            "processing file stories/075.txt\n",
            "Data size (chars) (document 74) 3247\n",
            "Sample string ['a ', 'di', 'sc', 'ha', 'rg', 'ed', ' s', 'ol', 'di', 'er', ' h', 'ad', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'li', 've', ' o', 'n,', ' a', 'nd', ' d', 'id', ' n', 'ot', ' k', 'no', 'w ', 'ho', 'w ', 'to', '\\nm', 'ak', 'e ', 'hi', 's ', 'wa', 'y.', '  ', 'so', ' h', 'e ', 'we', 'nt', ' o', 'ut', ' i']\n",
            "\n",
            "processing file stories/076.txt\n",
            "Data size (chars) (document 75) 5130\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'en', 'li', 'st', 'ed', ' a', 's ', 'a ', 'so', 'ld', 'ie', 'r,', ' c', 'on', 'du', 'ct', 'ed', '\\nh', 'im', 'se', 'lf', ' b', 'ra', 've', 'ly', ', ', 'an', 'd ', 'wa', 's ', 'al', 'wa', 'ys', ' t']\n",
            "\n",
            "processing file stories/077.txt\n",
            "Data size (chars) (document 76) 2401\n",
            "Sample string ['on', 'ce', ' i', 'n ', 'su', 'mm', 'er', '-t', 'im', 'e ', 'th', 'e ', 'be', 'ar', ' a', 'nd', ' t', 'he', ' w', 'ol', 'f ', 'we', 're', ' w', 'al', 'ki', 'ng', ' i', 'n ', 'th', 'e ', 'fo', 're', 'st', ',\\n', 'an', 'd ', 'th', 'e ', 'be', 'ar', ' h', 'ea', 'rd', ' a', ' b', 'ir', 'd ', 'si', 'ng']\n",
            "\n",
            "processing file stories/078.txt\n",
            "Data size (chars) (document 77) 624\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'po', 'or', ' b', 'ut', ' g', 'oo', 'd ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'wh', 'o ', 'li', 've', 'd ', 'al', 'on', 'e ', 'wi', 'th', ' h', 'er', '\\nm', 'ot', 'he', 'r,', ' a', 'nd', ' t', 'he', 'y ', 'no', ' l', 'on', 'ge', 'r ', 'ha', 'd ', 'an', 'yt', 'hi']\n",
            "\n",
            "processing file stories/079.txt\n",
            "Data size (chars) (document 78) 3991\n",
            "Sample string ['on', 'e ', 'da', 'y ', 'a ', 'pe', 'as', 'an', 't ', 'to', 'ok', ' h', 'is', ' g', 'oo', 'd ', 'ha', 'ze', 'l-', 'st', 'ic', 'k ', 'ou', 't ', 'of', ' t', 'he', ' c', 'or', 'ne', 'r\\n', 'an', 'd ', 'sa', 'id', ' t', 'o ', 'hi', 's ', 'wi', 'fe', ', ', 'tr', 'in', 'a,', ' i', ' a', 'm ', 'go', 'in']\n",
            "\n",
            "processing file stories/080.txt\n",
            "Data size (chars) (document 79) 1426\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' l', 'it', 'tl', 'e ', 'ch', 'il', 'd ', 'wh', 'os', 'e ', 'mo', 'th', 'er', ' g', 'av', 'e ', 'he', 'r ', 'ev', 'er', 'y\\n', 'af', 'te', 'rn', 'oo', 'n ', 'a ', 'sm', 'al', 'l ', 'bo', 'wl', ' o', 'f ', 'mi', 'lk', ' a', 'nd', ' b', 're', 'ad', ', ']\n",
            "\n",
            "processing file stories/081.txt\n",
            "Data size (chars) (document 80) 3574\n",
            "Sample string ['in', ' a', ' c', 'er', 'ta', 'in', ' m', 'il', 'l ', 'li', 've', 'd ', 'an', ' o', 'ld', ' m', 'il', 'le', 'r ', 'wh', 'o ', 'ha', 'd ', 'ne', 'it', 'he', 'r ', 'wi', 'fe', ' n', 'or', ' c', 'hi', 'ld', ',\\n', 'an', 'd ', 'th', 're', 'e ', 'ap', 'pr', 'en', 'ti', 'ce', 's ', 'se', 'rv', 'ed', ' u']\n",
            "\n",
            "processing file stories/082.txt\n",
            "Data size (chars) (document 81) 10822\n",
            "Sample string ['hi', 'll', ' a', 'nd', ' v', 'al', 'e ', 'do', ' n', 'ot', ' m', 'ee', 't,', ' b', 'ut', ' t', 'he', ' c', 'hi', 'ld', 're', 'n ', 'of', ' m', 'en', ' d', 'o,', ' g', 'oo', 'd ', 'an', 'd ', 'ba', 'd.', '\\ni', 'n ', 'th', 'is', ' w', 'ay', ' a', ' s', 'ho', 'em', 'ak', 'er', ' a', 'nd', ' a', ' t']\n",
            "\n",
            "processing file stories/083.txt\n",
            "Data size (chars) (document 82) 5480\n",
            "Sample string ['\\th', 'an', 's ', 'th', 'e ', 'he', 'dg', 'eh', 'og', '\\n\\n', 'th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' c', 'ou', 'nt', 'ry', ' m', 'an', ' w', 'ho', ' h', 'ad', ' m', 'on', 'ey', ' a', 'nd', ' l', 'an', 'd ', 'in', ' p', 'le', 'nt', 'y,', ' b', 'ut', '\\nh', 'ow', 'ev', 'er', ' r', 'ic', 'h ']\n",
            "\n",
            "processing file stories/084.txt\n",
            "Data size (chars) (document 83) 658\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'ot', 'he', 'r ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' o', 'f ', 'se', 've', 'n ', 'ye', 'ar', 's ', 'ol', 'd,', ' w', 'ho', '\\nw', 'as', ' s', 'o ', 'ha', 'nd', 'so', 'me', ' a', 'nd', ' l', 'ov', 'ab', 'le', ' t', 'ha']\n",
            "\n",
            "processing file stories/085.txt\n",
            "Data size (chars) (document 84) 5989\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' f', 'el', 'lo', 'w ', 'wh', 'o ', 'ha', 'd ', 'le', 'ar', 'nt', ' t', 'he', ' t', 'ra', 'de', ' o', 'f ', 'lo', 'ck', 'sm', 'it', 'h,', '\\na', 'nd', ' t', 'ol', 'd ', 'hi', 's ', 'fa', 'th', 'er', ' h', 'e ', 'wo', 'ul', 'd ', 'no']\n",
            "\n",
            "processing file stories/086.txt\n",
            "Data size (chars) (document 85) 8758\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' k', 'in', 'g ', 'wh', 'o ', 'ha', 'd ', 'a ', 'li', 'tt', 'le', ' b', 'oy', ' i', 'n ', 'wh', 'os', 'e ', 'st', 'ar', 's\\n', 'it', ' h', 'ad', ' b', 'ee', 'n ', 'fo', 're', 'to', 'ld', ' t', 'ha', 't ', 'he', ' s']\n",
            "\n",
            "processing file stories/087.txt\n",
            "Data size (chars) (document 86) 3109\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' a', ' p', 'ri', 'nc', 'es', 's ', 'wh', 'o ', 'wa', 's ', 'ex', 'tr', 'em', 'el', 'y ', 'pr', 'ou', 'd.', ' i', 'f ', 'a\\n', 'wo', 'oe', 'r ', 'ca', 'me', ' s', 'he', ' g', 'av', 'e ', 'hi', 'm ', 'so', 'me', ' r', 'id']\n",
            "\n",
            "processing file stories/088.txt\n",
            "Data size (chars) (document 87) 1365\n",
            "Sample string ['a ', 'ta', 'il', 'or', \"'s\", ' a', 'pp', 're', 'nt', 'ic', 'e ', 'wa', 's ', 'tr', 'av', 'el', 'in', 'g ', 'ab', 'ou', 't ', 'th', 'e ', 'wo', 'rl', 'd ', 'in', ' s', 'ea', 'rc', 'h ', 'of', '\\nw', 'or', 'k,', ' a', 'nd', ' a', 't ', 'on', 'e ', 'ti', 'me', ' h', 'e ', 'co', 'ul', 'd ', 'fi', 'nd']\n",
            "\n",
            "processing file stories/089.txt\n",
            "Data size (chars) (document 88) 4538\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' o', 'n ', 'a ', 'ti', 'me', ' a', ' s', 'ol', 'di', 'er', ' w', 'ho', ' f', 'or', ' m', 'an', 'y ', 'ye', 'ar', 's ', 'ha', 'd ', 'se', 'rv', 'ed', ' t', 'he', '\\nk', 'in', 'g ', 'fa', 'it', 'hf', 'ul', 'ly', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he', ' w']\n",
            "\n",
            "processing file stories/090.txt\n",
            "Data size (chars) (document 89) 345\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' t', 'he', 're', ' w', 'as', ' a', ' c', 'hi', 'ld', ' w', 'ho', ' w', 'as', ' w', 'il', 'lf', 'ul', ', ', 'an', 'd ', 'wo', 'ul', 'd ', 'no', 't ', 'do', '\\nw', 'ha', 't ', 'he', 'r ', 'mo', 'th', 'er', ' w', 'is', 'he', 'd.', '  ', 'fo', 'r ', 'th']\n",
            "\n",
            "processing file stories/091.txt\n",
            "Data size (chars) (document 90) 5460\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' k', 'in', \"g'\", 's ', 'so', 'n,', ' w', 'ho', ' w', 'as', ' n', 'o ', 'lo', 'ng', 'er', ' c', 'on', 'te', 'nt', ' t', 'o ', 'st', 'ay', ' a', 't\\n', 'ho', 'me', ' i', 'n ', 'hi', 's ', 'fa', 'th', 'er', \"'s\", ' h', 'ou', 'se', ', ', 'an', 'd ', 'as']\n",
            "\n",
            "processing file stories/092.txt\n",
            "Data size (chars) (document 91) 6854\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' y', 'ou', 'ng', ' h', 'un', 'ts', 'ma', 'n ', 'wh', 'o ', 'we', 'nt', ' i', 'nt', 'o ', 'th', 'e ', 'fo', 're', 'st', ' t', 'o ', 'li', 'e ', 'in', '\\nw', 'ai', 't.', '  ', 'he', ' h', 'ad', ' a', ' f', 're', 'sh', ' a', 'nd', ' j', 'oy', 'ou', 's ']\n",
            "\n",
            "processing file stories/093.txt\n",
            "Data size (chars) (document 92) 2314\n",
            "Sample string ['a ', 'po', 'or', ' s', 'er', 'va', 'nt', '-g', 'ir', 'l ', 'wa', 's ', 'on', 'ce', ' t', 'ra', 've', 'li', 'ng', ' w', 'it', 'h ', 'th', 'e ', 'fa', 'mi', 'ly', ' w', 'it', 'h ', 'wh', 'ic', 'h ', 'sh', 'e\\n', 'wa', 's ', 'in', ' s', 'er', 'vi', 'ce', ', ', 'th', 'ro', 'ug', 'h ', 'a ', 'gr', 'ea']\n",
            "\n",
            "processing file stories/094.txt\n",
            "Data size (chars) (document 93) 1706\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' m', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' s', 'on', 's,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' e', 'ls', 'e ', 'in', ' t', 'he', '\\nw', 'or', 'ld', ' b', 'ut', ' t', 'he', ' h', 'ou', 'se', ' i', 'n ', 'wh', 'ic', 'h ', 'he', ' l', 'iv']\n",
            "\n",
            "processing file stories/095.txt\n",
            "Data size (chars) (document 94) 3229\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'a ', 'gr', 'ea', 't ', 'wa', 'r,', ' a', 'nd', ' t', 'he', ' k', 'in', 'g ', 'ha', 'd ', 'ma', 'ny', ' s', 'ol', 'di', 'er', 's,', ' b', 'ut', ' g', 'av', 'e ', 'th', 'em', '\\ns', 'ma', 'll', ' p', 'ay', ', ', 'so', ' s', 'ma', 'll', ' t', 'ha', 't ', 'th', 'ey', ' c']\n",
            "\n",
            "processing file stories/096.txt\n",
            "Data size (chars) (document 95) 4954\n",
            "Sample string ['on', 'ce', ' u', 'po', 'n ', 'a ', 'ti', 'me', ' l', 'iv', 'ed', ' a', ' m', 'an', ' a', 'nd', ' a', ' w', 'om', 'an', ' w', 'ho', ' s', 'o ', 'lo', 'ng', ' a', 's ', 'th', 'ey', ' w', 'er', 'e\\n', 'ri', 'ch', ' h', 'ad', ' n', 'o ', 'ch', 'il', 'dr', 'en', ', ', 'bu', 't ', 'wh', 'en', ' t', 'he']\n",
            "\n",
            "processing file stories/097.txt\n",
            "Data size (chars) (document 96) 5732\n",
            "Sample string ['in', ' t', 'he', ' d', 'ay', 's ', 'wh', 'en', ' w', 'is', 'hi', 'ng', ' w', 'as', ' s', 'ti', 'll', ' o', 'f ', 'so', 'me', ' u', 'se', ', ', 'a ', 'ki', 'ng', \"'s\", ' s', 'on', ' w', 'as', '\\nb', 'ew', 'it', 'ch', 'ed', ' b', 'y ', 'an', ' o', 'ld', ' w', 'it', 'ch', ', ', 'an', 'd ', 'sh', 'ut']\n",
            "\n",
            "processing file stories/098.txt\n",
            "Data size (chars) (document 97) 4334\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' p', 'oo', 'r ', 'ma', 'n ', 'wh', 'o ', 'ha', 'd ', 'fo', 'ur', ' s', 'on', 's,', ' a', 'nd', ' w', 'he', 'n ', 'th', 'ey', ' w', 'er', 'e ', 'gr', 'ow', 'n\\n', 'up', ', ', 'he', ' s', 'ai', 'd ', 'to', ' t', 'he', 'm,', ' \"', 'my', ' d', 'ea', 'r ']\n",
            "\n",
            "processing file stories/099.txt\n",
            "Data size (chars) (document 98) 7090\n",
            "Sample string ['th', 'er', 'e ', 'wa', 's ', 'on', 'ce', ' a', ' w', 'om', 'an', ' w', 'ho', ' h', 'ad', ' t', 'hr', 'ee', ' d', 'au', 'gh', 'te', 'rs', ', ', 'th', 'e ', 'el', 'de', 'st', ' o', 'f ', 'wh', 'om', '\\nw', 'as', ' c', 'al', 'le', 'd ', 'on', 'e-', 'ey', 'e,', ' b', 'ec', 'au', 'se', ' s', 'he', ' h']\n",
            "\n",
            "processing file stories/100.txt\n",
            "Data size (chars) (document 99) 1007\n",
            "Sample string ['\"g', 'oo', 'd-', 'da', 'y,', ' f', 'at', 'he', 'r ', 'ho', 'll', 'en', 'th', 'e.', '\" ', '\"m', 'an', 'y ', 'th', 'an', 'ks', ', ', 'pi', 'f-', 'pa', 'f-', 'po', 'lt', 'ri', 'e.', '\" ', '\"m', 'ay', ' i', '\\nb', 'e ', 'al', 'lo', 'we', 'd ', 'to', ' h', 'av', 'e ', 'yo', 'ur', ' d', 'au', 'gh', 'te']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdD9a58852Ks",
        "outputId": "f25d0ce1-064b-4d99-a326-0a72a0939896"
      },
      "source": [
        "def build_dataset(documents):\n",
        "    chars = []\n",
        "    # list of lists\n",
        "    data_list = []\n",
        "\n",
        "    for d in documents:\n",
        "        chars.extend(d)\n",
        "    print('%d character found.' % len(chars))\n",
        "\n",
        "    count = []\n",
        "    # bigrams sorted by their frequency\n",
        "    count.extend(collections.Counter(chars).most_common())\n",
        "\n",
        "    # Create dict map word to id by given the current length of the dictionary\n",
        "    # UNK is for two rare word\n",
        "    dictionary = dict({'UNK': 0})\n",
        "    for char, c in count:\n",
        "        # Only add if its frequency is more than 10\n",
        "        if c > 10:\n",
        "            dictionary[char] = len(dictionary)\n",
        "    unk_count = 0\n",
        "    # replace word with id of word\n",
        "    for d in documents:\n",
        "        data = list()\n",
        "        for char in d:\n",
        "            # if word in dictionary use the id of word\n",
        "            # otherwise use id of UNK\n",
        "            if char in dictionary:\n",
        "                index = dictionary[char]\n",
        "            else:\n",
        "                index = dictionary['UNK']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        data_list.append(data)\n",
        "\n",
        "    # dict map id to word\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data_list, count, dictionary, reverse_dictionary\n",
        "\n",
        "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Least common words (+UNK)', count[-15:])\n",
        "print('Sample data', data_list[0][:10])\n",
        "print('Sample data', data_list[1][:10])\n",
        "print('Vocabulary: ',len(dictionary))\n",
        "vocabulary_size = len(dictionary)\n",
        "del documents  # To reduce memory."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "449177 character found.\n",
            "Most common words (+UNK) [('e ', 15229), ('he', 15164), (' t', 13443), ('th', 13076), ('d ', 10687)]\n",
            "Least common words (+UNK) [('bj', 1), ('ii', 1), ('i?', 1), ('z ', 1), ('c.', 1), ('\"k', 1), ('pw', 1), ('f?', 1), (' z', 1), ('xq', 1), ('nm', 1), ('m?', 1), ('\\t\"', 1), ('\\tw', 1), ('tz', 1)]\n",
            "Sample data [15, 28, 86, 23, 3, 95, 74, 11, 2, 16]\n",
            "Sample data [22, 156, 25, 37, 82, 185, 43, 9, 90, 19]\n",
            "Vocabulary:  544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrOJOspI52Kt",
        "outputId": "0ed82f4c-6506-41f4-a74a-06ab021f9d69"
      },
      "source": [
        "datasets = [tf.data.Dataset.from_tensor_slices(data) for data in data_list]\n",
        "datasets\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>,\n",
              " <DatasetV1Adapter shapes: (), types: tf.int32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU_wXw2z52Kt",
        "outputId": "62944cef-f587-47bb-8f7c-3b82405a57c8"
      },
      "source": [
        "for i in datasets[1].take(5):\n",
        "    print(reverse_dictionary[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ha\n",
            "rd\n",
            " b\n",
            "y \n",
            "a \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syFHl94e52Kt",
        "outputId": "204a7ed6-4b48-43f8-bbca-596dd3146a11"
      },
      "source": [
        "\n",
        "seq_length = 64\n",
        "# examples_per_epoch = vocabulary_size // (seq_length + 1)\n",
        "\n",
        "datasets_sequences = [dataset.batch(seq_length + 1, drop_remainder=True) for dataset in datasets]\n",
        "\n",
        "for item in datasets_sequences[1].take(5):\n",
        "    for c in item.numpy():\n",
        "        print(reverse_dictionary[c], end='')\n",
        "    print(\"\\n\" + \"-\" * 24)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hard by a great forest dwelt a wood-cutter with his wife, who had an\n",
            "only child, a little girl three years old.  they were so poor\n",
            "------------------------\n",
            ",\n",
            "however, that they no longer had daily bread, and did not know how to\n",
            "get food for her.  one morning the wood-cutter went out so\n",
            "------------------------\n",
            "rrowfully\n",
            "to his work in the forest, and while he was cutting wood, suddenly\n",
            "there stood before him a tall and beautiful woman wit\n",
            "------------------------\n",
            "h a crown of\n",
            "shining stars on her head, who said to him 'i am the virgin mary,\n",
            "mother of the child jesus. you are poor and needy, \n",
            "------------------------\n",
            "bring your child\n",
            "to me, i will take her with me and be her mother, and care for her.'\n",
            "the wood-cutter obeyed, brought his child, a\n",
            "------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoVy7Ugo52Ku",
        "outputId": "c6b35c3d-f85f-4025-d0f5-95b59ce99e0a"
      },
      "source": [
        "# Map text to input and target (both input and target have\n",
        "# the same seq_length but target is shifted to right one character)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1] # take all except the last character\n",
        "    target_text = chunk[1:] # take all except the first character\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "datasets = [datasets_sequence.map(split_input_target) for datasets_sequence in datasets_sequences]\n",
        "datasets\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function split_input_target at 0x7f7bbc730268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <function split_input_target at 0x7f7bbc730268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>,\n",
              " <DatasetV1Adapter shapes: ((64,), (64,)), types: (tf.int32, tf.int32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LZK1RZ552Ku",
        "outputId": "4fcfa04d-4c12-45bc-b0e2-f32bed10564a"
      },
      "source": [
        "for input_exp, target_exp in datasets[0].take(1):\n",
        "    print(\"Input data: \")\n",
        "    for i in input_exp.numpy():\n",
        "        print(reverse_dictionary[i], end='')\n",
        "    print(\"\\nTarget data: \")\n",
        "    for t in target_exp.numpy():\n",
        "        print(reverse_dictionary[t], end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data: \n",
            "in olden times when wishing still helped one, there lived a king\n",
            "whose daughters were all beautiful, but the youngest was so bea\n",
            "Target data: \n",
            " olden times when wishing still helped one, there lived a king\n",
            "whose daughters were all beautiful, but the youngest was so beaut"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAUCaG5052Ku",
        "outputId": "126dec88-d72b-481c-cf7f-ec010598a461"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "embedding_dim = 64\n",
        "\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "datasets = [dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(1) for dataset in datasets]\n",
        "\n",
        "len(datasets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ph5YIUaK52Kv"
      },
      "source": [
        "# class MyModel(tf.keras.Model):\n",
        "#   def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "#     super().__init__(self)\n",
        "#     self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "#     self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "#                                    return_sequences=True,\n",
        "#                                    return_state=True)\n",
        "#     self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "#\n",
        "#   def __call__(self, inputs, states=None, return_state=False, training=False):\n",
        "#     x = inputs\n",
        "#     x = self.embedding(x)\n",
        "#     print(x)\n",
        "#     if states is None:\n",
        "#       states = self.gru.get_initial_state(x)\n",
        "#     x, states = self.gru(x, initial_state=states, training=training)\n",
        "#     x = self.dense(x, training=training)\n",
        "#\n",
        "#     if return_state:\n",
        "#       return x, states\n",
        "#     else:\n",
        "#       return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CeS5NjAj52Kv"
      },
      "source": [
        "# my_model = MyModel(vocab_size=vocabulary_size, embedding_dim=embedding_dim, rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-gMnVuz52Kv",
        "outputId": "5f3cc816-61c0-4c32-df99-624db602954c"
      },
      "source": [
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, None)),\n",
        "        tf.keras.layers.GRU(units=rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 64)            34816     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3345408   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 544)           557600    \n",
            "=================================================================\n",
            "Total params: 3,937,824\n",
            "Trainable params: 3,937,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8etbNNU52Kw",
        "outputId": "cdc44716-b158-4055-f975-27adb0e3c9ff"
      },
      "source": [
        "\"\"\"\n",
        "    For each character the model looks up the\n",
        "    embedding, runs the GRU one timestep with\n",
        "    the embedding as input, and applies the dense\n",
        "    layer to generate logits predicting the log-likelihood of the next character:\n",
        "\"\"\"\n",
        "for input_example_batch, target_example_batch in datasets[1].take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "# my_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 64, 544) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGKK8uZB52Kx",
        "outputId": "7a3ca12b-18c0-4d65-a4a2-61a3db6cfffc"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 92, 312, 216, 251, 252,  28, 116, 181, 187, 304,  24, 220,  74,\n",
              "       126, 393, 514, 468, 337, 477, 379,  90, 357, 539, 439,  95, 173,\n",
              "       132, 431, 525, 243, 414, 171, 366, 429, 425,  72,  75,  30, 180,\n",
              "       248, 289, 412, 325, 439, 341, 313, 284, 416, 358,  54,   1, 395,\n",
              "       380, 213, 360,  16, 403,  56,   7, 144, 514, 389, 534, 393])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv90-6eT52Kx",
        "outputId": "f1fdc3cf-0b61-4239-8944-064a518e02f9"
      },
      "source": [
        "def loss_sparse(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss_sparse(target_example_batch, example_batch_predictions)\n",
        "example_batch_loss.numpy().mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.299084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dJBnGqjW52Kx"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "def train_step(inp, target):\n",
        "    with tf.GradientTape() as g:\n",
        "        predictions = model(inp)\n",
        "        loss = tf.reduce_mean(loss_sparse(target, predictions))\n",
        "\n",
        "    gradients = g.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj0BqxXN52Kx",
        "outputId": "2a2257f8-2997-419a-98a6-458b5849e77f"
      },
      "source": [
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "checkpoint_dir = './stories_checkpoint'\n",
        "\n",
        "checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_predix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    loss = 0\n",
        "    # reset hidden state\n",
        "    model.reset_states()\n",
        "\n",
        "    for (di, dataset) in enumerate(datasets):\n",
        "\n",
        "      for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "          loss = train_step(inp, target)\n",
        "\n",
        "          if di % 10 == 0:\n",
        "              print('Epoch {} Batch {} Loss {}'.format(epoch + 1, di, loss))\n",
        "\n",
        "      if (epoch + 1) % 5 == 0:\n",
        "          model.save_weights(checkpoint_predix.format(epoch=epoch))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "\n",
        "model.save_weights(checkpoint_predix.format(epoch=epoch))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 40 Loss 2.9742891788482666\n",
            "Epoch 1 Batch 50 Loss 2.8155484199523926\n",
            "Epoch 1 Batch 60 Loss 2.902259111404419\n",
            "Epoch 1 Batch 60 Loss 2.848997116088867\n",
            "Epoch 1 Batch 90 Loss 2.756791114807129\n",
            "Epoch 1 Loss 2.8786\n",
            "Time taken for 1 epoch 37.87354111671448 sec\n",
            "\n",
            "Epoch 2 Batch 40 Loss 2.8236641883850098\n",
            "Epoch 2 Batch 50 Loss 2.680950164794922\n",
            "Epoch 2 Batch 60 Loss 2.694181203842163\n",
            "Epoch 2 Batch 60 Loss 2.6612865924835205\n",
            "Epoch 2 Batch 90 Loss 2.621386766433716\n",
            "Epoch 2 Loss 2.6785\n",
            "Time taken for 1 epoch 37.910555839538574 sec\n",
            "\n",
            "Epoch 3 Batch 40 Loss 2.681704044342041\n",
            "Epoch 3 Batch 50 Loss 2.546788215637207\n",
            "Epoch 3 Batch 60 Loss 2.5215697288513184\n",
            "Epoch 3 Batch 60 Loss 2.493364095687866\n",
            "Epoch 3 Batch 90 Loss 2.490720272064209\n",
            "Epoch 3 Loss 2.4949\n",
            "Time taken for 1 epoch 38.14566254615784 sec\n",
            "\n",
            "Epoch 4 Batch 40 Loss 2.5244507789611816\n",
            "Epoch 4 Batch 50 Loss 2.4008736610412598\n",
            "Epoch 4 Batch 60 Loss 2.351067543029785\n",
            "Epoch 4 Batch 60 Loss 2.326223611831665\n",
            "Epoch 4 Batch 90 Loss 2.343366861343384\n",
            "Epoch 4 Loss 2.3012\n",
            "Time taken for 1 epoch 37.59836149215698 sec\n",
            "\n",
            "Epoch 5 Batch 40 Loss 2.3591432571411133\n",
            "Epoch 5 Batch 50 Loss 2.23590087890625\n",
            "Epoch 5 Batch 60 Loss 2.1793601512908936\n",
            "Epoch 5 Batch 60 Loss 2.1550302505493164\n",
            "Epoch 5 Batch 90 Loss 2.1859967708587646\n",
            "Epoch 5 Loss 2.1009\n",
            "Time taken for 1 epoch 58.2596275806427 sec\n",
            "\n",
            "Epoch 6 Batch 40 Loss 2.1969895362854004\n",
            "Epoch 6 Batch 50 Loss 2.0947093963623047\n",
            "Epoch 6 Batch 60 Loss 2.0541272163391113\n",
            "Epoch 6 Batch 60 Loss 2.050412178039551\n",
            "Epoch 6 Batch 90 Loss 2.0658154487609863\n",
            "Epoch 6 Loss 1.9743\n",
            "Time taken for 1 epoch 37.91659903526306 sec\n",
            "\n",
            "Epoch 7 Batch 40 Loss 2.0792670249938965\n",
            "Epoch 7 Batch 50 Loss 2.0527753829956055\n",
            "Epoch 7 Batch 60 Loss 1.9901034832000732\n",
            "Epoch 7 Batch 60 Loss 1.9473938941955566\n",
            "Epoch 7 Batch 90 Loss 1.9498157501220703\n",
            "Epoch 7 Loss 1.8670\n",
            "Time taken for 1 epoch 38.17723774909973 sec\n",
            "\n",
            "Epoch 8 Batch 40 Loss 1.941699743270874\n",
            "Epoch 8 Batch 50 Loss 1.870997667312622\n",
            "Epoch 8 Batch 60 Loss 1.8182681798934937\n",
            "Epoch 8 Batch 60 Loss 1.7931725978851318\n",
            "Epoch 8 Batch 90 Loss 1.8221707344055176\n",
            "Epoch 8 Loss 1.6750\n",
            "Time taken for 1 epoch 38.4623818397522 sec\n",
            "\n",
            "Epoch 9 Batch 40 Loss 1.8176143169403076\n",
            "Epoch 9 Batch 50 Loss 1.6890742778778076\n",
            "Epoch 9 Batch 60 Loss 1.65642511844635\n",
            "Epoch 9 Batch 60 Loss 1.6425319910049438\n",
            "Epoch 9 Batch 90 Loss 1.6505866050720215\n",
            "Epoch 9 Loss 1.5317\n",
            "Time taken for 1 epoch 38.09287643432617 sec\n",
            "\n",
            "Epoch 10 Batch 40 Loss 1.6529678106307983\n",
            "Epoch 10 Batch 50 Loss 1.5728999376296997\n",
            "Epoch 10 Batch 60 Loss 1.508487343788147\n",
            "Epoch 10 Batch 60 Loss 1.4995601177215576\n",
            "Epoch 10 Batch 90 Loss 1.535212755203247\n",
            "Epoch 10 Loss 1.4223\n",
            "Time taken for 1 epoch 60.02282643318176 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhopW4Gq52Ky",
        "outputId": "0be796a4-b2ef-4510-c00e-f22608c366e2"
      },
      "source": [
        "model = build_model(vocabulary_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 64)             34816     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3345408   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 544)            557600    \n",
            "=================================================================\n",
            "Total params: 3,937,824\n",
            "Trainable params: 3,937,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnLnHIrU52Ky",
        "outputId": "155ab24e-0cae-42bb-c671-690a88c19717"
      },
      "source": [
        "# Vectorize\n",
        "char2idx = {u:i for i, u in zip(dictionary.values(), dictionary.keys())}\n",
        "idx2char = {i:u for i, u in zip(dictionary.values(), dictionary.keys())}\n",
        "\n",
        "# Convert all character to int base on char2idx dict\n",
        "text_as_int = np.array([i for data in data_list for i in data])\n",
        "len(text_as_int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "449177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kw9BFqUW52Kz"
      },
      "source": [
        "\n",
        "def generate_text_bigram(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "  two_grams = [''.join(start_string[ch_i:ch_i+2]) for ch_i in range(0, len(start_string)-2, 2)]\n",
        "  two_grams_ids = []\n",
        "  for i in two_grams:\n",
        "    two_grams_ids.append(dictionary[i])\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = tf.convert_to_tensor(two_grams_ids)\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  print(input_eval.shape)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr3-gx7f52Kz",
        "outputId": "e29f7470-c8a4-4d19-8446-63301f022029"
      },
      "source": [
        "print(generate_text_bigram(model, start_string=u\"a certain father had two sons, the elder of whom was smart ands\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 31)\n",
            "a certain father had two sons, the elder of whom was smart ands\n",
            "sleds do the haverything so have the dwarf.  \"ah,\" w, and tolk\n",
            "you wh.'eve talking came home down by ead, it.  wine go cornered all m-f becmentur manger cornup off her right into sil of them, and that in through the copelived himself as,\n",
            "las leat out\n",
            "their mithering.  he likesy.  he like gicotilvy\n",
            "tailor were just into the great her.\" then the old\n",
            "places, want over they, and the lion put up but of his\n",
            "hough form.  then he pulses, there my so the vainage for you, so, i mannge bate my mart.\" \"yes and the herseUNKt's he mady\n",
            "evily sparits, and in and\n",
            "so strank earn\n",
            "appeautiful\n",
            "cusplesnd on his tanderelmbre, i do the wall,\n",
            "and begged himself until the seven mouth and l-did whole world came back to he comminded he day\n",
            "snow-white with them, they went youther about of fulf of her taillad, and she was to giest.  how fall lost his st if beforest.  in for whilind no lock for own, and a king always look lood time he lasked balls.  do not gree, so that he had selolded by delighs to may, if is planm, cut fly threx, and clo-fetch me.\"  but he cried to\n",
            "him, though which he\n",
            "can farther, and never yet go, and when\n",
            "she sawe, so heard a sittin protter went\n",
            "in the\n",
            "pigeone, at she said, \"lite could, and quroy he sto\n",
            "el whee hearth and a beas much them no one, striked it you do not.\"  \"day here over there, at lights, and at last other his bigome will know whose it was cansil, had taupiack him, and in it into the treasuresheart.  but it your every mine dis\n",
            "not was.\" \"we\n",
            "girlered that he had one day for a preach mage golen doing on which was a little sister.\n",
            "\n",
            "when he\n",
            "came to the appor's handle a didice figreats.\" carried him\n",
            "out hit clesse and said, \"i fores of it into the clot.\"\n",
            "\n",
            "heregan to piece had a given sat the seephstling the i twoeper got in, whe didly voietabous sip down, ore anythith the same of the golden cup,\n",
            "and she had\n",
            "somishing lieves, and in friving\n",
            "the leaver\n",
            "quid, and reached fores when as\n",
            "thatey was siling time and he was.  shand it, she may already his cousent went toft\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4yut4irCPHe"
      },
      "source": [
        "!zip -r "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}