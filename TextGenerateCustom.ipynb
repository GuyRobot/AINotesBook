{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TextGenerateCustom.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuyRobot/AINotesBook/blob/main/TextGenerateCustom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hlt4Pl5ex8B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc1I1xLmex8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3693fb-f9b4-4912-a757-f8eaa8d61d43"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4MQGtm9ex8H",
        "outputId": "e6269e04-226b-4979-f5f1-49dd0a4278bb"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode('utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "print(text[:100])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqYwZCSrex8I",
        "outputId": "3875409d-16d3-4fe9-bbb4-7e6fdcf9bc2f"
      },
      "source": [
        "# Unique character in files (a, b, c...)\n",
        "vocab = sorted(set(text))\n",
        "len(vocab)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UKThtV2ex8I",
        "outputId": "23fddb22-31e7-4e7d-f0d7-f5035023b116"
      },
      "source": [
        "# Vectorize\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert all character to int base on char2idx dict\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "idx2char, text_as_int\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
              "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
              "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
              "        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
              "        'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
              "       dtype='<U1'), array([18, 47, 56, ..., 45,  8,  0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNHRQ6d5ZZij",
        "outputId": "80a1a0da-5777-4cfe-b03f-3d5ed6d66d22"
      },
      "source": [
        "ids_from_chars = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))\n",
        "ids_from_chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup at 0x7f8235c0c198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ExleHaXZgc_",
        "outputId": "960e186b-083d-4865-ce70-c340020559ee"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "chars_from_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup at 0x7f8235b73e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WX_uMgHYzl7"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msqnPJZ1Y42x",
        "outputId": "c31619f2-7ee4-4a9f-e9ed-5115f7a97647"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([20, 49, 58, ..., 47, 10,  2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TICyPWacY68e"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyvXH7kiZy23",
        "outputId": "e6b0d13d-e1a2-4d18-961a-cf5901e6cdc4"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KeIDHW5Z8XK",
        "outputId": "b1f27686-fb93-4186-b73d-9d50896a9769"
      },
      "source": [
        "# Map text to input and target (both input and target have\n",
        "# the same seq_length but target is shifted to right one character)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1] # take all except the last character\n",
        "    target_text = chunk[1:] # take all except the first character\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "171hbfUZZ-ql",
        "outputId": "babf0235-f10d-425e-db60-c60813426f50"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdmaHKu7ex8I"
      },
      "source": [
        "# # Training examples  and targets\n",
        "# # Divide text into example sequences, each input sequence will\n",
        "# # contain seq_length characters from the text\n",
        "# # Each sequence, the targets contain the same seq_length of text, but shifted one character to right\n",
        "\n",
        "# seq_length = 100\n",
        "# examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "# for i in char_dataset.take(5):\n",
        "#     print(idx2char[i.numpy()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QFAOF7Hex8J"
      },
      "source": [
        "# sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "# for item in sequences.take(5):\n",
        "#     print(repr(''.join(idx2char[item.numpy()])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc9X43Uiex8J"
      },
      "source": [
        "# for input_exp, target_exp in dataset.take(1):\n",
        "#     print('Input data', repr(''.join(idx2char[input_exp.numpy()])))\n",
        "#     print(\"Target data\", repr(''.join(idx2char[target_exp.numpy()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2kHx0ICex8K"
      },
      "source": [
        "# \"\"\"\n",
        "#     Each index of these vectors is processed as a one\n",
        "#     time step. For the input at time step 0, the model\n",
        "#     receives the index for \"F\" and tries to predict\n",
        "#     the index for \"i\" as the next character. At the\n",
        "#     next timestep, it does the same thing but the RNN\n",
        "#     considers the previous step context in addition\n",
        "#     to the current input character.\n",
        "# \"\"\"\n",
        "\n",
        "# for i, (input_idx, target_idx) in enumerate(zip(input_exp[:5], target_exp[:5])):\n",
        "#     print(\"Step {:4d}\".format(i))\n",
        "#     print(\"\\tinput: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "#     print(\"\\toutput: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtA1u30pex8K",
        "outputId": "5be78e97-4264-4ed6-f532-639fecbaec15"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw78QyhVex8K"
      },
      "source": [
        "# Model\n",
        "# Embedding: input layer map the numbers of each character to a vector\n",
        "# with embedding_dim\n",
        "# GRU: special type of RNN with size units=rnn_units\n",
        "# Dense: vocab_size outputs\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzS4Jf7nex8K"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    print(x.shape)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x\n",
        "\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU3ZjJ_Eex8L",
        "outputId": "3feb47e8-aaee-40ad-977c-6a8025dab307"
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtoPjIIOex8L",
        "outputId": "41d3d1fb-0c0f-4397-e83f-ba2faf8dd3d3"
      },
      "source": [
        "\"\"\"\n",
        "    For each character the model looks up the\n",
        "    embedding, runs the GRU one timestep with\n",
        "    the embedding as input, and applies the dense\n",
        "    layer to generate logits predicting the log-likelihood of the next character:\n",
        "\"\"\"\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 256)\n",
            "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  17152     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u6yBwh_ex8M",
        "outputId": "766137c1-542a-4b52-e807-b05a2f369769"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9, 13, 39, 19, 65,  3,  2, 40,  8,  5, 48, 12, 44, 57, 66,  2, 60,\n",
              "       37, 40, 49, 57, 36, 49, 34, 42, 36, 64, 29, 24, 21,  5, 48,  8, 55,\n",
              "       27, 63,  9, 14, 12,  3, 37, 60, 14, 26, 37, 14, 28, 20, 61, 17, 36,\n",
              "       12, 33, 10,  1, 63, 59, 39, 63, 17, 24, 23, 63,  4, 46,  7, 61, 52,\n",
              "       54, 42, 36,  0,  6, 18, 53, 15, 62,  2, 56, 27,  3, 24,  6, 20, 60,\n",
              "       55, 42,  9, 30, 34, 35, 61, 59,  0, 31, 34, 32, 48, 30,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcXGsApaex8M"
      },
      "source": [
        "# print(\"Input: \\n\", repr(''.join(idx2char[input_example_batch[0]])))\n",
        "# print(\"Next Char Predictions: \\n\", repr(''.join(idx2char[sampled_indices])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPfTlJQnex8M",
        "outputId": "df3c519e-dfcd-48da-cd28-b8d3e736c7d2"
      },
      "source": [
        "def loss_sparse(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "example_batch_loss.numpy().mean()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2046723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW4GOIesao8-"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPEkpaxQauHw",
        "outputId": "0907fb33-aba2-4a3f-e7ae-ba94eac92fb7"
      },
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "(64, 100, 256)\n",
            "(64, 100, 256)\n",
            "172/172 [==============================] - 11s 50ms/step - loss: 3.3131\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 2.0889\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.7778\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.5912\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.4691\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.3982\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.3347\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.2868\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.2448\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.2043\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1647\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1244\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0765\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0316\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9848\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9321\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.8789\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8265\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7736\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.7236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBmDswf1a9iJ"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    print(\"Input chars\", input_chars)\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "    print(\"Input Shape\", input_ids.shape)\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP5vjYCnbB6c"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX9fFllQbFSR",
        "outputId": "5553b598-dd7a-41a7-85d9-fe2cec606aa2"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "  \n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input chars tf.RaggedTensor(values=Tensor(\"UnicodeSplit/UnicodeEncode/UnicodeEncode/UnicodeEncode/UnicodeEncode:0\", shape=(None,), dtype=string), row_splits=Tensor(\"UnicodeSplit/UnicodeDecode:0\", shape=(2,), dtype=int64))\n",
            "Input Shape (1, None)\n",
            "(1, None, 256)\n",
            "Input chars tf.RaggedTensor(values=Tensor(\"UnicodeSplit/UnicodeEncode/UnicodeEncode/UnicodeEncode/UnicodeEncode:0\", shape=(None,), dtype=string), row_splits=Tensor(\"UnicodeSplit/UnicodeDecode:0\", shape=(2,), dtype=int64))\n",
            "Input Shape (1, None)\n",
            "(1, None, 256)\n",
            "ROMEO:\n",
            "My lord, I purpose it.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "'Tis foe to-day, and gentlemen born.\n",
            "\n",
            "Clown:\n",
            "We cannot tell below.\n",
            "\n",
            "LADY ANNE:\n",
            "Before we could thither come athough to my death?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "My lord, have we now gone, 'tis parties: but if you\n",
            "serve the ordare hath kept with me; besides and kings,\n",
            "And bid her venturous to my love.\n",
            "\n",
            "FLORD ROSS:\n",
            "Why, phear his heart, and even your impression would Show her,\n",
            "Are denied to look on his noble hand:\n",
            "And because the idle dully unto the ground,\n",
            "In peace each one of those three of thines\n",
            "'Whisper your grace I men in his name,\n",
            "Mark'd by the field, Warwick! mast thou with Richmond!\n",
            "I would not often hangmen. Come perfect I speak;\n",
            "And when we see if any hand.\n",
            "\n",
            "AUTOLYCUS:\n",
            "I am but one of us think that I shall lay a tear;\n",
            "and so I take my lest and bring it thee!\n",
            "Pardon me, lords, make nose eyes good negly\n",
            "No better traitor: and, whiles I am?\n",
            "Tragicion, marry? wife!\n",
            "How will no married man how his use to\n",
            "be? he has deserved more than edwards\n",
            "As children th \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.6384127140045166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzbdpDqUcLmZ",
        "outputId": "9219bf35-54ce-4a82-cb9c-b25a313800f8"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f819bcfddd8>, because it is not built.\n",
            "(None, 100, 256)\n",
            "(None, 100, 256)\n",
            "(None, 100, 256)\n",
            "(None, 100, 256)\n",
            "Input chars tf.RaggedTensor(values=Tensor(\"UnicodeSplit/UnicodeEncode/UnicodeEncode/UnicodeEncode/UnicodeEncode:0\", shape=(None,), dtype=string), row_splits=Tensor(\"UnicodeSplit/UnicodeDecode:0\", shape=(2,), dtype=int64))\n",
            "Input Shape (1, None)\n",
            "(1, None, 256)\n",
            "Input chars tf.RaggedTensor(values=Tensor(\"UnicodeSplit/UnicodeEncode/UnicodeEncode/UnicodeEncode/UnicodeEncode:0\", shape=(None,), dtype=string), row_splits=Tensor(\"UnicodeSplit/UnicodeDecode:0\", shape=(2,), dtype=int64))\n",
            "Input Shape (1, None)\n",
            "(1, None, 256)\n",
            "(None, 100, 256)\n",
            "(None, 100, 256)\n",
            "(None, 100, 256)\n",
            "(None, 100, 256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O09wy-Vic1ky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f978ca-4dbf-42ee-b6b3-e3e36da8b7d8"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f819b3179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f819b3179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f819b3179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f819b3179d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "O, that once comes thereaf, good-song--manded; when\n",
            "Is this her face-blight day and stood ugning.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PXuWVFtex8M"
      },
      "source": [
        "# model.compile(optimizer='adam', loss=loss)\n",
        "#\n",
        "# checkpoint_dir = './training_checkpoint'\n",
        "#\n",
        "# checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "#\n",
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=checkpoint_predix,\n",
        "#     save_weights_only=True\n",
        "# )\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "    with tf.GradientTape() as g:\n",
        "        predictions = model(inp)\n",
        "        loss = tf.reduce_mean(loss_sparse(target, predictions))\n",
        "\n",
        "    gradients = g.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Sj8RJsex8M",
        "outputId": "6edf2f22-eb3b-4857-9f0e-0d0c0884b2cb"
      },
      "source": [
        "EPOCHS = 10\n",
        "checkpoint_dir = './training_checkpoint'\n",
        "\n",
        "checkpoint_predix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_predix,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # reset hidden state\n",
        "    model.reset_states()\n",
        "\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        loss = train_step(inp, target)\n",
        "\n",
        "        if batch_n % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {}'.format(epoch + 1, batch_n, loss))\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_predix.format(epoch=epoch))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "\n",
        "model.save_weights(checkpoint_predix.format(epoch=epoch))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 256)\n",
            "(64, 100, 256)\n",
            "Epoch 1 Batch 0 Loss 0.6629511117935181\n",
            "Epoch 1 Batch 100 Loss 0.7019443511962891\n",
            "Epoch 1 Loss 0.7169\n",
            "Time taken for 1 epoch 11.343119859695435 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5859658718109131\n",
            "Epoch 2 Batch 100 Loss 0.6386222243309021\n",
            "Epoch 2 Loss 0.6507\n",
            "Time taken for 1 epoch 10.347403287887573 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.5183229446411133\n",
            "Epoch 3 Batch 100 Loss 0.5777083039283752\n",
            "Epoch 3 Loss 0.6360\n",
            "Time taken for 1 epoch 10.415955305099487 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5053024888038635\n",
            "Epoch 4 Batch 100 Loss 0.5534664392471313\n",
            "Epoch 4 Loss 0.6094\n",
            "Time taken for 1 epoch 10.40802001953125 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4681413173675537\n",
            "Epoch 5 Batch 100 Loss 0.5183385610580444\n",
            "Epoch 5 Loss 0.5968\n",
            "Time taken for 1 epoch 10.565070629119873 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4594515860080719\n",
            "Epoch 6 Batch 100 Loss 0.49798935651779175\n",
            "Epoch 6 Loss 0.5562\n",
            "Time taken for 1 epoch 10.451234340667725 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.4375821352005005\n",
            "Epoch 7 Batch 100 Loss 0.4934030771255493\n",
            "Epoch 7 Loss 0.5383\n",
            "Time taken for 1 epoch 10.474464893341064 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4344988167285919\n",
            "Epoch 8 Batch 100 Loss 0.47986695170402527\n",
            "Epoch 8 Loss 0.5341\n",
            "Time taken for 1 epoch 10.643227815628052 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.43044278025627136\n",
            "Epoch 9 Batch 100 Loss 0.4549718201160431\n",
            "Epoch 9 Loss 0.5215\n",
            "Time taken for 1 epoch 10.490911483764648 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3920052647590637\n",
            "Epoch 10 Batch 100 Loss 0.4681278169155121\n",
            "Epoch 10 Loss 0.4909\n",
            "Time taken for 1 epoch 10.737764835357666 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3o-gfHgivUB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT-dGto8aswP",
        "outputId": "ba4973fb-b4b4-4395-f9cd-0124e917958d"
      },
      "source": [
        "skip_ids = ids_from_chars(['', '[UNK]'])[:, None]\n",
        "skip_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 1), dtype=int64, numpy=\n",
              "array([[0],\n",
              "       [1]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N3xE1osb_E1",
        "outputId": "0c30d9da-4fd8-4c47-eb99-59209b2ed049"
      },
      "source": [
        "ids_from_chars.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " '\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhs-UTARex8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5918ac-3bf6-497b-fac5-9100b140fde1"
      },
      "source": [
        "tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f819abc96a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr54n5y5ex8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb0f248-d44c-4a24-f8b3-d256072793ea"
      },
      "source": [
        "len(ids_from_chars.get_vocabulary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFWD8BuZgUa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94865214-f486-4206-f5de-34132d26379a"
      },
      "source": [
        "inputs = \"ROMEO: \"\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "print(next_char)\n",
        "input_chars = tf.strings.unicode_split(next_char, 'UTF-8')\n",
        "input_chars.to_list()\n",
        "tf.ragged.constant(input_chars.to_list())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([b'ROMEO:'], shape=(1,), dtype=string)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'R', b'O', b'M', b'E', b'O', b':']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecJElTekjSn1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "358ad873-35d9-4877-d0e5-b8bf2aa9a6ec"
      },
      "source": [
        ",tf.ragged.constant()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5192b9c1160c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: constant() missing 1 required positional argument: 'pylist'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whcFlNKFhxk1"
      },
      "source": [
        "# input_ids = ids_from_chars(input_chars)\n",
        "# input_ids\n",
        "input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "input_ids = ids_from_chars(input_chars)\n",
        "input_ids = tf.convert_to_tensor(input_ids)\n",
        "input_ids\n",
        "print(\"Fuck\", input_ids.shape)\n",
        "tf.reshape(input_ids, shape=[7, None])\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "# predicted_logits, states =  model(inputs=input_ids, states=states, \n",
        "                                          # return_state=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQY2nwvGfDyA"
      },
      "source": [
        "states = None\n",
        "model(inputs=input_ids, states=states, return_state=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}