{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFM6oJMrlb3INKzdmrRaH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuyRobot/AINotesBook/blob/main/EHealthVNCrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5zRoSyAKesbC"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "step = 40\n",
        "BASE_URL = \"https://suckhoetoday.com/\"\n",
        "\n",
        "# url = \"https://suckhoetoday.com/p/179-Phong-benh-mua-dong.html/\"\n",
        "url = \"https://suckhoetoday.com/p/97-BENH-VA-CHUA-BENH-CHO-TRE-CON.html/\"\n",
        "# url = \"https://suckhoetoday.com/p/98-BENH-DICH-THEO-MUA.html/\"\n",
        "# url = \"https://suckhoetoday.com/p/185-Phong-benh-mua-he.html/\"\n",
        "# url = \"https://suckhoetoday.com/p/95-CHAM-SOC-SUC-KHOE-GIA-DINH.html/\"\n",
        "# url = \"https://suckhoetoday.com/p/93-CHE-DO-DINH-DUONG.html/\"\n",
        "page = urlopen(url)\n",
        "html = page.read().decode(\"utf-8\")\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "init_url = url"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# links = [a.get(\"href\") for a in soup.find_all(\"a\", {\"class\": \"title\"})]\n",
        "# links"
      ],
      "metadata": {
        "id": "pLN8Ju2Pe6Nt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [a.get_text() for a in soup.find_all(\"a\", {\"class\": \"title\"})]"
      ],
      "metadata": {
        "id": "nKNJJ-23hc8m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url = f\"{BASE_URL}{links[0]}\"\n",
        "# page = urlopen(url)\n",
        "# html = page.read().decode(\"utf-8\")\n",
        "# soup = BeautifulSoup(html, \"html.parser\")\n",
        "# p = soup.find_all(\"div\", {\"class\": \"postbody\"})[0].find_all(\"div\", {\"class\": \"content\"})[0].find_all(\"p\")[0]\n",
        "# for x in p.find_all(\"li\"):\n",
        "#   if x is not None:\n",
        "#     x.decompose()\n",
        "# p.get_text().strip()"
      ],
      "metadata": {
        "id": "Fak9Er6xfvM7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data = []\n",
        "for p in range(1, 11):\n",
        "  print(f\"Crawl {p}\")\n",
        "  try:\n",
        "    url = init_url\n",
        "    url = f\"{url}/page{p}\"\n",
        "    page = urlopen(url)\n",
        "    html = page.read().decode(\"utf-8\")\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    ass = soup.find_all(\"a\", {\"class\": \"title\"})\n",
        "    links = [a.get(\"href\") for a in ass]\n",
        "    questions = [a.get_text() for a in ass]\n",
        "    for i, link in enumerate(links):\n",
        "      try:\n",
        "        url = f\"{BASE_URL}{link}\"\n",
        "        page = urlopen(url)\n",
        "        html = page.read().decode(\"utf-8\")\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        p = soup.find_all(\"div\", {\"class\": \"postbody\"})[0].find_all(\"div\", {\"class\": \"content\"})[0].find_all(\"p\")[0]\n",
        "        for x in p.find_all(\"li\"):\n",
        "          if x is not None:\n",
        "            x.decompose()\n",
        "\n",
        "        data.append({\"question\": re.sub(r'Question:', '', re.sub(r'(\\n\\s*)+\\n+', '\\n', re.sub(r'\\r', r'\\n', questions[i].strip())), re.I),\n",
        "                     \"answer\": re.sub(r'Detailed Answer:', '', re.sub(r'Brief Answer:', '', re.sub(r'(\\n\\s*)+\\n+', '\\n', re.sub(r'\\r', r'\\n', p.get_text().strip())), re.I), re.I), \"url\": url})\n",
        "      except:\n",
        "        pass\n",
        "    if p % 2 == 0:\n",
        "      with open(\"crawled.json\", \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "with open(\"crawled.json\", \"w\", encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdWdWqtnfSCq",
        "outputId": "dc66af33-7999-4bdb-b06d-c64215b2b7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawl 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"crawled.json\", \"w\", encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "LMSsmMe9jVA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "nV1TNVcaioem"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}