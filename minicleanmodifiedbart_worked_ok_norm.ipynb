{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cce0a598e264a3298e640658bd60652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1c68cd6e388449796b2ccebc9136992",
              "IPY_MODEL_ae71d95db2af4b8d9e5d9a3a4a79cda4",
              "IPY_MODEL_848dcbffcc8a459ea0076a0fc1441011"
            ],
            "layout": "IPY_MODEL_215d0502f1de4c44b4ec863341c6081e"
          }
        },
        "f1c68cd6e388449796b2ccebc9136992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_170912398b18414e8af952347fb8eb5d",
            "placeholder": "​",
            "style": "IPY_MODEL_946fef3c77e14af1a8ca7d86c305fe46",
            "value": "Downloading builder script: 100%"
          }
        },
        "ae71d95db2af4b8d9e5d9a3a4a79cda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_507d71e1d39f47f2896e6712119a525e",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad54475782ac4e10908a0a5c3d20d60f",
            "value": 6270
          }
        },
        "848dcbffcc8a459ea0076a0fc1441011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b586ecccba7544f8b65a14a4200f30fe",
            "placeholder": "​",
            "style": "IPY_MODEL_8872e1efbb5348278a1dc508b297672d",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 337kB/s]"
          }
        },
        "215d0502f1de4c44b4ec863341c6081e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "170912398b18414e8af952347fb8eb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946fef3c77e14af1a8ca7d86c305fe46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507d71e1d39f47f2896e6712119a525e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad54475782ac4e10908a0a5c3d20d60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b586ecccba7544f8b65a14a4200f30fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8872e1efbb5348278a1dc508b297672d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91bd04ffca754e5db3779c76b4cd0383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0eb2d297d5dd427ba03d3b4cb5fd709b",
              "IPY_MODEL_a40dbf210cac4242ad015b3df62c8156",
              "IPY_MODEL_4d7a75915d4041a59fd1e4618bcac2c5"
            ],
            "layout": "IPY_MODEL_c3e5ccd1f42b4171be607b83c2063ee3"
          }
        },
        "0eb2d297d5dd427ba03d3b4cb5fd709b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3701a466616d4bb381ce747a9fdf494a",
            "placeholder": "​",
            "style": "IPY_MODEL_1b839d4f22e2456e8d0cccdf39133043",
            "value": "config.json: 100%"
          }
        },
        "a40dbf210cac4242ad015b3df62c8156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8274f78b8e44c3b91f3f5dbddbf25d7",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ead8b98cb17540ea9875bbfb90a8b967",
            "value": 1716
          }
        },
        "4d7a75915d4041a59fd1e4618bcac2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d910502852a47819c9c43abd74d709a",
            "placeholder": "​",
            "style": "IPY_MODEL_d3288f91f5154f7da25f20f49ebe27f5",
            "value": " 1.72k/1.72k [00:00&lt;00:00, 121kB/s]"
          }
        },
        "c3e5ccd1f42b4171be607b83c2063ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3701a466616d4bb381ce747a9fdf494a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b839d4f22e2456e8d0cccdf39133043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8274f78b8e44c3b91f3f5dbddbf25d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead8b98cb17540ea9875bbfb90a8b967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d910502852a47819c9c43abd74d709a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3288f91f5154f7da25f20f49ebe27f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "480731125aa1438dbd79cb543bb8d9ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d6cbb95849846fa80e8f6b72b9b7729",
              "IPY_MODEL_ba229ccf497c450caab08e344d0b00b3",
              "IPY_MODEL_a220eaf4596b4ce5a28a957722aa2fbc"
            ],
            "layout": "IPY_MODEL_fd2c28f4cdbb4bd4a23431a1e6a4bd74"
          }
        },
        "1d6cbb95849846fa80e8f6b72b9b7729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa0fa2212ef4dcf98b8ad2974536271",
            "placeholder": "​",
            "style": "IPY_MODEL_a0756a8e2496450da6304513a7ac4fd5",
            "value": "vocab.json: 100%"
          }
        },
        "ba229ccf497c450caab08e344d0b00b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f976e89034417d909176ccc3199b13",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_118f740a96574190917ac0dd5f9ad6b2",
            "value": 898823
          }
        },
        "a220eaf4596b4ce5a28a957722aa2fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7756bd4397944d9e822dc7f5f4c35967",
            "placeholder": "​",
            "style": "IPY_MODEL_53e30201189343d2ac64346a311ef571",
            "value": " 899k/899k [00:00&lt;00:00, 7.09MB/s]"
          }
        },
        "fd2c28f4cdbb4bd4a23431a1e6a4bd74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa0fa2212ef4dcf98b8ad2974536271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0756a8e2496450da6304513a7ac4fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19f976e89034417d909176ccc3199b13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118f740a96574190917ac0dd5f9ad6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7756bd4397944d9e822dc7f5f4c35967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e30201189343d2ac64346a311ef571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89aabe2e3b6049308db13e67fe75bcac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e9ca9c95b5e4ab9969bbd0927ad7ca9",
              "IPY_MODEL_b53abf756f0e4ecbbc9fc5a346e0b3c9",
              "IPY_MODEL_9e61f19e3dbd4530a03e955d29bc71bb"
            ],
            "layout": "IPY_MODEL_d0cb990492f044c89fff7b087f4ef00c"
          }
        },
        "2e9ca9c95b5e4ab9969bbd0927ad7ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ebaf0c0c384e0b8aa142176d07d427",
            "placeholder": "​",
            "style": "IPY_MODEL_b9df5449bb5a4d1c8d27208f969b27df",
            "value": "merges.txt: 100%"
          }
        },
        "b53abf756f0e4ecbbc9fc5a346e0b3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a1b83d9cfa5423f848cb12cc76f331c",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_301e4c7987a1400ea64120c552f17d56",
            "value": 456318
          }
        },
        "9e61f19e3dbd4530a03e955d29bc71bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48d175497df54ca9aa6e8479c41f509b",
            "placeholder": "​",
            "style": "IPY_MODEL_138e499796654a0e9f7f3ef251197012",
            "value": " 456k/456k [00:00&lt;00:00, 10.3MB/s]"
          }
        },
        "d0cb990492f044c89fff7b087f4ef00c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03ebaf0c0c384e0b8aa142176d07d427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9df5449bb5a4d1c8d27208f969b27df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a1b83d9cfa5423f848cb12cc76f331c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301e4c7987a1400ea64120c552f17d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48d175497df54ca9aa6e8479c41f509b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "138e499796654a0e9f7f3ef251197012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b7f729a3ac14b0fa4b50252fc96afe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7738bf877d94bc79f4aed04039c5a1a",
              "IPY_MODEL_21d52b7ee0fc4d31a6cf4111de87a004",
              "IPY_MODEL_21e4c7b42aaa4791a4f87eeeacf7f39c"
            ],
            "layout": "IPY_MODEL_f93d47722dad4bddbc9eb86a66082255"
          }
        },
        "b7738bf877d94bc79f4aed04039c5a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4941f968b0b5448ea255244a6bbe3e77",
            "placeholder": "​",
            "style": "IPY_MODEL_fe3a1fcd74d541d2a34c6b1d670be1a7",
            "value": "tokenizer.json: 100%"
          }
        },
        "21d52b7ee0fc4d31a6cf4111de87a004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cd2b33eb0884f7c97ec14fcd7fe5cde",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d083bbb5b9d496ab6f9ba25c6edca27",
            "value": 1355863
          }
        },
        "21e4c7b42aaa4791a4f87eeeacf7f39c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_505538e7ed7e45e882862a1f56dc2217",
            "placeholder": "​",
            "style": "IPY_MODEL_9454e30bc8e447739498e62ccf29c100",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 16.7MB/s]"
          }
        },
        "f93d47722dad4bddbc9eb86a66082255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4941f968b0b5448ea255244a6bbe3e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe3a1fcd74d541d2a34c6b1d670be1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cd2b33eb0884f7c97ec14fcd7fe5cde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d083bbb5b9d496ab6f9ba25c6edca27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "505538e7ed7e45e882862a1f56dc2217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9454e30bc8e447739498e62ccf29c100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eac58315b7d941e6958cad7e009117e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_159b91fe74d344bbafacae20d1ee75c9",
              "IPY_MODEL_ebe0a8fc637844619b6a3995e2406909",
              "IPY_MODEL_74aeea84d1f24b2ba15864ef0752fe08"
            ],
            "layout": "IPY_MODEL_fe9b3a6e95624633a63bd62378cf1d4d"
          }
        },
        "159b91fe74d344bbafacae20d1ee75c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c9d112601d456f943ff5616ebaba47",
            "placeholder": "​",
            "style": "IPY_MODEL_5bf15dcfc0b849be9412c94e6c7f2237",
            "value": "Map: 100%"
          }
        },
        "ebe0a8fc637844619b6a3995e2406909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01455d6b5bd14d099e6bea1ea92a72c9",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0b2ddb2b9b3436bbd96099717600bf8",
            "value": 1000
          }
        },
        "74aeea84d1f24b2ba15864ef0752fe08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f164a41db08d4f738dde8f0096ce9960",
            "placeholder": "​",
            "style": "IPY_MODEL_4b88dce384284e0f8874a9c025e101ae",
            "value": " 1000/1000 [00:01&lt;00:00, 1003.09 examples/s]"
          }
        },
        "fe9b3a6e95624633a63bd62378cf1d4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c9d112601d456f943ff5616ebaba47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf15dcfc0b849be9412c94e6c7f2237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01455d6b5bd14d099e6bea1ea92a72c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0b2ddb2b9b3436bbd96099717600bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f164a41db08d4f738dde8f0096ce9960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b88dce384284e0f8874a9c025e101ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6f1da9aaeb3438ba54a9052d6eb1fe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c65d585d29af45d79a25a426a2116d75",
              "IPY_MODEL_40e581fcf2d642289d148df79b51e27a",
              "IPY_MODEL_e0010ed667104980b844e33dade83e7a"
            ],
            "layout": "IPY_MODEL_24118ba2ac5e468fb2b5822e92d3ac17"
          }
        },
        "c65d585d29af45d79a25a426a2116d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d726311ce8c44965afb0f9ec92d69ef5",
            "placeholder": "​",
            "style": "IPY_MODEL_bf79f4a2fe7142339cb173b22a57b3e6",
            "value": "Map: 100%"
          }
        },
        "40e581fcf2d642289d148df79b51e27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73cd557dfb1444539424a03111fde5c1",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4195dfc510884309a819331c6422183b",
            "value": 100
          }
        },
        "e0010ed667104980b844e33dade83e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96d6a1537d394be4bd3d83a8c06dcbc7",
            "placeholder": "​",
            "style": "IPY_MODEL_0c8c84743fcc48d4aefb71561eb31f63",
            "value": " 100/100 [00:00&lt;00:00, 634.89 examples/s]"
          }
        },
        "24118ba2ac5e468fb2b5822e92d3ac17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d726311ce8c44965afb0f9ec92d69ef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf79f4a2fe7142339cb173b22a57b3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73cd557dfb1444539424a03111fde5c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4195dfc510884309a819331c6422183b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96d6a1537d394be4bd3d83a8c06dcbc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8c84743fcc48d4aefb71561eb31f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets as well as other dependencies. Uncomment the following cell and run it."
      ],
      "metadata": {
        "id": "X4cRE8IbIrIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install torch~=2.0.0 https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-2.0-cp38-cp38-linux_x86_64.whl"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:33.718949Z",
          "iopub.execute_input": "2023-10-25T07:07:33.719890Z",
          "iopub.status.idle": "2023-10-25T07:07:33.725468Z",
          "shell.execute_reply.started": "2023-10-25T07:07:33.719855Z",
          "shell.execute_reply": "2023-10-25T07:07:33.724339Z"
        },
        "trusted": true,
        "id": "1X-15ips0lSj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub datasets evaluate transformers rouge-score nltk transformers[torch] einops"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:33.729924Z",
          "iopub.execute_input": "2023-10-25T07:07:33.730258Z",
          "iopub.status.idle": "2023-10-25T07:07:45.765883Z",
          "shell.execute_reply.started": "2023-10-25T07:07:33.730205Z",
          "shell.execute_reply": "2023-10-25T07:07:45.764436Z"
        },
        "trusted": true,
        "id": "Kfpi330i0lSk",
        "outputId": "771fa191-844b-493f-8aaa-2e3c19847b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/44.6 kB\u001b[0m \u001b[31m716.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m797.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['XLA_USE_BF16']=\"1\"\n",
        "# os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n",
        "\n",
        "# import torch\n",
        "# import pandas as pd\n",
        "# from scipy import stats\n",
        "# import numpy as np\n",
        "\n",
        "# from tqdm import tqdm\n",
        "# from collections import OrderedDict, namedtuple\n",
        "# import torch.nn as nn\n",
        "# from torch.optim import lr_scheduler\n",
        "# import joblib\n",
        "\n",
        "# import logging\n",
        "# import transformers\n",
        "# from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig\n",
        "# import sys\n",
        "# from sklearn import metrics, model_selection"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:45.769185Z",
          "iopub.execute_input": "2023-10-25T07:07:45.769718Z",
          "iopub.status.idle": "2023-10-25T07:07:45.777120Z",
          "shell.execute_reply.started": "2023-10-25T07:07:45.769669Z",
          "shell.execute_reply": "2023-10-25T07:07:45.776079Z"
        },
        "trusted": true,
        "id": "YYMhD9Xh0lSm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch_xla.core.xla_model as xm\n",
        "# import torch_xla.distributed.parallel_loader as pl\n",
        "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "# device = xm.xla_device()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:45.778601Z",
          "iopub.execute_input": "2023-10-25T07:07:45.778955Z",
          "iopub.status.idle": "2023-10-25T07:07:45.789854Z",
          "shell.execute_reply.started": "2023-10-25T07:07:45.778921Z",
          "shell.execute_reply": "2023-10-25T07:07:45.788984Z"
        },
        "trusted": true,
        "id": "4Hte2XRp0lSm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
      ],
      "metadata": {
        "id": "xDgAT1IviIFZ",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:45.793066Z",
          "iopub.execute_input": "2023-10-25T07:07:45.793901Z",
          "iopub.status.idle": "2023-10-25T07:07:47.140062Z",
          "shell.execute_reply.started": "2023-10-25T07:07:45.793865Z",
          "shell.execute_reply": "2023-10-25T07:07:47.138621Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_VISIBLE_DEVICES=0,1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:47.141734Z",
          "iopub.execute_input": "2023-10-25T07:07:47.142050Z",
          "iopub.status.idle": "2023-10-25T07:07:48.184526Z",
          "shell.execute_reply.started": "2023-10-25T07:07:47.142023Z",
          "shell.execute_reply": "2023-10-25T07:07:48.183323Z"
        },
        "trusted": true,
        "id": "ws6idHmZ0lSn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"fd78ea9bd1f15165e547ac607fa3d95c18d50433\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:48.186466Z",
          "iopub.execute_input": "2023-10-25T07:07:48.187788Z",
          "iopub.status.idle": "2023-10-25T07:07:48.195616Z",
          "shell.execute_reply.started": "2023-10-25T07:07:48.187746Z",
          "shell.execute_reply": "2023-10-25T07:07:48.194673Z"
        },
        "trusted": true,
        "id": "o_vCNo9d0lSn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
      ],
      "metadata": {
        "id": "Bem2kQaviIFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you need to install Git-LFS. Uncomment the following instructions:"
      ],
      "metadata": {
        "id": "-LwLWH2WiIFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #!apt-get install git-lfs\n",
        "# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash"
      ],
      "metadata": {
        "id": "HSRwiC_EiIFa",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:48.197004Z",
          "iopub.execute_input": "2023-10-25T07:07:48.197368Z",
          "iopub.status.idle": "2023-10-25T07:07:48.205160Z",
          "shell.execute_reply.started": "2023-10-25T07:07:48.197336Z",
          "shell.execute_reply": "2023-10-25T07:07:48.203924Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
      ],
      "metadata": {
        "id": "KEoqq-6tiIFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "3rsZjvaJiIFb",
        "outputId": "cd5e485d-39f6-43b5-8701-8e260615365f",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:48.206491Z",
          "iopub.execute_input": "2023-10-25T07:07:48.206766Z",
          "iopub.status.idle": "2023-10-25T07:07:48.218176Z",
          "shell.execute_reply.started": "2023-10-25T07:07:48.206743Z",
          "shell.execute_reply": "2023-10-25T07:07:48.216600Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.35.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq)."
      ],
      "metadata": {
        "id": "HFASsisvIrIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
      ],
      "metadata": {
        "id": "IVbKHMj3iIFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils import send_example_telemetry\n",
        "\n",
        "send_example_telemetry(\"summarization_notebook\", framework=\"pytorch\")"
      ],
      "metadata": {
        "id": "ngRm8xehiIFc",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:48.219992Z",
          "iopub.execute_input": "2023-10-25T07:07:48.220423Z",
          "iopub.status.idle": "2023-10-25T07:07:48.480045Z",
          "shell.execute_reply.started": "2023-10-25T07:07:48.220390Z",
          "shell.execute_reply": "2023-10-25T07:07:48.478838Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a model on a summarization task"
      ],
      "metadata": {
        "id": "rEJBSTyZIrIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n",
        "\n",
        "![Widget inference on a summarization task](https://github.com/huggingface/notebooks/blob/main/examples/images/summarization.png?raw=1)\n",
        "\n",
        "We will see how to easily load the dataset for this task using 🤗 Datasets and how to fine-tune a model on it using the `Trainer` API."
      ],
      "metadata": {
        "id": "kTCFado4IrIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"facebook/bart-base\""
      ],
      "metadata": {
        "id": "g6PugG96iIFd",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:48.486267Z",
          "iopub.execute_input": "2023-10-25T07:07:48.486665Z",
          "iopub.status.idle": "2023-10-25T07:07:48.492716Z",
          "shell.execute_reply.started": "2023-10-25T07:07:48.486629Z",
          "shell.execute_reply": "2023-10-25T07:07:48.491468Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`t5-small`](https://huggingface.co/t5-small) checkpoint."
      ],
      "metadata": {
        "id": "4RRkXuteIrIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "whPRbBNbIrIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ],
      "metadata": {
        "id": "W7QYTpxXIrIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://github.com/GuyRobot/AINotesBook/releases/download/v1/EHealthChatDataset.json.gz\n",
        "# !gzip -d EHealthChatDataset.json.gz"
      ],
      "metadata": {
        "id": "xrRMvk-GukJI",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:48.494137Z",
          "iopub.execute_input": "2023-10-25T07:07:48.494545Z",
          "iopub.status.idle": "2023-10-25T07:07:48.504691Z",
          "shell.execute_reply.started": "2023-10-25T07:07:48.494503Z",
          "shell.execute_reply": "2023-10-25T07:07:48.503791Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nlp"
      ],
      "metadata": {
        "id": "HH3UmIo7w1z-",
        "outputId": "a5b46bb0-d599-4080-a761-4813735665c0",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:07:48.506039Z",
          "iopub.execute_input": "2023-10-25T07:07:48.506353Z",
          "iopub.status.idle": "2023-10-25T07:08:00.423813Z",
          "shell.execute_reply.started": "2023-10-25T07:07:48.506326Z",
          "shell.execute_reply": "2023-10-25T07:08:00.422620Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.4/1.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "\n",
        "# train_ds = load_dataset(\"json\", data_files=[\"EHealthChatDataset.json\"], split=\"train[:80%]\")\n",
        "# vals_ds = load_dataset(\"json\", data_files=[\"EHealthChatDataset.json\"], split=\"train[80%:]\")"
      ],
      "metadata": {
        "id": "IreSlFmlIrIm",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:00.426364Z",
          "iopub.execute_input": "2023-10-25T07:08:00.426683Z",
          "iopub.status.idle": "2023-10-25T07:08:00.434069Z",
          "shell.execute_reply.started": "2023-10-25T07:08:00.426654Z",
          "shell.execute_reply": "2023-10-25T07:08:00.432888Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "# with open(\"EHealthChatDataset.json\") as f:\n",
        "#     data = json.load(f)\n",
        "#     train_ds = Dataset.from_pandas(pd.DataFrame(data=data[:int(len(data)*0.8)]))\n",
        "#     vals_ds = Dataset.from_pandas(pd.DataFrame(data=data[int(len(data)*0.8):]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:00.435561Z",
          "iopub.execute_input": "2023-10-25T07:08:00.436395Z",
          "iopub.status.idle": "2023-10-25T07:08:00.445542Z",
          "shell.execute_reply.started": "2023-10-25T07:08:00.436362Z",
          "shell.execute_reply": "2023-10-25T07:08:00.444581Z"
        },
        "trusted": true,
        "id": "kdBjhXsd0lSq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/GuyRobot/AINotesBook/releases/download/clean/EHealthChatDataset_seq_512.csv"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:00.446810Z",
          "iopub.execute_input": "2023-10-25T07:08:00.447158Z",
          "iopub.status.idle": "2023-10-25T07:08:03.178567Z",
          "shell.execute_reply.started": "2023-10-25T07:08:00.447129Z",
          "shell.execute_reply": "2023-10-25T07:08:03.177409Z"
        },
        "trusted": true,
        "id": "eJLNOYZY0lSq",
        "outputId": "7421c7e1-cca6-44a4-8172-52dfc611bdb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-27 03:33:22--  https://github.com/GuyRobot/AINotesBook/releases/download/clean/EHealthChatDataset_seq_512.csv\n",
            "Resolving github.com (github.com)... 20.29.134.23\n",
            "Connecting to github.com (github.com)|20.29.134.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/350588256/7de58201-6958-41f7-9980-5363334ed2b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231127%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231127T033322Z&X-Amz-Expires=300&X-Amz-Signature=77336e5dbe45a718d1c16f0dda18eb45dbe171236f22d205869213e67433ba5d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=350588256&response-content-disposition=attachment%3B%20filename%3DEHealthChatDataset_seq_512.csv&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-27 03:33:22--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/350588256/7de58201-6958-41f7-9980-5363334ed2b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231127%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231127T033322Z&X-Amz-Expires=300&X-Amz-Signature=77336e5dbe45a718d1c16f0dda18eb45dbe171236f22d205869213e67433ba5d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=350588256&response-content-disposition=attachment%3B%20filename%3DEHealthChatDataset_seq_512.csv&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88000773 (84M) [application/octet-stream]\n",
            "Saving to: ‘EHealthChatDataset_seq_512.csv’\n",
            "\n",
            "EHealthChatDataset_ 100%[===================>]  83.92M  55.9MB/s    in 1.5s    \n",
            "\n",
            "2023-11-27 03:33:24 (55.9 MB/s) - ‘EHealthChatDataset_seq_512.csv’ saved [88000773/88000773]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"EHealthChatDataset_seq_512.csv\") as f:\n",
        "    data = pd.read_csv(f).dropna()\n",
        "    train_ds = Dataset.from_pandas(pd.DataFrame(data=data[:1000]))\n",
        "    vals_ds = Dataset.from_pandas(pd.DataFrame(data=data[:100]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:03.180601Z",
          "iopub.execute_input": "2023-10-25T07:08:03.181045Z",
          "iopub.status.idle": "2023-10-25T07:08:04.637112Z",
          "shell.execute_reply.started": "2023-10-25T07:08:03.181003Z",
          "shell.execute_reply": "2023-10-25T07:08:04.636114Z"
        },
        "trusted": true,
        "id": "vtSdrKmy0lSr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "raw_datasets = DatasetDict()\n",
        "raw_datasets[\"train\"] = train_ds\n",
        "raw_datasets[\"validation\"] = vals_ds\n",
        "# raw_datasets = load_dataset(\"xsum\")\n",
        "# raw_datasets = load_dataset(\"json\", data_files={\"train\": \"ehealthforumQAs.json\", \"validation\": \"ehealthforumQAs.json\"})\n",
        "metric = load(\"rouge\")"
      ],
      "metadata": {
        "id": "vFWoMd5vUg10",
        "outputId": "7864d803-a406-4a1d-d8b0-cc40c8225e78",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:04.638445Z",
          "iopub.execute_input": "2023-10-25T07:08:04.639532Z",
          "iopub.status.idle": "2023-10-25T07:08:06.050556Z",
          "shell.execute_reply.started": "2023-10-25T07:08:04.639493Z",
          "shell.execute_reply": "2023-10-25T07:08:06.049545Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0cce0a598e264a3298e640658bd60652",
            "f1c68cd6e388449796b2ccebc9136992",
            "ae71d95db2af4b8d9e5d9a3a4a79cda4",
            "848dcbffcc8a459ea0076a0fc1441011",
            "215d0502f1de4c44b4ec863341c6081e",
            "170912398b18414e8af952347fb8eb5d",
            "946fef3c77e14af1a8ca7d86c305fe46",
            "507d71e1d39f47f2896e6712119a525e",
            "ad54475782ac4e10908a0a5c3d20d60f",
            "b586ecccba7544f8b65a14a4200f30fe",
            "8872e1efbb5348278a1dc508b297672d"
          ]
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cce0a598e264a3298e640658bd60652"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nlp\n",
        "# dataset_cc = nlp.concatenate_datasets([train_ds, vals_ds])"
      ],
      "metadata": {
        "id": "zUTx_pwLw6qE",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.052033Z",
          "iopub.execute_input": "2023-10-25T07:08:06.052422Z",
          "iopub.status.idle": "2023-10-25T07:08:06.057955Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.052394Z",
          "shell.execute_reply": "2023-10-25T07:08:06.056896Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
      ],
      "metadata": {
        "id": "RzfPtOMoIrIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "id": "GWiVUF0jIrIv",
        "outputId": "02d2f936-e8a6-4fca-e65e-b4e04740f5be",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.059477Z",
          "iopub.execute_input": "2023-10-25T07:08:06.059811Z",
          "iopub.status.idle": "2023-10-25T07:08:06.071658Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.059780Z",
          "shell.execute_reply": "2023-10-25T07:08:06.070727Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Unnamed: 0.2', 'Unnamed: 0', 'Unnamed: 0.1', 'answer', 'url', 'question', '__index_level_0__'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['Unnamed: 0.2', 'Unnamed: 0', 'Unnamed: 0.1', 'answer', 'url', 'question', '__index_level_0__'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ],
      "metadata": {
        "id": "u3EtYfeHIrIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"][0]"
      ],
      "metadata": {
        "id": "X6HrpprwIrIz",
        "outputId": "70fdc0e4-eb67-4dd6-e825-f785bc7c3c95",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.072942Z",
          "iopub.execute_input": "2023-10-25T07:08:06.073332Z",
          "iopub.status.idle": "2023-10-25T07:08:06.085826Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.073300Z",
          "shell.execute_reply": "2023-10-25T07:08:06.084687Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unnamed: 0.2': 1,\n",
              " 'Unnamed: 0': 1,\n",
              " 'Unnamed: 0.1': 1,\n",
              " 'answer': 'Gynae examination and ultrasound advised  I understand your anxiety about the issue and it is indeed serious and should\\nnot be taken lightly  Get a pelvic examination from a good gynaecologist and an ultrasound at the earliest  It can be\\nserious so much so like cancer or can just be related to postmenopausal hormone deficiency  Get the tests and then we\\ncan decide further',\n",
              " 'url': 'https://www.healthcaremagic.com/premiumquestions/What-causes-persistent-painless-vaginal-bleeding/305320',\n",
              " 'question': 'I have vaginal bleeding for three years. No pain . I would like to know why.',\n",
              " '__index_level_0__': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ],
      "metadata": {
        "id": "WHUmphG3IrI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ],
      "metadata": {
        "id": "i3j8APAoIrI3",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.086768Z",
          "iopub.execute_input": "2023-10-25T07:08:06.087098Z",
          "iopub.status.idle": "2023-10-25T07:08:06.102568Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.087067Z",
          "shell.execute_reply": "2023-10-25T07:08:06.101692Z"
        },
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_elements(raw_datasets[\"train\"])"
      ],
      "metadata": {
        "id": "SZy5tRB_IrI7",
        "outputId": "72831654-4903-4c32-8270-00eec70a9582",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.103714Z",
          "iopub.execute_input": "2023-10-25T07:08:06.104504Z",
          "iopub.status.idle": "2023-10-25T07:08:06.125130Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.104469Z",
          "shell.execute_reply": "2023-10-25T07:08:06.124256Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.2</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>answer</th>\n",
              "      <th>url</th>\n",
              "      <th>question</th>\n",
              "      <th>__index_level_0__</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3662</td>\n",
              "      <td>3662</td>\n",
              "      <td>3662</td>\n",
              "      <td>C T guided biopsy    Since your doctor told you that either u had pneumonia or nodularity within right lung upper lobe i\\nsuggest u complete a 7 day course of antibiotic and then repeat  C T scan of chest with contrast and if the space\\noccupying lesion in your lungs still persist in the scan then u do a  C T guided biopsy from that lesion to rule out\\nmalignancy.  Generally pneumonic patches clear off in scan after a course of antibiotic.</td>\n",
              "      <td>https://www.healthcaremagic.com/premiumquestions/Is-persistent-nodularity-in-the-right-lung-indicative-of-cancer/108501</td>\n",
              "      <td>Hi I'm XXXXXXX XXXXXXX I was told by adoctor I have either pneumonia or nodularity within the right lung upper lobe if idon't  respond to antibiotics.Is that poosible and can you pneumni?Penelope  or I have a mass and it's probably cancer</td>\n",
              "      <td>816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1406</td>\n",
              "      <td>1406</td>\n",
              "      <td>1406</td>\n",
              "      <td>Please upload entire  M R I scans to a file sharing facility Thanks for writing in to us. The images attached are\\ninadequate to make an accurate analysis of your problems.  Please upload the entire contents of the  C D  or  D V D of\\nboth the  M R I scans to a file sharing web site like  Dropbox or  Google drive and send me the download link.  I will\\nhave a detailed look on my work station.</td>\n",
              "      <td>https://www.healthcaremagic.com/premiumquestions/What-does-my-MRI-scan-report-indicate/304393</td>\n",
              "      <td>I need you to compare two films of my neck for me.  I had one done on 7/15/2015 and the other done on 8/27/2015.  I was hit by a car as a pedestrian July 31st and had a severe blow to the pavement on my back.  I had a significant contusion/concussion and believe my neck may have had a whiplash effect. On the MRI report dated 8/27/2015 it states I have \"at the C6-7 level there are left sided posterior vertebral body spus with contact upon the thecal sac\".  It does not state this on the MRI from 7/15/2015.  Also, the MRI from 8/27/2015 states, \"at the T5-6 level, there appears to be a moderate right paracentral protrusion with slight contact upon the spinal cord\".  Is this on the MRI of my neck from 7/15/2015?  I need to know if you see any differences between the films of 7/15/2015 and 8/27/2015, please.  Unfortunately I don't see anywhere to attach the films...could you please send me instructions, thank you, XXXXXXX xxxxx I forgot to mention I have had 3 cervical fusions from 2010-2014.  I uploaded all the films of my neck, thank you, XXXXXXX</td>\n",
              "      <td>330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>927</td>\n",
              "      <td>927</td>\n",
              "      <td>927</td>\n",
              "      <td>Did you try tadalafil?? You may continue it.  Generally, they are not known to have side effects.  Ayurveda,  I am not\\nfully aware of its potential.  If you feel those pills are not helping you to the extent of your satisfaction  I will\\nconsider consulting a sexologist.  Counseling when your partner is alongside will benefit both deals this problem. In\\nthe interim did you try  Sildenafil or  Tadalafil tablets.  These are medicines used to treat erectile dysfunction.\\nYour local doctor will know about it.</td>\n",
              "      <td>https://www.healthcaremagic.com/premiumquestions/Suggest-treatment-for-erectile-dysfunction-while-on-Nurokind-Gold-and-Neorelax-MR/304691</td>\n",
              "      <td>Hello Sir,Couple of months earlier i consulted to you with my health problem. This is something related with erectile dysfunction which you indicated. As per your suggestion, i consulted our family physician and he educated me that this is nothing much to worry about, such things are common with many people. he also provided me a medeical prescription as - Tab- Neo,(2 tabs with milk) cap - Nurokind GoldCap - Artone xx (1 hour before bedtime).I took this prescription for a month, but i have not see any improvement with my problem. I am pretty disappointed at the moment, as it is troubling my personal life, I was looking forward to take stay on capsules, as they are ayurvedic and has no side effects. May i know your suggestions please?</td>\n",
              "      <td>226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1355</td>\n",
              "      <td>1355</td>\n",
              "      <td>1355</td>\n",
              "      <td>How may  I help you Thanks for using  Healthcaremagic. I read your query and understand your concerns.  Please elaborate\\nwhat you need to know about trichotillomania.  I am a psychiatrist and can guide you in specific manner. Thanks again.</td>\n",
              "      <td>https://www.healthcaremagic.com/premiumquestions/Suggest-medication-for-trichotillomania/304420</td>\n",
              "      <td>Do you recommend this for Trichotillomania?</td>\n",
              "      <td>317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1588</td>\n",
              "      <td>1588</td>\n",
              "      <td>1588</td>\n",
              "      <td>Your touch will not affect the analysis. I my opinion your touch will not affect the analysis.  But pouring of sample\\nfrom one to another container might affect.  So, get a repeat analysis done.  If reports again shows cryptospermia, you\\nmay need a testicular biopsy and  I V F with  I C S I.</td>\n",
              "      <td>https://www.healthcaremagic.com/premiumquestions/Suggest-treatment-for-cryptozoospermia/303997</td>\n",
              "      <td>Hi I was diagnosed as crytospermina. But during collecting seamen I touched it and even poured from one container to other .Does it effects the sperm analysis .</td>\n",
              "      <td>364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
      ],
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric"
      ],
      "metadata": {
        "id": "5o4rUteaIrI_",
        "outputId": "cd2bdc94-08c5-414a-f408-f6cadb1464f4",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.126376Z",
          "iopub.execute_input": "2023-10-25T07:08:06.126711Z",
          "iopub.status.idle": "2023-10-25T07:08:06.136667Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.126679Z",
          "shell.execute_reply": "2023-10-25T07:08:06.135666Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
              "Calculates average rouge scores for a list of hypotheses and references\n",
              "Args:\n",
              "    predictions: list of predictions to score. Each prediction\n",
              "        should be a string with tokens separated by spaces.\n",
              "    references: list of reference for each prediction. Each\n",
              "        reference should be a string with tokens separated by spaces.\n",
              "    rouge_types: A list of rouge types to calculate.\n",
              "        Valid names:\n",
              "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
              "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
              "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
              "\"`.\n",
              "        See details in https://github.com/huggingface/datasets/issues/617\n",
              "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
              "    use_aggregator: Return aggregates if this is set to True\n",
              "Returns:\n",
              "    rouge1: rouge_1 (f1),\n",
              "    rouge2: rouge_2 (f1),\n",
              "    rougeL: rouge_l (f1),\n",
              "    rougeLsum: rouge_lsum (f1)\n",
              "Examples:\n",
              "\n",
              "    >>> rouge = evaluate.load('rouge')\n",
              "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
              "    >>> references = [\"hello there\", \"general kenobi\"]\n",
              "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
      ],
      "metadata": {
        "id": "jAWdqcUBIrJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_preds = [\"hello there\", \"general kenobi\"]\n",
        "fake_labels = [\"hello there\", \"general kenobi\"]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ],
      "metadata": {
        "id": "6XN1Rq0aIrJC",
        "outputId": "8b7ef75a-1565-4026-8d70-e46478b58f70",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.137826Z",
          "iopub.execute_input": "2023-10-25T07:08:06.139587Z",
          "iopub.status.idle": "2023-10-25T07:08:06.350311Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.139561Z",
          "shell.execute_reply": "2023-10-25T07:08:06.349070Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the data"
      ],
      "metadata": {
        "id": "n9qywopnIrJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that the model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ],
      "metadata": {
        "id": "YVx71GdAIrJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "eXNLu_-nIrJI",
        "outputId": "6781c68f-aac2-463e-b6b2-cafc59742c42",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:06.351747Z",
          "iopub.execute_input": "2023-10-25T07:08:06.352096Z",
          "iopub.status.idle": "2023-10-25T07:08:07.363177Z",
          "shell.execute_reply.started": "2023-10-25T07:08:06.352065Z",
          "shell.execute_reply": "2023-10-25T07:08:07.362063Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "91bd04ffca754e5db3779c76b4cd0383",
            "0eb2d297d5dd427ba03d3b4cb5fd709b",
            "a40dbf210cac4242ad015b3df62c8156",
            "4d7a75915d4041a59fd1e4618bcac2c5",
            "c3e5ccd1f42b4171be607b83c2063ee3",
            "3701a466616d4bb381ce747a9fdf494a",
            "1b839d4f22e2456e8d0cccdf39133043",
            "e8274f78b8e44c3b91f3f5dbddbf25d7",
            "ead8b98cb17540ea9875bbfb90a8b967",
            "0d910502852a47819c9c43abd74d709a",
            "d3288f91f5154f7da25f20f49ebe27f5",
            "480731125aa1438dbd79cb543bb8d9ae",
            "1d6cbb95849846fa80e8f6b72b9b7729",
            "ba229ccf497c450caab08e344d0b00b3",
            "a220eaf4596b4ce5a28a957722aa2fbc",
            "fd2c28f4cdbb4bd4a23431a1e6a4bd74",
            "dfa0fa2212ef4dcf98b8ad2974536271",
            "a0756a8e2496450da6304513a7ac4fd5",
            "19f976e89034417d909176ccc3199b13",
            "118f740a96574190917ac0dd5f9ad6b2",
            "7756bd4397944d9e822dc7f5f4c35967",
            "53e30201189343d2ac64346a311ef571",
            "89aabe2e3b6049308db13e67fe75bcac",
            "2e9ca9c95b5e4ab9969bbd0927ad7ca9",
            "b53abf756f0e4ecbbc9fc5a346e0b3c9",
            "9e61f19e3dbd4530a03e955d29bc71bb",
            "d0cb990492f044c89fff7b087f4ef00c",
            "03ebaf0c0c384e0b8aa142176d07d427",
            "b9df5449bb5a4d1c8d27208f969b27df",
            "6a1b83d9cfa5423f848cb12cc76f331c",
            "301e4c7987a1400ea64120c552f17d56",
            "48d175497df54ca9aa6e8479c41f509b",
            "138e499796654a0e9f7f3ef251197012",
            "3b7f729a3ac14b0fa4b50252fc96afe7",
            "b7738bf877d94bc79f4aed04039c5a1a",
            "21d52b7ee0fc4d31a6cf4111de87a004",
            "21e4c7b42aaa4791a4f87eeeacf7f39c",
            "f93d47722dad4bddbc9eb86a66082255",
            "4941f968b0b5448ea255244a6bbe3e77",
            "fe3a1fcd74d541d2a34c6b1d670be1a7",
            "0cd2b33eb0884f7c97ec14fcd7fe5cde",
            "9d083bbb5b9d496ab6f9ba25c6edca27",
            "505538e7ed7e45e882862a1f56dc2217",
            "9454e30bc8e447739498e62ccf29c100"
          ]
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91bd04ffca754e5db3779c76b4cd0383"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "480731125aa1438dbd79cb543bb8d9ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89aabe2e3b6049308db13e67fe75bcac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b7f729a3ac14b0fa4b50252fc96afe7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the call above will use one of the fast tokenizers (backed by Rust) from the 🤗 Tokenizers library."
      ],
      "metadata": {
        "id": "Vl6IidfdIrJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ],
      "metadata": {
        "id": "rowT4iCLIrJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Hello, this one sentence!\")"
      ],
      "metadata": {
        "id": "a5hBlsrHIrJL",
        "outputId": "fbbbc45c-1b45-4a8c-d61c-4b85076059d5",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.364578Z",
          "iopub.execute_input": "2023-10-25T07:08:07.364935Z",
          "iopub.status.idle": "2023-10-25T07:08:07.374327Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.364899Z",
          "shell.execute_reply": "2023-10-25T07:08:07.373149Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [0, 31414, 6, 42, 65, 3645, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "Instead of one sentence, we can pass along a list of sentences:"
      ],
      "metadata": {
        "id": "qo_0B1M2IrJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
      ],
      "metadata": {
        "id": "zmWmo1SIiIFh",
        "outputId": "d9c04831-4c1e-44f5-9a90-8f3cd4a1a08b",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.375547Z",
          "iopub.execute_input": "2023-10-25T07:08:07.375902Z",
          "iopub.status.idle": "2023-10-25T07:08:07.389148Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.375844Z",
          "shell.execute_reply": "2023-10-25T07:08:07.388264Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 31414, 6, 42, 65, 3645, 328, 2], [0, 713, 16, 277, 3645, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the targets for our model, we need to tokenize them using the `text_target` parameter. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
      ],
      "metadata": {
        "id": "gfeukRDaiIFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer(text_target=[\"Hello, this one sentence!\", \"This is another sentence.\"]))"
      ],
      "metadata": {
        "id": "QXxL-gbziIFi",
        "outputId": "17fc2918-3f3c-4cf1-dcf7-db0748966da9",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.396530Z",
          "iopub.execute_input": "2023-10-25T07:08:07.396813Z",
          "iopub.status.idle": "2023-10-25T07:08:07.403334Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.396788Z",
          "shell.execute_reply": "2023-10-25T07:08:07.402238Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[0, 31414, 6, 42, 65, 3645, 328, 2], [0, 713, 16, 277, 3645, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" (the model can also translate and it needs the prefix to know which task it has to perform)."
      ],
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\""
      ],
      "metadata": {
        "id": "a3RAu2wniIFm",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.404548Z",
          "iopub.execute_input": "2023-10-25T07:08:07.404853Z",
          "iopub.status.idle": "2023-10-25T07:08:07.413101Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.404821Z",
          "shell.execute_reply": "2023-10-25T07:08:07.412112Z"
        },
        "trusted": true
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
      ],
      "metadata": {
        "id": "NUh2j127iIFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 512\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"question\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    labels = tokenizer(text_target=examples[\"answer\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "vc0BSBLIIrJQ",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.414208Z",
          "iopub.execute_input": "2023-10-25T07:08:07.414586Z",
          "iopub.status.idle": "2023-10-25T07:08:07.423940Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.414551Z",
          "shell.execute_reply": "2023-10-25T07:08:07.422883Z"
        },
        "trusted": true
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ],
      "metadata": {
        "id": "0lm8ozrJIrJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(raw_datasets['train'][:2])"
      ],
      "metadata": {
        "id": "-b70jh26IrJS",
        "outputId": "1ffe7adb-4ff6-4e09-a615-5984c710d9f2",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.425717Z",
          "iopub.execute_input": "2023-10-25T07:08:07.426044Z",
          "iopub.status.idle": "2023-10-25T07:08:07.440425Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.426012Z",
          "shell.execute_reply": "2023-10-25T07:08:07.439566Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[0, 100, 33, 34126, 13162, 13, 130, 107, 4, 440, 2400, 479, 38, 74, 101, 7, 216, 596, 4, 2], [0, 876, 21284, 364, 611, 6457, 11483, 2407, 9, 12581, 117, 22628, 7427, 1499, 15715, 2340, 16698, 22090, 5395, 480, 7, 2178, 181, 9475, 611, 8307, 337, 12581, 2199, 7586, 2292, 17733, 4399, 571, 2368, 19, 1533, 9843, 5109, 1536, 4, 19869, 32108, 16, 30389, 7018, 9663, 7586, 33473, 6, 642, 3290, 241, 281, 6, 7049, 877, 70, 2340, 155, 377, 939, 33, 551, 1416, 7586, 127, 12079, 32, 1717, 417, 718, 1879, 6, 329, 3976, 1417, 405, 6, 462, 8538, 718, 13, 859, 7586, 78, 127, 2292, 17733, 21, 753, 4, 245, 13753, 2156, 485, 266, 924, 545, 13753, 31617, 853, 1792, 179, 12, 134, 4, 466, 2642, 179, 12, 306, 4, 245, 37544, 27377, 12, 176, 4, 245, 579, 22371, 12, 2518, 579, 571, 3320, 12, 1558, 38994, 38204, 34637, 41278, 12, 4156, 1437, 1437, 127, 12581, 1836, 21, 361, 4, 245, 13753, 4, 1437, 178, 130, 377, 41268, 338, 127, 2292, 17733, 2906, 7, 545, 4, 245, 13753, 8, 9843, 5109, 1536, 1836, 2906, 4528, 910, 127, 266, 21958, 11, 570, 1577, 48193, 18313, 1437, 11378, 162, 549, 24, 16, 33370, 890, 868, 50, 24, 8349, 2017, 7, 40441, 20378, 13310, 7586, 98, 939, 240, 14067, 734, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 939, 351, 75, 4076, 50, 4603, 7586, 8, 5171, 9, 45441, 741, 6, 438, 385, 7586, 479, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1892, 528, 7, 173, 939, 439, 7, 277, 1098, 7586, 89, 1437, 71, 65, 39750, 939, 362, 14194, 122, 456, 127, 2292, 17733, 1411, 7, 291, 13753, 29942, 259, 51, 2294, 1437, 1717, 417, 718, 1879, 9995, 1236, 620, 51, 1311, 129, 65, 9995, 8462, 38315, 13, 94, 130, 377, 29942, 277, 181, 3662, 119, 528, 7, 2292, 17733, 21, 5299, 10219, 2107, 151, 8, 885, 23219, 3788, 7586, 27882, 244, 162, 99, 939, 197, 109, 13, 42, 7586, 1219, 13, 127, 12581, 936, 21, 4727, 7586, 53, 122, 259, 51, 32, 584, 189, 71, 195, 50, 158, 1423, 4926, 47, 240, 14067, 734, 11, 78, 1098, 51, 174, 24, 16, 33370, 890, 5084, 47, 230, 16966, 120, 110, 2530, 1836, 12581, 53, 1717, 64, 24059, 110, 30223, 9, 12581, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[0, 534, 3892, 4791, 9027, 8, 29976, 5578, 1437, 38, 1346, 110, 6882, 59, 5, 696, 8, 24, 16, 5329, 1473, 8, 197, 50118, 3654, 28, 551, 14998, 1437, 2315, 10, 38346, 9027, 31, 10, 205, 821, 19577, 3204, 6393, 8, 41, 29976, 23, 5, 13342, 1437, 85, 64, 28, 50118, 21231, 98, 203, 98, 101, 1668, 50, 64, 95, 28, 1330, 7, 618, 2262, 42363, 20940, 30367, 1437, 2315, 5, 3457, 8, 172, 52, 50118, 7424, 2845, 617, 2], [0, 40181, 55, 335, 4557, 13, 6016, 110, 25860, 4, 38, 524, 1437, 925, 4, 248, 4, 229, 8, 1437, 38, 524, 4343, 7, 3991, 47, 4, 38, 240, 103, 55, 50118, 31480, 137, 1437, 38, 64, 492, 127, 2979, 4, 38, 74, 101, 7, 33, 103, 55, 335, 137, 1437, 38, 64, 492, 127, 2979, 4, 134, 4, 50118, 32112, 19961, 1001, 43511, 50, 10, 12581, 4003, 33716, 626, 116, 176, 4, 1437, 6871, 253, 17591, 16572, 626, 116, 246, 4, 1437, 3945, 47, 10, 33560, 116, 925, 4, 248, 4, 229, 4, 2]]}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
      ],
      "metadata": {
        "id": "zS-6iXTkIrJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "797e35ae-894f-47f2-cb8e-6f4c56207648",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.442138Z",
          "iopub.execute_input": "2023-10-25T07:08:07.442610Z",
          "iopub.status.idle": "2023-10-25T07:08:07.868377Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.442573Z",
          "shell.execute_reply": "2023-10-25T07:08:07.867192Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "eac58315b7d941e6958cad7e009117e8",
            "159b91fe74d344bbafacae20d1ee75c9",
            "ebe0a8fc637844619b6a3995e2406909",
            "74aeea84d1f24b2ba15864ef0752fe08",
            "fe9b3a6e95624633a63bd62378cf1d4d",
            "b4c9d112601d456f943ff5616ebaba47",
            "5bf15dcfc0b849be9412c94e6c7f2237",
            "01455d6b5bd14d099e6bea1ea92a72c9",
            "d0b2ddb2b9b3436bbd96099717600bf8",
            "f164a41db08d4f738dde8f0096ce9960",
            "4b88dce384284e0f8874a9c025e101ae",
            "f6f1da9aaeb3438ba54a9052d6eb1fe7",
            "c65d585d29af45d79a25a426a2116d75",
            "40e581fcf2d642289d148df79b51e27a",
            "e0010ed667104980b844e33dade83e7a",
            "24118ba2ac5e468fb2b5822e92d3ac17",
            "d726311ce8c44965afb0f9ec92d69ef5",
            "bf79f4a2fe7142339cb173b22a57b3e6",
            "73cd557dfb1444539424a03111fde5c1",
            "4195dfc510884309a819331c6422183b",
            "96d6a1537d394be4bd3d83a8c06dcbc7",
            "0c8c84743fcc48d4aefb71561eb31f63"
          ]
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eac58315b7d941e6958cad7e009117e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6f1da9aaeb3438ba54a9052d6eb1fe7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even better, the results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. 🤗 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ],
      "metadata": {
        "id": "voWiw8C7IrJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning the model"
      ],
      "metadata": {
        "id": "545PP3o8IrJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
      ],
      "metadata": {
        "id": "FBiW8UpKIrJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" PyTorch BART model.\"\"\"\n",
        "import copy\n",
        "import math\n",
        "import warnings\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn, einsum\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput,\n",
        "    Seq2SeqQuestionAnsweringModelOutput,\n",
        "    Seq2SeqSequenceClassifierOutput,\n",
        ")\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_end_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers.models.bart.configuration_bart import BartConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n",
        "_CONFIG_FOR_DOC = \"BartConfig\"\n",
        "\n",
        "# Base model docstring\n",
        "_EXPECTED_OUTPUT_SHAPE = [1, 8, 768]\n",
        "\n",
        "# SequenceClassification docstring\n",
        "_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"valhalla/bart-large-sst2\"\n",
        "_SEQ_CLASS_EXPECTED_LOSS = 0.0\n",
        "_SEQ_CLASS_EXPECTED_OUTPUT = \"'POSITIVE'\"\n",
        "\n",
        "# QuestionAsnwering docstring\n",
        "_CHECKPOINT_FOR_QA = \"valhalla/bart-large-finetuned-squadv1\"\n",
        "_QA_EXPECTED_LOSS = 0.59\n",
        "_QA_EXPECTED_OUTPUT = \"' nice puppet'\"\n",
        "\n",
        "\n",
        "BART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"facebook/bart-large\",\n",
        "    # see all BART models at https://huggingface.co/models?filter=bart\n",
        "]\n",
        "\n",
        "\n",
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    if pad_token_id is None:\n",
        "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "def _make_causal_mask(\n",
        "    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n",
        "):\n",
        "    \"\"\"\n",
        "    Make causal mask used for bi-directional self-attention.\n",
        "    \"\"\"\n",
        "    bsz, tgt_len = input_ids_shape\n",
        "    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n",
        "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "\n",
        "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
        "    \"\"\"\n",
        "    bsz, src_len = mask.size()\n",
        "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "\n",
        "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "\n",
        "    inverted_mask = 1.0 - expanded_mask\n",
        "\n",
        "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
        "\n",
        "\n",
        "class BartLearnedPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module learns positional embeddings up to a fixed maximum size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n",
        "        # and adjust num_embeddings appropriately. Other models don't have this hack\n",
        "        self.offset = 2\n",
        "        super().__init__(num_embeddings + self.offset, embedding_dim)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n",
        "        \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n",
        "\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        positions = torch.arange(\n",
        "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
        "        ).expand(bsz, -1)\n",
        "\n",
        "        return super().forward(positions + self.offset)\n",
        "\n",
        "\n",
        "class BartAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        # Re-attention\n",
        "        self.reatten_matrix = nn.Parameter(torch.randn(self.num_heads, self.num_heads))\n",
        "        self.var_norm = nn.BatchNorm2d(self.num_heads)\n",
        "        self.reattn_norm = nn.Sequential(\n",
        "            Rearrange('b h i j -> b i j h'),\n",
        "            nn.LayerNorm(self.num_heads),\n",
        "            Rearrange('b i j h -> b h i j')\n",
        "        )\n",
        "\n",
        "\n",
        "        if (self.head_dim * num_heads) != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "                f\" and `num_heads`: {num_heads}).\"\n",
        "            )\n",
        "        self.scaling = self.head_dim**-0.5\n",
        "        self.reatten_scale = self.scaling\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.proj_drop = nn.Dropout(0.0)\n",
        "        self.attn_drop = nn.Dropout(0.0)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        re_attention = False\n",
        "        is_cross_attention = key_value_states is not None\n",
        "\n",
        "        bsz, tgt_len, _ = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n",
        "        # is checking that the `sequence_length` of the `past_key_value` is the same as\n",
        "        # the provided `key_value_states` to support prefix tuning\n",
        "        if (\n",
        "            is_cross_attention\n",
        "            and past_key_value is not None\n",
        "            and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
        "        ):\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            re_attention = True\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.reshape(*proj_shape)\n",
        "        value_states = value_states.reshape(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
        "                raise ValueError(\n",
        "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "        # Re-attention\n",
        "#         if re_attention:\n",
        "# #         attn_weights = self.attn_drop(attn_weights)\n",
        "#             attn_weights = attn_weights.reshape(bsz, self.num_heads, tgt_len, src_len)\n",
        "#             attn_weights = einsum('b h i j, h g -> b g i j', attn_weights, self.reatten_matrix) * self.reatten_scale\n",
        "# #             attn_weights = self.var_norm(attn_weights) * self.reatten_scale\n",
        "#             attn_weights = attn_weights.reshape(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            if layer_head_mask.size() != (self.num_heads,):\n",
        "                raise ValueError(\n",
        "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
        "                    f\" {layer_head_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = einsum('b h i j, h g -> b g i j', attn_weights, self.reatten_matrix) * self.reatten_scale\n",
        "            attn_weights = self.reattn_norm(attn_weights)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit awkward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to be reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "\n",
        "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
        "        # partitioned across GPUs when using tensor-parallelism.\n",
        "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value\n",
        "\n",
        "\n",
        "class BartEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = BartAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.FloatTensor,\n",
        "        attention_mask: torch.FloatTensor,\n",
        "        layer_head_mask: torch.FloatTensor,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
        "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(encoder_attention_heads,)`.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "        hidden_states, attn_weights, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        if hidden_states.dtype == torch.float16 and (\n",
        "            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n",
        "        ):\n",
        "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
        "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BartDecoderLayer(nn.Module):\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "\n",
        "        self.self_attn = BartAttention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = BartAttention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = True,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
        "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            encoder_hidden_states (`torch.FloatTensor`):\n",
        "                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n",
        "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(encoder_attention_heads,)`.\n",
        "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
        "                size `(decoder_attention_heads,)`.\n",
        "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Self Attention\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        cross_attn_present_key_value = None\n",
        "        cross_attn_weights = None\n",
        "        if encoder_hidden_states is not None:\n",
        "            residual = hidden_states\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
        "                hidden_states=hidden_states,\n",
        "                key_value_states=encoder_hidden_states,\n",
        "                attention_mask=encoder_attention_mask,\n",
        "                layer_head_mask=cross_attn_layer_head_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "            hidden_states = residual + hidden_states\n",
        "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
        "\n",
        "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights, cross_attn_weights)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BartClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        inner_dim: int,\n",
        "        num_classes: int,\n",
        "        pooler_dropout: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inner_dim)\n",
        "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = torch.tanh(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.out_proj(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BartPreTrainedModel(PreTrainedModel):\n",
        "    config_class = BartConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _keys_to_ignore_on_load_unexpected = [\"encoder.version\", \"decoder.version\"]\n",
        "    _no_split_modules = [r\"BartEncoderLayer\", r\"BartDecoderLayer\"]\n",
        "    _skip_keys_device_placement = \"past_key_values\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, (BartDecoder, BartEncoder)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs\n",
        "\n",
        "\n",
        "class PretrainedBartModel(BartPreTrainedModel):\n",
        "    def __init_subclass__(self):\n",
        "        warnings.warn(\n",
        "            \"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\",\n",
        "            FutureWarning,\n",
        "        )\n",
        "\n",
        "\n",
        "class BartPretrainedModel(BartPreTrainedModel):\n",
        "    def __init_subclass__(self):\n",
        "        warnings.warn(\n",
        "            \"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\",\n",
        "            FutureWarning,\n",
        "        )\n",
        "\n",
        "\n",
        "BART_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "    etc.)\n",
        "\n",
        "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "    and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config ([`BartConfig`]):\n",
        "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
        "            load the weights associated with the model, only the configuration. Check out the\n",
        "            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "BART_GENERATION_EXAMPLE = r\"\"\"\n",
        "    Summarization example:\n",
        "\n",
        "    ```python\n",
        "    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "\n",
        "    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "    >>> ARTICLE_TO_SUMMARIZE = (\n",
        "    ...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    ...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
        "    ...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
        "    ... )\n",
        "    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "    >>> # Generate Summary\n",
        "    >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
        "    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "    'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n",
        "    ```\n",
        "\n",
        "    Mask filling example:\n",
        "\n",
        "    ```python\n",
        "    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "\n",
        "    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n",
        "    >>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
        "    >>> logits = model(input_ids).logits\n",
        "\n",
        "    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
        "    >>> probs = logits[0, masked_index].softmax(dim=0)\n",
        "    >>> values, predictions = probs.topk(5)\n",
        "\n",
        "    >>> tokenizer.decode(predictions).split()\n",
        "    ['not', 'good', 'healthy', 'great', 'very']\n",
        "    ```\n",
        "\"\"\"\n",
        "\n",
        "BART_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
        "            it.\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            [What are attention masks?](../glossary#attention-mask)\n",
        "        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Indices of decoder input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            [What are decoder input IDs?](../glossary#decoder-input-ids)\n",
        "\n",
        "            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n",
        "            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n",
        "\n",
        "            For translation and summarization training, `decoder_input_ids` should be provided. If no\n",
        "            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n",
        "            for denoising pre-training following the paper.\n",
        "        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n",
        "            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n",
        "            be used by default.\n",
        "\n",
        "            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n",
        "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
        "            information on the default strategy.\n",
        "        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n",
        "            1]`:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n",
        "            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n",
        "            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n",
        "            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
        "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
        "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
        "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n",
        "            `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n",
        "            can choose to directly pass an embedded representation. This is useful if you want more control over how to\n",
        "            convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n",
        "            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n",
        "            input (see `past_key_values`). This is useful if you want more control over how to convert\n",
        "            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n",
        "\n",
        "            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n",
        "            of `inputs_embeds`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BartEncoder(BartPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    [`BartEncoderLayer`].\n",
        "\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens.weight = embed_tokens.weight\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutput]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "                [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "                [What are input IDs?](../glossary#input-ids)\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "\n",
        "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
        "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
        "                than the model's internal embedding lookup matrix.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input = input_ids\n",
        "            input_ids = input_ids.view(-1, input_ids.shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input = inputs_embeds[:, :, -1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(input)\n",
        "        embed_pos = embed_pos.to(inputs_embeds.device)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = self.layernorm_embedding(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            if head_mask.size()[0] != (len(self.layers)):\n",
        "                raise ValueError(\n",
        "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
        "                    f\" {head_mask.size()[0]}.\"\n",
        "                )\n",
        "\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            to_drop = False\n",
        "            if self.training:\n",
        "                dropout_probability = torch.rand([])\n",
        "                if dropout_probability < self.layerdrop:  # skip the layer\n",
        "                    to_drop = True\n",
        "\n",
        "            if to_drop:\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )\n",
        "\n",
        "\n",
        "class BartDecoder(BartPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]\n",
        "\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens.weight = embed_tokens.weight\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.d_model,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n",
        "        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "        # create causal mask\n",
        "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "        combined_attention_mask = None\n",
        "        if input_shape[-1] > 1:\n",
        "            combined_attention_mask = _make_causal_mask(\n",
        "                input_shape,\n",
        "                inputs_embeds.dtype,\n",
        "                device=inputs_embeds.device,\n",
        "                past_key_values_length=past_key_values_length,\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n",
        "                inputs_embeds.device\n",
        "            )\n",
        "            combined_attention_mask = (\n",
        "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "            )\n",
        "\n",
        "        return combined_attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "                [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "                [What are input IDs?](../glossary#input-ids)\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n",
        "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
        "                of the decoder.\n",
        "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n",
        "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
        "                selected in `[0, 1]`:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "\n",
        "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n",
        "                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "\n",
        "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
        "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
        "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
        "\n",
        "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
        "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
        "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
        "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of\n",
        "                shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing\n",
        "                `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more\n",
        "                control over how to convert `input_ids` indices into associated vectors than the model's internal\n",
        "                embedding lookup matrix.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input = input_ids\n",
        "            input_shape = input.shape\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "            input = inputs_embeds[:, :, -1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input) * self.embed_scale\n",
        "\n",
        "        attention_mask = self._prepare_decoder_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "\n",
        "        # expand encoder attention mask\n",
        "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input, past_key_values_length)\n",
        "        positions = positions.to(inputs_embeds.device)\n",
        "\n",
        "        hidden_states = inputs_embeds + positions\n",
        "        hidden_states = self.layernorm_embedding(hidden_states)\n",
        "\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            if use_cache:\n",
        "                logger.warning_once(\n",
        "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                )\n",
        "                use_cache = False\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
        "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
        "            if attn_mask is not None:\n",
        "                if attn_mask.size()[0] != (len(self.layers)):\n",
        "                    raise ValueError(\n",
        "                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
        "                        f\" {head_mask.size()[0]}.\"\n",
        "                    )\n",
        "\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "            if self.training:\n",
        "                dropout_probability = torch.rand([])\n",
        "                if dropout_probability < self.layerdrop:\n",
        "                    continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        # None for past_key_value\n",
        "                        return module(*inputs, output_attentions, use_cache)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(decoder_layer),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    head_mask[idx] if head_mask is not None else None,\n",
        "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
        "                    None,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    encoder_attention_mask=encoder_attention_mask,\n",
        "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                    cross_attn_layer_head_mask=(\n",
        "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
        "                    ),\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "                if encoder_hidden_states is not None:\n",
        "                    all_cross_attentions += (layer_outputs[2],)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n",
        "    BART_START_DOCSTRING,\n",
        ")\n",
        "class BartModel(BartPreTrainedModel):\n",
        "    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        self.encoder = BartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.decoder\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=Seq2SeqModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        expected_output=_EXPECTED_OUTPUT_SHAPE,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, Seq2SeqModelOutput]:\n",
        "        # different to other models, Bart automatically creates decoder_input_ids from\n",
        "        # input_ids if no decoder_input_ids are provided\n",
        "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            if input_ids is None:\n",
        "                raise ValueError(\n",
        "                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n",
        "                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n",
        "                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n",
        "                )\n",
        "\n",
        "            decoder_input_ids = shift_tokens_right(\n",
        "                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "            )\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n",
        ")\n",
        "class BartForConditionalGeneration(BartPreTrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n",
        "    _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n",
        "\n",
        "    def __init__(self, config: BartConfig):\n",
        "        super().__init__(config)\n",
        "        self.model = BartModel(config)\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_encoder(self):\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n",
        "        self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
        "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
        "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
        "\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            if use_cache:\n",
        "                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
        "            use_cache = False\n",
        "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        lm_logits = self.lm_head(outputs[0])\n",
        "        lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            labels = labels.to(lm_logits.device)\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids,\n",
        "        past_key_values=None,\n",
        "        attention_mask=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # cut decoder_input_ids if past_key_values is used\n",
        "        if past_key_values is not None:\n",
        "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past_key_values,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"decoder_attention_mask\": decoder_attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"decoder_head_mask\": decoder_head_mask,\n",
        "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n",
        "                + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n",
        "    tasks.\n",
        "    \"\"\",\n",
        "    BART_START_DOCSTRING,\n",
        ")\n",
        "class BartForSequenceClassification(BartPreTrainedModel):\n",
        "    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
        "\n",
        "    def __init__(self, config: BartConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.model = BartModel(config)\n",
        "        self.classification_head = BartClassificationHead(\n",
        "            config.d_model,\n",
        "            config.d_model,\n",
        "            config.num_labels,\n",
        "            config.classifier_dropout,\n",
        "        )\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n",
        "        output_type=Seq2SeqSequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n",
        "        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is None and inputs_embeds is not None:\n",
        "            raise NotImplementedError(\n",
        "                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n",
        "            )\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = outputs[0]  # last hidden state\n",
        "\n",
        "        eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n",
        "\n",
        "        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n",
        "            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n",
        "        sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n",
        "            :, -1, :\n",
        "        ]\n",
        "        logits = self.classification_head(sentence_representation)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = labels.to(logits.device)\n",
        "            if self.config.problem_type is None:\n",
        "                if self.config.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.config.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return Seq2SeqSequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    BART_START_DOCSTRING,\n",
        ")\n",
        "class BartForQuestionAnswering(BartPreTrainedModel):\n",
        "    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        config.num_labels = 2\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.model = BartModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_QA,\n",
        "        output_type=Seq2SeqQuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        expected_loss=_QA_EXPECTED_LOSS,\n",
        "        expected_output=_QA_EXPECTED_OUTPUT,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
        "        start_positions: Optional[torch.LongTensor] = None,\n",
        "        end_positions: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n",
        "        r\"\"\"\n",
        "        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n",
        "            are not taken into account for computing the loss.\n",
        "        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n",
        "            are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (\n",
        "                start_logits,\n",
        "                end_logits,\n",
        "            ) + outputs[1:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return Seq2SeqQuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class BartDecoderWrapper(BartPreTrainedModel):\n",
        "    \"\"\"\n",
        "    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\n",
        "    used in combination with the [`EncoderDecoderModel`] framework.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.decoder = BartDecoder(config)\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.decoder(*args, **kwargs)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    BART decoder with with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "    \"\"\",\n",
        "    BART_START_DOCSTRING,\n",
        ")\n",
        "class BartForCausalLM(BartPreTrainedModel):\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        config = copy.deepcopy(config)\n",
        "        config.is_decoder = True\n",
        "        config.is_encoder_decoder = False\n",
        "        super().__init__(config)\n",
        "        self.model = BartDecoderWrapper(config)\n",
        "\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.decoder.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.decoder.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model.decoder = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.decoder\n",
        "\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "                [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "                [What are input IDs?](../glossary#input-ids)\n",
        "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                [What are attention masks?](../glossary#attention-mask)\n",
        "            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
        "                if the model is configured as a decoder.\n",
        "            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n",
        "                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
        "            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "\n",
        "            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n",
        "                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the head is **masked**.\n",
        "\n",
        "            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
        "                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n",
        "                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n",
        "                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n",
        "                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n",
        "\n",
        "                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n",
        "                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
        "\n",
        "                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n",
        "                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n",
        "                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
        "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
        "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
        "            use_cache (`bool`, *optional*):\n",
        "                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
        "                (see `past_key_values`).\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "            output_attentions (`bool`, *optional*):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (`bool`, *optional*):\n",
        "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (`bool`, *optional*):\n",
        "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```python\n",
        "        >>> from transformers import AutoTokenizer, BartForCausalLM\n",
        "\n",
        "        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "        >>> model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)\n",
        "        >>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n",
        "        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "        >>> outputs = model(**inputs)\n",
        "\n",
        "        >>> logits = outputs.logits\n",
        "        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n",
        "        >>> list(logits.shape) == expected_shape\n",
        "        True\n",
        "        ```\"\"\"\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs = self.model.decoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        logits = self.lm_head(outputs[0])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = labels.to(logits.device)\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return (loss,) + output if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, **kwargs\n",
        "    ):\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_ids.shape)\n",
        "\n",
        "        if past_key_values:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "        # first step, decoder_cached_states are empty\n",
        "        return {\n",
        "            \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"past_key_values\": past_key_values,\n",
        "            \"use_cache\": use_cache,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
        "            )\n",
        "        return reordered_past"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:07.870821Z",
          "iopub.execute_input": "2023-10-25T07:08:07.871242Z",
          "iopub.status.idle": "2023-10-25T07:08:08.120633Z",
          "shell.execute_reply.started": "2023-10-25T07:08:07.871185Z",
          "shell.execute_reply": "2023-10-25T07:08:08.119438Z"
        },
        "trusted": true,
        "id": "HTquMOOP0lS5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"GuysTrans/bart-base-re-attention-mini-seq-512\")"
      ],
      "metadata": {
        "id": "TlqNaB8jIrJW",
        "outputId": "d8265d49-a3cc-4ca7-9457-97973ccea706",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:08.122073Z",
          "iopub.execute_input": "2023-10-25T07:08:08.122752Z",
          "iopub.status.idle": "2023-10-25T07:08:11.390207Z",
          "shell.execute_reply.started": "2023-10-25T07:08:08.122714Z",
          "shell.execute_reply": "2023-10-25T07:08:11.389196Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at GuysTrans/bart-base-re-attention-mini-seq-512 and are newly initialized: ['model.decoder.layers.2.encoder_attn.var_norm.bias', 'model.encoder.layers.0.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.0.self_attn.reattn_norm.1.bias', 'model.decoder.layers.3.self_attn.var_norm.bias', 'model.decoder.layers.0.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.4.encoder_attn.var_norm.weight', 'model.encoder.layers.2.self_attn.var_norm.weight', 'model.decoder.layers.4.encoder_attn.var_norm.bias', 'model.decoder.layers.1.self_attn.var_norm.running_mean', 'model.encoder.layers.4.self_attn.var_norm.running_var', 'model.encoder.layers.4.self_attn.var_norm.weight', 'model.decoder.layers.4.encoder_attn.reattn_norm.1.bias', 'model.decoder.layers.3.encoder_attn.reattn_norm.1.bias', 'model.decoder.layers.1.encoder_attn.reattn_norm.1.weight', 'model.decoder.layers.1.self_attn.var_norm.num_batches_tracked', 'model.encoder.layers.0.self_attn.var_norm.weight', 'model.encoder.layers.4.self_attn.reattn_norm.1.bias', 'model.decoder.layers.4.self_attn.var_norm.running_var', 'model.encoder.layers.4.self_attn.var_norm.running_mean', 'model.encoder.layers.3.self_attn.var_norm.running_var', 'model.encoder.layers.5.self_attn.reattn_norm.1.weight', 'model.decoder.layers.1.encoder_attn.reattn_norm.1.bias', 'model.encoder.layers.5.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.2.self_attn.var_norm.running_var', 'model.encoder.layers.0.self_attn.reattn_norm.1.weight', 'model.decoder.layers.0.encoder_attn.var_norm.running_mean', 'model.encoder.layers.0.self_attn.reattn_norm.1.bias', 'model.encoder.layers.3.self_attn.var_norm.bias', 'model.decoder.layers.2.encoder_attn.var_norm.running_var', 'model.encoder.layers.3.self_attn.reattn_norm.1.weight', 'model.decoder.layers.5.encoder_attn.reattn_norm.1.weight', 'model.decoder.layers.4.self_attn.reattn_norm.1.weight', 'model.encoder.layers.4.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.5.self_attn.var_norm.running_mean', 'model.decoder.layers.3.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.5.encoder_attn.var_norm.bias', 'model.decoder.layers.5.self_attn.reattn_norm.1.weight', 'model.encoder.layers.3.self_attn.var_norm.running_mean', 'model.encoder.layers.1.self_attn.var_norm.weight', 'model.decoder.layers.0.self_attn.var_norm.running_mean', 'model.encoder.layers.2.self_attn.reattn_norm.1.weight', 'model.decoder.layers.1.self_attn.reattn_norm.1.weight', 'model.decoder.layers.2.self_attn.reattn_norm.1.weight', 'model.decoder.layers.2.self_attn.var_norm.bias', 'model.decoder.layers.2.encoder_attn.var_norm.num_batches_tracked', 'model.encoder.layers.3.self_attn.var_norm.weight', 'model.decoder.layers.5.encoder_attn.reattn_norm.1.bias', 'model.decoder.layers.3.encoder_attn.var_norm.running_var', 'model.decoder.layers.1.self_attn.var_norm.running_var', 'model.decoder.layers.0.encoder_attn.reattn_norm.1.bias', 'model.decoder.layers.0.encoder_attn.var_norm.running_var', 'model.decoder.layers.0.encoder_attn.var_norm.weight', 'model.decoder.layers.1.self_attn.var_norm.weight', 'model.decoder.layers.1.encoder_attn.var_norm.running_var', 'model.decoder.layers.4.encoder_attn.var_norm.running_mean', 'model.decoder.layers.4.encoder_attn.reattn_norm.1.weight', 'model.encoder.layers.4.self_attn.reattn_norm.1.weight', 'model.decoder.layers.4.encoder_attn.var_norm.num_batches_tracked', 'model.decoder.layers.4.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.0.encoder_attn.reattn_norm.1.weight', 'model.decoder.layers.0.self_attn.var_norm.bias', 'model.encoder.layers.1.self_attn.reattn_norm.1.weight', 'model.decoder.layers.0.self_attn.var_norm.running_var', 'model.encoder.layers.2.self_attn.var_norm.running_mean', 'model.decoder.layers.5.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.1.encoder_attn.var_norm.num_batches_tracked', 'model.encoder.layers.1.self_attn.reattn_norm.1.bias', 'model.decoder.layers.2.self_attn.var_norm.weight', 'model.decoder.layers.0.encoder_attn.var_norm.bias', 'model.decoder.layers.0.self_attn.var_norm.weight', 'model.decoder.layers.0.self_attn.reattn_norm.1.weight', 'model.encoder.layers.5.self_attn.var_norm.running_var', 'model.decoder.layers.1.encoder_attn.var_norm.running_mean', 'model.decoder.layers.3.encoder_attn.var_norm.running_mean', 'model.decoder.layers.5.self_attn.var_norm.bias', 'model.decoder.layers.4.self_attn.reattn_norm.1.bias', 'model.encoder.layers.0.self_attn.var_norm.running_var', 'model.encoder.layers.1.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.3.self_attn.reattn_norm.1.weight', 'model.encoder.layers.2.self_attn.reattn_norm.1.bias', 'model.decoder.layers.1.self_attn.var_norm.bias', 'model.encoder.layers.5.self_attn.var_norm.running_mean', 'model.encoder.layers.3.self_attn.reattn_norm.1.bias', 'model.encoder.layers.1.self_attn.var_norm.bias', 'model.decoder.layers.3.self_attn.var_norm.running_var', 'model.decoder.layers.2.encoder_attn.var_norm.running_mean', 'model.decoder.layers.3.encoder_attn.var_norm.num_batches_tracked', 'model.decoder.layers.5.self_attn.var_norm.weight', 'model.encoder.layers.3.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.5.encoder_attn.var_norm.running_mean', 'model.encoder.layers.1.self_attn.var_norm.running_mean', 'model.decoder.layers.1.self_attn.reattn_norm.1.bias', 'model.decoder.layers.1.encoder_attn.var_norm.bias', 'model.decoder.layers.5.self_attn.reattn_norm.1.bias', 'model.decoder.layers.5.encoder_attn.var_norm.weight', 'model.decoder.layers.5.encoder_attn.var_norm.num_batches_tracked', 'model.decoder.layers.2.encoder_attn.reattn_norm.1.bias', 'model.decoder.layers.2.encoder_attn.reattn_norm.1.weight', 'model.decoder.layers.4.self_attn.var_norm.weight', 'model.encoder.layers.4.self_attn.var_norm.bias', 'model.encoder.layers.5.self_attn.var_norm.bias', 'model.decoder.layers.3.encoder_attn.reattn_norm.1.weight', 'model.encoder.layers.2.self_attn.var_norm.bias', 'model.decoder.layers.5.self_attn.var_norm.running_var', 'model.decoder.layers.3.encoder_attn.var_norm.weight', 'model.decoder.layers.2.encoder_attn.var_norm.weight', 'model.decoder.layers.5.encoder_attn.var_norm.running_var', 'model.decoder.layers.2.self_attn.var_norm.num_batches_tracked', 'model.decoder.layers.4.self_attn.var_norm.running_mean', 'model.encoder.layers.1.self_attn.var_norm.running_var', 'model.decoder.layers.4.encoder_attn.var_norm.running_var', 'model.decoder.layers.1.encoder_attn.var_norm.weight', 'model.decoder.layers.3.self_attn.var_norm.running_mean', 'model.decoder.layers.2.self_attn.reattn_norm.1.bias', 'model.decoder.layers.3.encoder_attn.var_norm.bias', 'model.encoder.layers.2.self_attn.var_norm.num_batches_tracked', 'model.encoder.layers.2.self_attn.var_norm.running_var', 'model.decoder.layers.2.self_attn.var_norm.running_mean', 'model.encoder.layers.0.self_attn.var_norm.bias', 'model.encoder.layers.5.self_attn.var_norm.weight', 'model.decoder.layers.4.self_attn.var_norm.bias', 'model.encoder.layers.5.self_attn.reattn_norm.1.bias', 'model.decoder.layers.3.self_attn.reattn_norm.1.bias', 'model.decoder.layers.3.self_attn.var_norm.weight', 'model.encoder.layers.0.self_attn.var_norm.running_mean', 'model.decoder.layers.0.encoder_attn.var_norm.num_batches_tracked']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
      ],
      "metadata": {
        "id": "CczA5lJlIrJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To instantiate a `Seq2SeqTrainer`, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ],
      "metadata": {
        "id": "_N8urzhyIrJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-re-attention-mini-seq-512\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "#     push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ie_CLhlEiIFq",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:11.391816Z",
          "iopub.execute_input": "2023-10-25T07:08:11.392115Z",
          "iopub.status.idle": "2023-10-25T07:08:11.406434Z",
          "shell.execute_reply.started": "2023-10-25T07:08:11.392088Z",
          "shell.execute_reply": "2023-10-25T07:08:11.405121Z"
        },
        "trusted": true
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the cell and customize the weight decay. Since the `Seq2SeqTrainer` will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the `predict_with_generate` option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n",
        "\n",
        "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/t5-finetuned-xsum\"` or `\"huggingface/t5-finetuned-xsum\"`).\n",
        "\n",
        "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
      ],
      "metadata": {
        "id": "km3pGVdTIrJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "lP7bL6M3iIFr",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:11.407639Z",
          "iopub.execute_input": "2023-10-25T07:08:11.407959Z",
          "iopub.status.idle": "2023-10-25T07:08:11.415572Z",
          "shell.execute_reply.started": "2023-10-25T07:08:11.407934Z",
          "shell.execute_reply": "2023-10-25T07:08:11.414633Z"
        },
        "trusted": true
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
      ],
      "metadata": {
        "id": "7sZOdRlRIrJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    # Note that other metrics may not have a `use_aggregator` parameter\n",
        "    # and thus will return a list, computing a metric for each sentence.\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "UmvbnJ9JIrJd",
        "outputId": "1a62717f-14e2-4893-ca29-f97b8cb74223",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:11.416803Z",
          "iopub.execute_input": "2023-10-25T07:08:11.417525Z",
          "iopub.status.idle": "2023-10-25T07:08:11.431551Z",
          "shell.execute_reply.started": "2023-10-25T07:08:11.417486Z",
          "shell.execute_reply": "2023-10-25T07:08:11.430537Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.python.sh | bash"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:11.432776Z",
          "iopub.execute_input": "2023-10-25T07:08:11.433115Z",
          "iopub.status.idle": "2023-10-25T07:08:13.065981Z",
          "shell.execute_reply.started": "2023-10-25T07:08:11.433084Z",
          "shell.execute_reply": "2023-10-25T07:08:13.064828Z"
        },
        "trusted": true,
        "id": "4kd3r-ly0lTF",
        "outputId": "1761a8f8-55ea-4693-c550-b29c157fc22b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already configured pip for this repository, skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git-lfs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:13.068003Z",
          "iopub.execute_input": "2023-10-25T07:08:13.068515Z",
          "iopub.status.idle": "2023-10-25T07:08:15.514938Z",
          "shell.execute_reply.started": "2023-10-25T07:08:13.068461Z",
          "shell.execute_reply": "2023-10-25T07:08:15.513777Z"
        },
        "trusted": true,
        "id": "m9w7e6mI0lTF",
        "outputId": "49382431-ee7d-4962-a014-6585e6a98bcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"
      ],
      "metadata": {
        "id": "rXuFTAzDIrJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "imY1oC3SIrJf",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:08:15.516966Z",
          "iopub.execute_input": "2023-10-25T07:08:15.517450Z",
          "iopub.status.idle": "2023-10-25T07:08:15.688397Z",
          "shell.execute_reply.started": "2023-10-25T07:08:15.517409Z",
          "shell.execute_reply": "2023-10-25T07:08:15.687287Z"
        },
        "trusted": true
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ],
      "metadata": {
        "id": "CdzABDVcIrJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    trainer.train()\n",
        "#     trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:44:18.042489Z",
          "iopub.execute_input": "2023-10-25T07:44:18.043566Z"
        },
        "trusted": true,
        "outputId": "8762c98b-4d63-4d2d-b24b-50d81995882a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.025574</td>\n",
              "      <td>35.426200</td>\n",
              "      <td>33.372600</td>\n",
              "      <td>35.332400</td>\n",
              "      <td>35.522900</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 86/125 00:18 < 00:08, 4.63 it/s, Epoch 0.68/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "GNAvTDPSaW1H",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:31:24.630579Z",
          "iopub.execute_input": "2023-10-25T07:31:24.630970Z",
          "iopub.status.idle": "2023-10-25T07:31:24.658370Z",
          "shell.execute_reply.started": "2023-10-25T07:31:24.630924Z",
          "shell.execute_reply": "2023-10-25T07:31:24.657277Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_before_finetuned = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "qGwajSFvp13X",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:31:24.659817Z",
          "iopub.execute_input": "2023-10-25T07:31:24.660183Z",
          "iopub.status.idle": "2023-10-25T07:31:35.450059Z",
          "shell.execute_reply.started": "2023-10-25T07:31:24.660148Z",
          "shell.execute_reply": "2023-10-25T07:31:35.448767Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(test_samples, model):\n",
        "    inputs = tokenizer(\n",
        "        test_samples[\"question\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "    attention_mask = inputs.attention_mask.to(model.device)\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=512)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return outputs, output_str"
      ],
      "metadata": {
        "id": "oSce-pEPZ0Jq",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:31:35.451907Z",
          "iopub.execute_input": "2023-10-25T07:31:35.452641Z",
          "iopub.status.idle": "2023-10-25T07:31:35.461715Z",
          "shell.execute_reply.started": "2023-10-25T07:31:35.452599Z",
          "shell.execute_reply": "2023-10-25T07:31:35.460635Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = vals_ds.select(range(8))\n",
        "\n",
        "\n",
        "summaries_before_tuning = generate_summary(test_samples, model_before_finetuned)[1]\n",
        "summaries_after_tuning = generate_summary(test_samples, model)[1]"
      ],
      "metadata": {
        "id": "OX4PJRRCZ5Wl",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:31:35.463320Z",
          "iopub.execute_input": "2023-10-25T07:31:35.463653Z",
          "iopub.status.idle": "2023-10-25T07:34:40.088950Z",
          "shell.execute_reply.started": "2023-10-25T07:31:35.463621Z",
          "shell.execute_reply": "2023-10-25T07:34:40.087878Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    tabulate(\n",
        "        zip(\n",
        "            range(len(summaries_after_tuning)),\n",
        "            summaries_after_tuning,\n",
        "            summaries_before_tuning,\n",
        "        ),\n",
        "        headers=[\"Id\", \"Answer after\", \"Answer before\"],\n",
        "    )\n",
        ")\n",
        "print(\"\\nTarget answers:\\n\")\n",
        "print(\n",
        "    tabulate(list(enumerate(test_samples[\"answer\"])), headers=[\"Id\", \"Target answer\"])\n",
        ")\n",
        "print(\"\\nSource questions:\\n\")\n",
        "print(tabulate(list(enumerate(test_samples[\"question\"])), headers=[\"Id\", \"Question\"]))"
      ],
      "metadata": {
        "id": "ghUn2o8jaRSm",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:34:40.090469Z",
          "iopub.execute_input": "2023-10-25T07:34:40.090820Z",
          "iopub.status.idle": "2023-10-25T07:34:40.138406Z",
          "shell.execute_reply.started": "2023-10-25T07:34:40.090787Z",
          "shell.execute_reply": "2023-10-25T07:34:40.137373Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries_after_tuning"
      ],
      "metadata": {
        "id": "iVEDsRW6sc_j",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:34:40.140075Z",
          "iopub.execute_input": "2023-10-25T07:34:40.140490Z",
          "iopub.status.idle": "2023-10-25T07:34:40.149924Z",
          "shell.execute_reply.started": "2023-10-25T07:34:40.140456Z",
          "shell.execute_reply": "2023-10-25T07:34:40.148961Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(enumerate(test_samples[\"answer\"]))"
      ],
      "metadata": {
        "id": "NJVLQqcvuZBx",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:34:40.151145Z",
          "iopub.execute_input": "2023-10-25T07:34:40.151509Z",
          "iopub.status.idle": "2023-10-25T07:34:40.161783Z",
          "shell.execute_reply.started": "2023-10-25T07:34:40.151484Z",
          "shell.execute_reply": "2023-10-25T07:34:40.160690Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline, Conversation\n",
        "\n",
        "# chatbot = pipeline(\"conversational\", model=model, tokenizer=tokenizer)\n",
        "# conversation = Conversation(\"hello am 22 years old and i have type 2diabet i wanted to sign for a new gym which use electric vibes to improve heart pulses and fasten the process of loosing weight i wanted to know if it is dangerous for me knowing that 20min of this sports equals 4 h of normal one thanks\t\")\n",
        "# conversation = chatbot(conversation)\n",
        "# conversation.generated_responses[-1]"
      ],
      "metadata": {
        "id": "ImT5drFr5fzw",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:34:40.163938Z",
          "iopub.execute_input": "2023-10-25T07:34:40.164616Z",
          "iopub.status.idle": "2023-10-25T07:34:40.171299Z",
          "shell.execute_reply.started": "2023-10-25T07:34:40.164579Z",
          "shell.execute_reply": "2023-10-25T07:34:40.170154Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now upload the result of the training to the Hub, just execute this instruction:"
      ],
      "metadata": {
        "id": "b5rItmHxiIFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "NPyW6RDjiIFs",
        "execution": {
          "iopub.status.busy": "2023-10-25T07:34:40.172517Z",
          "iopub.execute_input": "2023-10-25T07:34:40.172766Z",
          "iopub.status.idle": "2023-10-25T07:35:16.475983Z",
          "shell.execute_reply.started": "2023-10-25T07:34:40.172743Z",
          "shell.execute_reply": "2023-10-25T07:35:16.474757Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"sgugger/my-awesome-model\")\n",
        "```"
      ],
      "metadata": {
        "id": "xqle58ISiIFs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OnvVbSnOiIFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}