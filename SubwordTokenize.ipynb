{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "SubwordTokenize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuyRobot/AINotesBook/blob/main/SubwordTokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7fhTPF3onI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572d5d37-3149-4945-861f-8eaea7ba3ca3"
      },
      "source": [
        "!pip install -q tensorflow_datasets\n",
        "# `BertTokenizer.detokenize` is not in `tf-text` stable yet (currently 2.4.3).\n",
        "!pip install -q tensorflow_text_nightly\n",
        "# tf-text-nightly resquires tf-nightly\n",
        "!pip install -q tf-nightly "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4MB 5.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 454.3MB 35kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 40.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 44.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0MB 35.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 40.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 42.4MB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "L9gyWjwYoac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e74f1eb-e435-45f3-df82-7b6e49ad6e3a"
      },
      "source": [
        "import collections\n",
        "import io\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Enabling eager execution\n",
            "INFO:tensorflow:Enabling v2 tensorshape\n",
            "INFO:tensorflow:Enabling resource variables\n",
            "INFO:tensorflow:Enabling tensor equality\n",
            "INFO:tensorflow:Enabling control flow v2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQJyQkIyojba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive_mount_path = \"/content/data\"\n",
        "drive.mount(drive_mount_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kdMON81Joac8"
      },
      "source": [
        "path_data_file = \"/content/data/MyDrive/data/rus-eng/rus.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bYJwJolvoac8"
      },
      "source": [
        "def unicode_to_ascii(w):\n",
        "    return ''.join(unicodedata.normalize(\"NFD\", c) for c in w if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "word = \"Оно там?\"\n",
        "unicode_to_ascii(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8Cv_5dXSoac9"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w)\n",
        "\n",
        "    w = re.sub(r\"([.!?,])\", r\" \\1\", w)\n",
        "    w = re.sub(r'[\" ]', \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "    return w\n",
        "    # return \"<start> %s <end>\" % w\n",
        "\n",
        "preprocess_sentence(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7LxiaETRoac9"
      },
      "source": [
        "def create_dataset(path, num_instance, from_end=False):\n",
        "    lines = io.open(path, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "    lines = lines[-num_instance:] if from_end else lines[:num_instance]\n",
        "    return zip(*[[preprocess_sentence(w) for w in line.split(\"\\t\")[:2]] for line in lines])\n",
        "\n",
        "a, b = create_dataset(path_data_file, 100, from_end=True)\n",
        "a[0], b[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuSuPN3miEPi"
      },
      "source": [
        "# Test ...\n",
        "x = list(map(lambda x: tf.convert_to_tensor(x), a))\n",
        "y = list(map(lambda y: tf.convert_to_tensor(y), b))\n",
        "for i in zip(x, y):\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV_X2kNJlyZe"
      },
      "source": [
        "len(x), len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAjL6_fhk2Lj"
      },
      "source": [
        "BUFFER_SIZE = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).cache().prefetch(1)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uKaEdZ5mYZ9"
      },
      "source": [
        "for pt, en in dataset.take(1):\n",
        "    print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "    print(\"English:   \", en.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NiY_OjiLoac9"
      },
      "source": [
        "\n",
        "BUFFER_SIZE = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "def tokenize(texts):\n",
        "    \"\"\"\n",
        "\n",
        "    :param texts: the text to tokenize\n",
        "    :return: the tensors and tokenizer of the texts\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(texts)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n",
        "\n",
        "    return tensor, tokenizer\n",
        "\n",
        "\n",
        "def load_dataset(path, num_instance):\n",
        "    tar, inp = create_dataset(path, num_instance)\n",
        "\n",
        "    tar_tensor, tar_tokenizer = tokenize(tar)\n",
        "    inp_tensor, inp_tokenizer = tokenize(inp)\n",
        "\n",
        "    return tar_tensor, inp_tensor, tar_tokenizer, inp_tokenizer\n",
        "\n",
        "\n",
        "def load_texts_train_val_dataset(path, num_instance, from_end=False):\n",
        "    tar, inp = create_dataset(path, num_instance, from_end=from_end)\n",
        "\n",
        "    inp = list(map(lambda x: tf.convert_to_tensor(x), inp))\n",
        "    tar = list(map(lambda y: tf.convert_to_tensor(y), tar))\n",
        "\n",
        "    train_inp, val_inp, train_tar, val_tar = train_test_split(inp, tar)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_inp, train_tar))\n",
        "    # train_dataset = dataset.shuffle(BUFFER_SIZE).cache().prefetch(1)\n",
        "    # for i in zip(train_inp, train_tar):\n",
        "    #     print(i)\n",
        "    #     break\n",
        "    # for i in train_dataset:\n",
        "    #     print(i)\n",
        "    #     break\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((val_inp, val_tar))\n",
        "    # val_dataset = dataset.shuffle(BUFFER_SIZE).cache().prefetch(1)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def load_dataset(path, num_instance, from_end=False):\n",
        "    tar, inp = create_dataset(path, num_instance, from_end=from_end)\n",
        "\n",
        "    inp = list(map(lambda x: tf.convert_to_tensor(x), inp))\n",
        "    tar = list(map(lambda y: tf.convert_to_tensor(y), tar))\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((inp, tar))\n",
        "\n",
        "    return train_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_g46nZ70oac-"
      },
      "source": [
        "# NUM_EXAMPLES = 150000\n",
        "\n",
        "# tar_tensor, inp_tensor, tar_tokenizer, inp_tokenizer = load_dataset(path_data_file, NUM_EXAMPLES)\n",
        "\n",
        "# max_len_tar = tar_tensor.shape[1]\n",
        "# max_len_inp = inp_tensor.shape[1]\n",
        "\n",
        "# max_len_tar, max_len_inp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gEeS9uu7oac-"
      },
      "source": [
        "# inp_tensor_train, inp_tensor_val, tar_tensor_train, tar_tensor_val = train_test_split(inp_tensor, tar_tensor)\n",
        "\n",
        "# len(inp_tensor_train), len(inp_tensor_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kaf4QXWVoac-"
      },
      "source": [
        "\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((inp_tensor_train, tar_tensor_train))\n",
        "# dataset = dataset.shuffle(BUFFER_SIZE).cache().batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
        "# dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktPypdelocYQ"
      },
      "source": [
        "# NUM_EXAMPLES = None\n",
        "# train_dataset, val_dataset = load_texts_train_val_dataset(path_data_file, NUM_EXAMPLES, from_end=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7FpfjXtKzvS"
      },
      "source": [
        "NUM_EXAMPLES = None\n",
        "train_dataset = load_dataset(path_data_file, NUM_EXAMPLES, from_end=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zPeQMxlun7g"
      },
      "source": [
        "for i in train_dataset.take(2):\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGeoDK0oorGC"
      },
      "source": [
        "train_en = train_dataset.map(lambda ru, en: en)\n",
        "train_ru = train_dataset.map(lambda ru, en: ru)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "B7Iz7jMDoac_"
      },
      "source": [
        "bert_tokenizer_params = dict(lower_case=True)\n",
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "# reserved_tokens = []\n",
        "bert_vocab_args = dict(\n",
        "    vocab_size = 12000,\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    learn_params={}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1pLEYUDtoac_"
      },
      "source": [
        "%%time\n",
        "ru_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_ru.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEIPox7Gqeil"
      },
      "source": [
        "print(ru_vocab[:10])\n",
        "print(ru_vocab[100:110])\n",
        "print(ru_vocab[1000:1010])\n",
        "print(ru_vocab[-10:])\n",
        "\n",
        "print(en_vocab[:10])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jI9H2mLbWXJ"
      },
      "source": [
        "def write_to_file(filepath, vocab):\n",
        "    with open(filepath, 'w') as f:\n",
        "        for token in vocab:\n",
        "            print(token, file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJxJYTjVcJp2"
      },
      "source": [
        "ru_vocab_file = \"ru_vocab.txt\"\n",
        "en_vocab_file = \"en_vocab.txt\"\n",
        "\n",
        "write_to_file(ru_vocab_file, ru_vocab)\n",
        "write_to_file(en_vocab_file, en_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoBYI34jb5la"
      },
      "source": [
        "ru_tokenizer = text.BertTokenizer(ru_vocab_file, **bert_tokenizer_params)\n",
        "en_tokenizer = text.BertTokenizer(en_vocab_file, **bert_tokenizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AISqzTL5rXdk"
      },
      "source": [
        "for ru_examples, en_examples in train_dataset.batch(3).take(1):\n",
        "    for ex in en_examples:\n",
        "        print(ex.numpy())\n",
        "    for ex in ru_examples:\n",
        "        print(ex.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESApzE1krr0_"
      },
      "source": [
        "token_batch = en_tokenizer.tokenize(en_examples)\n",
        "print(token_batch)\n",
        "token_batch = token_batch.merge_dims(-2, -1)\n",
        "for ex in token_batch.to_list():\n",
        "    print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYw4G8rqsdod"
      },
      "source": [
        "text_tokens = tf.gather(en_vocab, token_batch)\n",
        "tf.strings.reduce_join(text_tokens, separator=' ', axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj2IJj8XtTrB"
      },
      "source": [
        "words = en_tokenizer.detokenize(token_batch)\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ZR7PIozW3A"
      },
      "source": [
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
        "\n",
        "def add_start_end(ragged):\n",
        "    count = ragged.bounding_shape()[0]\n",
        "    starts = tf.fill([count, 1], START)\n",
        "    ends = tf.fill([count, 1], END)\n",
        "\n",
        "    return tf.concat([starts, ragged, ends], axis=1)\n",
        "\n",
        "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
        "tf.strings.reduce_join(words, separator=' ', axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjWmcoOmF8Lx"
      },
      "source": [
        "def cleanup_text(reserved_tokens, token_txt):\n",
        "    # Drop the reserved tokens, except for \"[UNK]\".\n",
        "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "    bad_token_re = \"|\".join(bad_tokens)\n",
        "\n",
        "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "\n",
        "    # Join them into strings.\n",
        "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "    return result\n",
        "\n",
        "cleanup_text(reserved_tokens, words).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLUr42nzIrbZ"
      },
      "source": [
        "class CustomeTokenizer(tf.Module):\n",
        "    def __init__(self, reserved_tokens, vocab_path):\n",
        "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "        self._reserved_tokens = reserved_tokens\n",
        "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "\n",
        "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "        self.vocab = tf.Variable(vocab)\n",
        "\n",
        "        # Include a tokenize signature for a batch of strings. \n",
        "        self.tokenize.get_concrete_function(tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "\n",
        "        # Include `detokenize` and `lookup` signatures for:\n",
        "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "        #   * `RaggedTensors` with shape [batch, tokens]\n",
        "        self.detokenize.get_concrete_function(tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.detokenize.get_concrete_function(tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        \n",
        "        # These `get_*` methods take no arguments\n",
        "        self.get_vocab_size.get_concrete_function()\n",
        "        self.get_vocab_path.get_concrete_function()\n",
        "        self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "    @tf.function\n",
        "    def detokenize(self, tokenized):\n",
        "        words = self.tokenizer.detokenize(tokenized)\n",
        "        return cleanup_text(self._reserved_tokens, words)\n",
        "    \n",
        "    @tf.function\n",
        "    def tokenize(self, strings):\n",
        "        enc = self.tokenizer.tokenize(strings)\n",
        "        # Merge the `word` and `word-piece` axes.\n",
        "        enc = enc.merge_dims(-2, -1)\n",
        "        enc = add_start_end(enc)\n",
        "\n",
        "        return enc\n",
        "    \n",
        "    @tf.function\n",
        "    def lookup(self, token_ids):\n",
        "        return tf.gather(self.vocab, token_ids)\n",
        "    \n",
        "    @tf.function\n",
        "    def get_vocab_size(self):\n",
        "        return tf.shape(self.vocab)[0]\n",
        "    \n",
        "    @tf.function\n",
        "    def get_vocab_path(self):\n",
        "        return self._vocab_path\n",
        "    \n",
        "    @tf.function\n",
        "    def get_reserved_tokens(self):\n",
        "        return tf.constant(self._reserved_tokens)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVu30twuLv-v"
      },
      "source": [
        "tokenizers = tf.Module()\n",
        "tokenizers.ru = CustomeTokenizer(reserved_tokens, ru_vocab_file)\n",
        "tokenizers.en = CustomeTokenizer(reserved_tokens, en_vocab_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCUkQruhN2tN"
      },
      "source": [
        "model_name = \"ru_en_subwordtokenizer\"\n",
        "model_path = os.path.join(drive_mount_path, \"MyDrive\", \"Models\", model_name)\n",
        "tf.saved_model.save(tokenizers, model_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPJou96HPD3U"
      },
      "source": [
        "reloaded_tokenizers = tf.saved_model.load(model_path)\n",
        "reloaded_tokenizers.en.get_vocab_size().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95NF81uTPQDb"
      },
      "source": [
        "tokens = reloaded_tokenizers.en.tokenize([\"Hello From Tokenizer\"])\n",
        "tokens.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M9Q5xUIPdhg"
      },
      "source": [
        "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
        "text_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs-07SM0PoKO"
      },
      "source": [
        "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}