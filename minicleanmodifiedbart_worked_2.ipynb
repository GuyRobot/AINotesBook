{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets as well as other dependencies. Uncomment the following cell and run it.","metadata":{"id":"X4cRE8IbIrIV"}},{"cell_type":"code","source":"# pip install torch~=2.0.0 https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-2.0-cp38-cp38-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:07:33.718949Z","iopub.execute_input":"2023-10-25T07:07:33.719890Z","iopub.status.idle":"2023-10-25T07:07:33.725468Z","shell.execute_reply.started":"2023-10-25T07:07:33.719855Z","shell.execute_reply":"2023-10-25T07:07:33.724339Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"!pip install -q huggingface_hub datasets evaluate transformers rouge-score nltk transformers[torch]","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:07:33.729924Z","iopub.execute_input":"2023-10-25T07:07:33.730258Z","iopub.status.idle":"2023-10-25T07:07:45.765883Z","shell.execute_reply.started":"2023-10-25T07:07:33.730205Z","shell.execute_reply":"2023-10-25T07:07:45.764436Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n# os.environ['XLA_USE_BF16']=\"1\"\n# os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\n# import torch\n# import pandas as pd\n# from scipy import stats\n# import numpy as np\n\n# from tqdm import tqdm\n# from collections import OrderedDict, namedtuple\n# import torch.nn as nn\n# from torch.optim import lr_scheduler\n# import joblib\n\n# import logging\n# import transformers\n# from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig\n# import sys\n# from sklearn import metrics, model_selection","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:07:45.769185Z","iopub.execute_input":"2023-10-25T07:07:45.769718Z","iopub.status.idle":"2023-10-25T07:07:45.777120Z","shell.execute_reply.started":"2023-10-25T07:07:45.769669Z","shell.execute_reply":"2023-10-25T07:07:45.776079Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# device = xm.xla_device()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:07:45.778601Z","iopub.execute_input":"2023-10-25T07:07:45.778955Z","iopub.status.idle":"2023-10-25T07:07:45.789854Z","shell.execute_reply.started":"2023-10-25T07:07:45.778921Z","shell.execute_reply":"2023-10-25T07:07:45.788984Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"","metadata":{"id":"xDgAT1IviIFZ","outputId":"b337124b-811b-4e0d-ee7f-48a17e07a3f3","execution":{"iopub.status.busy":"2023-10-25T07:07:45.793066Z","iopub.execute_input":"2023-10-25T07:07:45.793901Z","iopub.status.idle":"2023-10-25T07:07:47.140062Z","shell.execute_reply.started":"2023-10-25T07:07:45.793865Z","shell.execute_reply":"2023-10-25T07:07:47.138621Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"!export CUDA_VISIBLE_DEVICES=0,1","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:07:47.141734Z","iopub.execute_input":"2023-10-25T07:07:47.142050Z","iopub.status.idle":"2023-10-25T07:07:48.184526Z","shell.execute_reply.started":"2023-10-25T07:07:47.142023Z","shell.execute_reply":"2023-10-25T07:07:48.183323Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = \"fd78ea9bd1f15165e547ac607fa3d95c18d50433\"","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:07:48.186466Z","iopub.execute_input":"2023-10-25T07:07:48.187788Z","iopub.status.idle":"2023-10-25T07:07:48.195616Z","shell.execute_reply.started":"2023-10-25T07:07:48.187746Z","shell.execute_reply":"2023-10-25T07:07:48.194673Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n\nTo be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n\nFirst you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:","metadata":{"id":"Bem2kQaviIFZ"}},{"cell_type":"markdown","source":"Then you need to install Git-LFS. Uncomment the following instructions:","metadata":{"id":"-LwLWH2WiIFa"}},{"cell_type":"code","source":"# #!apt-get install git-lfs\n# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash","metadata":{"id":"HSRwiC_EiIFa","outputId":"1a290998-79af-4934-e44d-6a085476eeeb","execution":{"iopub.status.busy":"2023-10-25T07:07:48.197004Z","iopub.execute_input":"2023-10-25T07:07:48.197368Z","iopub.status.idle":"2023-10-25T07:07:48.205160Z","shell.execute_reply.started":"2023-10-25T07:07:48.197336Z","shell.execute_reply":"2023-10-25T07:07:48.203924Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"markdown","source":"Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:","metadata":{"id":"KEoqq-6tiIFb"}},{"cell_type":"code","source":"import transformers\n\nprint(transformers.__version__)","metadata":{"id":"3rsZjvaJiIFb","outputId":"5f502fce-11d3-49e8-bb13-83dd07dbeb98","execution":{"iopub.status.busy":"2023-10-25T07:07:48.206491Z","iopub.execute_input":"2023-10-25T07:07:48.206766Z","iopub.status.idle":"2023-10-25T07:07:48.218176Z","shell.execute_reply.started":"2023-10-25T07:07:48.206743Z","shell.execute_reply":"2023-10-25T07:07:48.216600Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"4.33.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/seq2seq).","metadata":{"id":"HFASsisvIrIb"}},{"cell_type":"markdown","source":"We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely.","metadata":{"id":"IVbKHMj3iIFc"}},{"cell_type":"code","source":"from transformers.utils import send_example_telemetry\n\nsend_example_telemetry(\"summarization_notebook\", framework=\"pytorch\")","metadata":{"id":"ngRm8xehiIFc","execution":{"iopub.status.busy":"2023-10-25T07:07:48.219992Z","iopub.execute_input":"2023-10-25T07:07:48.220423Z","iopub.status.idle":"2023-10-25T07:07:48.480045Z","shell.execute_reply.started":"2023-10-25T07:07:48.220390Z","shell.execute_reply":"2023-10-25T07:07:48.478838Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tuning a model on a summarization task","metadata":{"id":"rEJBSTyZIrIb"}},{"cell_type":"markdown","source":"In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n\n![Widget inference on a summarization task](https://github.com/huggingface/notebooks/blob/main/examples/images/summarization.png?raw=1)\n\nWe will see how to easily load the dataset for this task using ðŸ¤— Datasets and how to fine-tune a model on it using the `Trainer` API.","metadata":{"id":"kTCFado4IrIc"}},{"cell_type":"code","source":"model_checkpoint = \"facebook/bart-base\"","metadata":{"id":"g6PugG96iIFd","execution":{"iopub.status.busy":"2023-10-25T07:07:48.486267Z","iopub.execute_input":"2023-10-25T07:07:48.486665Z","iopub.status.idle":"2023-10-25T07:07:48.492716Z","shell.execute_reply.started":"2023-10-25T07:07:48.486629Z","shell.execute_reply":"2023-10-25T07:07:48.491468Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library. Here we picked the [`t5-small`](https://huggingface.co/t5-small) checkpoint.","metadata":{"id":"4RRkXuteIrIh"}},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{"id":"whPRbBNbIrIl"}},{"cell_type":"markdown","source":"We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  ","metadata":{"id":"W7QYTpxXIrIl"}},{"cell_type":"code","source":"# !wget https://github.com/GuyRobot/AINotesBook/releases/download/v1/EHealthChatDataset.json.gz\n# !gzip -d EHealthChatDataset.json.gz","metadata":{"id":"xrRMvk-GukJI","outputId":"4e5a654e-b665-4d9c-fed6-6aefa7721f23","execution":{"iopub.status.busy":"2023-10-25T07:07:48.494137Z","iopub.execute_input":"2023-10-25T07:07:48.494545Z","iopub.status.idle":"2023-10-25T07:07:48.504691Z","shell.execute_reply.started":"2023-10-25T07:07:48.494503Z","shell.execute_reply":"2023-10-25T07:07:48.503791Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"!pip install -q nlp","metadata":{"id":"HH3UmIo7w1z-","outputId":"8efdbe5b-4562-4150-d7e6-7c9d21eebc98","execution":{"iopub.status.busy":"2023-10-25T07:07:48.506039Z","iopub.execute_input":"2023-10-25T07:07:48.506353Z","iopub.status.idle":"2023-10-25T07:08:00.423813Z","shell.execute_reply.started":"2023-10-25T07:07:48.506326Z","shell.execute_reply":"2023-10-25T07:08:00.422620Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import datasets\nfrom datasets import load_dataset\nfrom evaluate import load\n\n# train_ds = load_dataset(\"json\", data_files=[\"EHealthChatDataset.json\"], split=\"train[:80%]\")\n# vals_ds = load_dataset(\"json\", data_files=[\"EHealthChatDataset.json\"], split=\"train[80%:]\")","metadata":{"id":"IreSlFmlIrIm","outputId":"3ce99109-e90d-4b56-a7fa-1f58429681c7","execution":{"iopub.status.busy":"2023-10-25T07:08:00.426364Z","iopub.execute_input":"2023-10-25T07:08:00.426683Z","iopub.status.idle":"2023-10-25T07:08:00.434069Z","shell.execute_reply.started":"2023-10-25T07:08:00.426654Z","shell.execute_reply":"2023-10-25T07:08:00.432888Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom datasets import Dataset\n# with open(\"EHealthChatDataset.json\") as f:\n#     data = json.load(f)\n#     train_ds = Dataset.from_pandas(pd.DataFrame(data=data[:int(len(data)*0.8)]))\n#     vals_ds = Dataset.from_pandas(pd.DataFrame(data=data[int(len(data)*0.8):]))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:08:00.435561Z","iopub.execute_input":"2023-10-25T07:08:00.436395Z","iopub.status.idle":"2023-10-25T07:08:00.445542Z","shell.execute_reply.started":"2023-10-25T07:08:00.436362Z","shell.execute_reply":"2023-10-25T07:08:00.444581Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/GuyRobot/AINotesBook/releases/download/clean/EHealthChatDataset_seq_512.csv","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:08:00.446810Z","iopub.execute_input":"2023-10-25T07:08:00.447158Z","iopub.status.idle":"2023-10-25T07:08:03.178567Z","shell.execute_reply.started":"2023-10-25T07:08:00.447129Z","shell.execute_reply":"2023-10-25T07:08:03.177409Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--2023-10-25 07:08:01--  https://github.com/GuyRobot/AINotesBook/releases/download/clean/EHealthChatDataset_seq_512.csv\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/350588256/7de58201-6958-41f7-9980-5363334ed2b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231025%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231025T070801Z&X-Amz-Expires=300&X-Amz-Signature=809222aa19993115e46899bf2ea7777c7af1ee8aadf2f08557e7446501bc615a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=350588256&response-content-disposition=attachment%3B%20filename%3DEHealthChatDataset_seq_512.csv&response-content-type=application%2Foctet-stream [following]\n--2023-10-25 07:08:01--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/350588256/7de58201-6958-41f7-9980-5363334ed2b8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231025%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231025T070801Z&X-Amz-Expires=300&X-Amz-Signature=809222aa19993115e46899bf2ea7777c7af1ee8aadf2f08557e7446501bc615a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=350588256&response-content-disposition=attachment%3B%20filename%3DEHealthChatDataset_seq_512.csv&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 88000773 (84M) [application/octet-stream]\nSaving to: â€˜EHealthChatDataset_seq_512.csv.3â€™\n\nEHealthChatDataset_ 100%[===================>]  83.92M  89.7MB/s    in 0.9s    \n\n2023-10-25 07:08:03 (89.7 MB/s) - â€˜EHealthChatDataset_seq_512.csv.3â€™ saved [88000773/88000773]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(\"EHealthChatDataset_seq_512.csv\") as f:\n    data = pd.read_csv(f).dropna()\n    train_ds = Dataset.from_pandas(pd.DataFrame(data=data[:1000]))\n    vals_ds = Dataset.from_pandas(pd.DataFrame(data=data[:100]))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:08:03.180601Z","iopub.execute_input":"2023-10-25T07:08:03.181045Z","iopub.status.idle":"2023-10-25T07:08:04.637112Z","shell.execute_reply.started":"2023-10-25T07:08:03.181003Z","shell.execute_reply":"2023-10-25T07:08:04.636114Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict\nraw_datasets = DatasetDict()\nraw_datasets[\"train\"] = train_ds\nraw_datasets[\"validation\"] = vals_ds\n# raw_datasets = load_dataset(\"xsum\")\n# raw_datasets = load_dataset(\"json\", data_files={\"train\": \"ehealthforumQAs.json\", \"validation\": \"ehealthforumQAs.json\"})\nmetric = load(\"rouge\")","metadata":{"id":"vFWoMd5vUg10","outputId":"343748ab-167a-448d-f364-2e01a7844e35","execution":{"iopub.status.busy":"2023-10-25T07:08:04.638445Z","iopub.execute_input":"2023-10-25T07:08:04.639532Z","iopub.status.idle":"2023-10-25T07:08:06.050556Z","shell.execute_reply.started":"2023-10-25T07:08:04.639493Z","shell.execute_reply":"2023-10-25T07:08:06.049545Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"# import nlp\n# dataset_cc = nlp.concatenate_datasets([train_ds, vals_ds])","metadata":{"id":"zUTx_pwLw6qE","execution":{"iopub.status.busy":"2023-10-25T07:08:06.052033Z","iopub.execute_input":"2023-10-25T07:08:06.052422Z","iopub.status.idle":"2023-10-25T07:08:06.057955Z","shell.execute_reply.started":"2023-10-25T07:08:06.052394Z","shell.execute_reply":"2023-10-25T07:08:06.056896Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":"The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:","metadata":{"id":"RzfPtOMoIrIu"}},{"cell_type":"code","source":"raw_datasets","metadata":{"id":"GWiVUF0jIrIv","outputId":"a998d269-c260-45a1-e0f0-9b9cd1217b64","execution":{"iopub.status.busy":"2023-10-25T07:08:06.059477Z","iopub.execute_input":"2023-10-25T07:08:06.059811Z","iopub.status.idle":"2023-10-25T07:08:06.071658Z","shell.execute_reply.started":"2023-10-25T07:08:06.059780Z","shell.execute_reply":"2023-10-25T07:08:06.070727Z"},"trusted":true},"execution_count":155,"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0.2', 'Unnamed: 0', 'Unnamed: 0.1', 'answer', 'url', 'question', '__index_level_0__'],\n        num_rows: 1000\n    })\n    validation: Dataset({\n        features: ['Unnamed: 0.2', 'Unnamed: 0', 'Unnamed: 0.1', 'answer', 'url', 'question', '__index_level_0__'],\n        num_rows: 100\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"To access an actual element, you need to select a split first, then give an index:","metadata":{"id":"u3EtYfeHIrIz"}},{"cell_type":"code","source":"raw_datasets[\"train\"][0]","metadata":{"id":"X6HrpprwIrIz","outputId":"ef7af259-e3f5-46a0-87a3-0c06c697656e","execution":{"iopub.status.busy":"2023-10-25T07:08:06.072942Z","iopub.execute_input":"2023-10-25T07:08:06.073332Z","iopub.status.idle":"2023-10-25T07:08:06.085826Z","shell.execute_reply.started":"2023-10-25T07:08:06.073300Z","shell.execute_reply":"2023-10-25T07:08:06.084687Z"},"trusted":true},"execution_count":156,"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"{'Unnamed: 0.2': 1,\n 'Unnamed: 0': 1,\n 'Unnamed: 0.1': 1,\n 'answer': 'Gynae examination and ultrasound advised  I understand your anxiety about the issue and it is indeed serious and should\\nnot be taken lightly  Get a pelvic examination from a good gynaecologist and an ultrasound at the earliest  It can be\\nserious so much so like cancer or can just be related to postmenopausal hormone deficiency  Get the tests and then we\\ncan decide further',\n 'url': 'https://www.healthcaremagic.com/premiumquestions/What-causes-persistent-painless-vaginal-bleeding/305320',\n 'question': 'I have vaginal bleeding for three years. No pain . I would like to know why.',\n '__index_level_0__': 0}"},"metadata":{}}]},{"cell_type":"markdown","source":"To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.","metadata":{"id":"WHUmphG3IrI3"}},{"cell_type":"code","source":"import datasets\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\n\ndef show_random_elements(dataset, num_examples=5):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n\n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, datasets.ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n    display(HTML(df.to_html()))","metadata":{"id":"i3j8APAoIrI3","execution":{"iopub.status.busy":"2023-10-25T07:08:06.086768Z","iopub.execute_input":"2023-10-25T07:08:06.087098Z","iopub.status.idle":"2023-10-25T07:08:06.102568Z","shell.execute_reply.started":"2023-10-25T07:08:06.087067Z","shell.execute_reply":"2023-10-25T07:08:06.101692Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"show_random_elements(raw_datasets[\"train\"])","metadata":{"id":"SZy5tRB_IrI7","outputId":"8beb07ab-dd91-4bd7-8875-fcad9f46a81e","execution":{"iopub.status.busy":"2023-10-25T07:08:06.103714Z","iopub.execute_input":"2023-10-25T07:08:06.104504Z","iopub.status.idle":"2023-10-25T07:08:06.125130Z","shell.execute_reply.started":"2023-10-25T07:08:06.104469Z","shell.execute_reply":"2023-10-25T07:08:06.124256Z"},"trusted":true},"execution_count":158,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.2</th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>answer</th>\n      <th>url</th>\n      <th>question</th>\n      <th>__index_level_0__</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2944</td>\n      <td>2944</td>\n      <td>2944</td>\n      <td>C T guided biopsy    Since your doctor told you that either u had pneumonia or nodularity within right lung upper lobe i\\nsuggest u complete a 7 day course of antibiotic and then repeat  C T scan of chest with contrast and if the space\\noccupying lesion in your lungs still persist in the scan then u do a  C T guided biopsy from that lesion to rule out\\nmalignancy.  Generally pneumonic patches clear off in scan after a course of antibiotic.</td>\n      <td>https://www.healthcaremagic.com/premiumquestions/Is-persistent-nodularity-in-the-right-lung-indicative-of-cancer/108501</td>\n      <td>Hi I'm XXXXXXX XXXXXXX I was told by adoctor I have either pneumonia or nodularity within the right lung upper lobe if idon't  respond to antibiotics.Is that poosible and can you pneumni?Penelope  or I have a mass and it's probably cancer</td>\n      <td>654</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>454</td>\n      <td>454</td>\n      <td>454</td>\n      <td>There is nothing to worry about! I have reviewed your information fully.  There is nothing to worry about.  For sure, it\\nis not in your air passages.  If this were the case, you would have been coughing all the times and almost unable to\\nbreath.  This gum is certainly in the stomach and will go out with food.  There is just nothing to bother about.  Be\\ncalm and relaxed and stop getting too worried about this. I hope this helps.  I wish you well.  Feel free to follow up\\nwith me if need be.</td>\n      <td>https://www.healthcaremagic.com/premiumquestions/What-causes-dizziness-and-breathlessness-after-accidental-inhalation-of-chewing-gum/305006</td>\n      <td>about a month ago i accidentally inhaled the tiny piece of chewing gum.  I work out mildly and seem to be short of breath, and after exercising sometimes I am lightheaded, where I have to sit down before I fall down.  It almost seems I can feel an obstruction.  Is it going to grow, disappear?</td>\n      <td>114</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>81</td>\n      <td>81</td>\n      <td>81</td>\n      <td>You are free from  H I V infection.  No further test is necessary    Welcome once again to  H C M.  No drug can\\ninterfere or delay the antibody formation of  H I V in the presence of an infection more than 6 months after the\\nexposure.  H I V unless you have a recent fresh exposure (including  H I V  P C R  Viral load test - which has to be\\ndone if infection detected only to know about the severity).  Dr  S. Murugan</td>\n      <td>https://www.healthcaremagic.com/premiumquestions/What-does-my-lab-test-report-indicate/305259</td>\n      <td>To be answered by Dr.S XXXXXXX \\nSir as you already know my case my high risk exposure was in Nov 2013 and my 1 st test( tridot 3 Rd generation)was in may 2015,2 nd test July 2016( 4 rth generation ag/ ab test) and 3 Rd test was in March 2017 by the same 4 rth generation test and all were non- reactive.And all along I was on several medications like ORGAMED/MEPRATE/PROVERA/DRONIS 30/METFORMIN/SYNTHROID/CLOMIPHENE CITRATE/ANTI- HISTAMINES /PRENATALS(folic acid and vit d supplements ( for conditions like PCOS,HYPOTHROIDISM,INFERTILITY AND POLLEN ALLERGY .I was on all these drugs ON AND OFF from XXXXXXX 2007 till MARCH 2017 (and my exposure was in Nov 2013)some at the time of testing and some inbetween tests.so my question to you sir is DO I NEED  PCR (VIRAL LOAD TEST) AFTER 2 NON-REACTIVE 4 RTH GENERATION TESTS 3 YEARS POST EXPOSURE?i was on VERTIGO DRUGS AS WELL.WILL THEY HAVE ANY INFLUENCE ON THE HIV TESTS? THANK YOU</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3383</td>\n      <td>3383</td>\n      <td>3383</td>\n      <td>Poor prognosis. Thanks or choosing  H C M, By severe heart failure i would consider that ejection fraction of patient is\\nless then 20%. Prognosis is poor if  E F is less then 20% and  I A C D device should be implanted to prevent sudden\\ncardiac death in patient. However  Treatment of choice is  I N I T I A L L Y  T O  G O  F O R  Left ventricular assisted\\ndevice followe by cardiac transplantation. For further details kindly enclose  Echo report of patient along with all\\nsymptoms.</td>\n      <td>https://www.healthcaremagic.com/premiumquestions/What-is-the-prognosis-of-a-heart-failure/299736</td>\n      <td>What prognosis\\nLike for severe heart failure</td>\n      <td>759</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1217</td>\n      <td>1217</td>\n      <td>1217</td>\n      <td>Kindly upload the reports Thanks for posting your query in  Healthcare Magic, I have gone through your query and can\\nunderstand your concern, But  I'm not able to find any attached reports, Kindly upload your recent reports of  M R I or\\nother scans.  You may also send it to   Y Y Y Y@ Y Y Y Y  with attention to  Dr.  Remy.</td>\n      <td>https://www.healthcaremagic.com/premiumquestions/What-do-these-following-MRI-findings-indicate/304487</td>\n      <td>dear sir/ madam I am suffering from back pain from last 3 yrs  . L4-L5 disc protrusion . I can't sit or stand for long time she to extreme back and radiating left leg scatia pain.  I consult .many doctors and physiotherapist. advised. back exersies. like extension . I am doing them ..but can't get good  results.so I want to confirm with u ..that may I join gym to make my lower back.legs and hips strong.I am also attaching my reports.</td>\n      <td>281</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"markdown","source":"The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):","metadata":{"id":"lnjDIuQ3IrI-"}},{"cell_type":"code","source":"metric","metadata":{"id":"5o4rUteaIrI_","outputId":"2c235410-5ad7-4390-8470-56c9fe7194e3","execution":{"iopub.status.busy":"2023-10-25T07:08:06.126376Z","iopub.execute_input":"2023-10-25T07:08:06.126711Z","iopub.status.idle":"2023-10-25T07:08:06.136667Z","shell.execute_reply.started":"2023-10-25T07:08:06.126679Z","shell.execute_reply":"2023-10-25T07:08:06.135666Z"},"trusted":true},"execution_count":159,"outputs":[{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n        `\"rougeL\"`: Longest common subsequence based scoring.\n        `\"rougeLsum\"`: rougeLsum splits text using `\"\n\"`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_aggregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (f1),\n    rouge2: rouge_2 (f1),\n    rougeL: rouge_l (f1),\n    rougeLsum: rouge_lsum (f1)\nExamples:\n\n    >>> rouge = evaluate.load('rouge')\n    >>> predictions = [\"hello there\", \"general kenobi\"]\n    >>> references = [\"hello there\", \"general kenobi\"]\n    >>> results = rouge.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n\"\"\", stored examples: 0)"},"metadata":{}}]},{"cell_type":"markdown","source":"You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:","metadata":{"id":"jAWdqcUBIrJC"}},{"cell_type":"code","source":"fake_preds = [\"hello there\", \"general kenobi\"]\nfake_labels = [\"hello there\", \"general kenobi\"]\nmetric.compute(predictions=fake_preds, references=fake_labels)","metadata":{"id":"6XN1Rq0aIrJC","outputId":"82ce5772-6199-4916-d0dc-798b86670fcc","execution":{"iopub.status.busy":"2023-10-25T07:08:06.137826Z","iopub.execute_input":"2023-10-25T07:08:06.139587Z","iopub.status.idle":"2023-10-25T07:08:06.350311Z","shell.execute_reply.started":"2023-10-25T07:08:06.139561Z","shell.execute_reply":"2023-10-25T07:08:06.349070Z"},"trusted":true},"execution_count":160,"outputs":[{"execution_count":160,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing the data","metadata":{"id":"n9qywopnIrJH"}},{"cell_type":"markdown","source":"Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that the model requires.\n\nTo do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n\n- we get a tokenizer that corresponds to the model architecture we want to use,\n- we download the vocabulary used when pretraining this specific checkpoint.\n\nThat vocabulary will be cached, so it's not downloaded again the next time we run the cell.","metadata":{"id":"YVx71GdAIrJH"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"eXNLu_-nIrJI","outputId":"3e7e738f-3465-4e56-a9dd-8e5983b65a5d","execution":{"iopub.status.busy":"2023-10-25T07:08:06.351747Z","iopub.execute_input":"2023-10-25T07:08:06.352096Z","iopub.status.idle":"2023-10-25T07:08:07.363177Z","shell.execute_reply.started":"2023-10-25T07:08:06.352065Z","shell.execute_reply":"2023-10-25T07:08:07.362063Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"markdown","source":"By default, the call above will use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library.","metadata":{"id":"Vl6IidfdIrJK"}},{"cell_type":"markdown","source":"You can directly call this tokenizer on one sentence or a pair of sentences:","metadata":{"id":"rowT4iCLIrJK"}},{"cell_type":"code","source":"tokenizer(\"Hello, this one sentence!\")","metadata":{"id":"a5hBlsrHIrJL","outputId":"6034abc3-1a20-4037-8f7b-a42b0c21e3e6","execution":{"iopub.status.busy":"2023-10-25T07:08:07.364578Z","iopub.execute_input":"2023-10-25T07:08:07.364935Z","iopub.status.idle":"2023-10-25T07:08:07.374327Z","shell.execute_reply.started":"2023-10-25T07:08:07.364899Z","shell.execute_reply":"2023-10-25T07:08:07.373149Z"},"trusted":true},"execution_count":162,"outputs":[{"execution_count":162,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [0, 31414, 6, 42, 65, 3645, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"markdown","source":"Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n\nInstead of one sentence, we can pass along a list of sentences:","metadata":{"id":"qo_0B1M2IrJM"}},{"cell_type":"code","source":"tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])","metadata":{"id":"zmWmo1SIiIFh","outputId":"4233aac8-9714-4670-8ffe-fbf411564eb4","execution":{"iopub.status.busy":"2023-10-25T07:08:07.375547Z","iopub.execute_input":"2023-10-25T07:08:07.375902Z","iopub.status.idle":"2023-10-25T07:08:07.389148Z","shell.execute_reply.started":"2023-10-25T07:08:07.375844Z","shell.execute_reply":"2023-10-25T07:08:07.388264Z"},"trusted":true},"execution_count":163,"outputs":[{"execution_count":163,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[0, 31414, 6, 42, 65, 3645, 328, 2], [0, 713, 16, 277, 3645, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}"},"metadata":{}}]},{"cell_type":"markdown","source":"To prepare the targets for our model, we need to tokenize them using the `text_target` parameter. This will make sure the tokenizer uses the special tokens corresponding to the targets:","metadata":{"id":"gfeukRDaiIFi"}},{"cell_type":"code","source":"print(tokenizer(text_target=[\"Hello, this one sentence!\", \"This is another sentence.\"]))","metadata":{"id":"QXxL-gbziIFi","outputId":"273de184-62df-4edc-a985-2a11ab6dd5f3","execution":{"iopub.status.busy":"2023-10-25T07:08:07.396530Z","iopub.execute_input":"2023-10-25T07:08:07.396813Z","iopub.status.idle":"2023-10-25T07:08:07.403334Z","shell.execute_reply.started":"2023-10-25T07:08:07.396788Z","shell.execute_reply":"2023-10-25T07:08:07.402238Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stdout","text":"{'input_ids': [[0, 31414, 6, 42, 65, 3645, 328, 2], [0, 713, 16, 277, 3645, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" (the model can also translate and it needs the prefix to know which task it has to perform).","metadata":{"id":"2C0hcmp9IrJQ"}},{"cell_type":"code","source":"if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n    prefix = \"summarize: \"\nelse:\n    prefix = \"\"","metadata":{"id":"a3RAu2wniIFm","execution":{"iopub.status.busy":"2023-10-25T07:08:07.404548Z","iopub.execute_input":"2023-10-25T07:08:07.404853Z","iopub.status.idle":"2023-10-25T07:08:07.413101Z","shell.execute_reply.started":"2023-10-25T07:08:07.404821Z","shell.execute_reply":"2023-10-25T07:08:07.412112Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"markdown","source":"We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset.","metadata":{"id":"NUh2j127iIFm"}},{"cell_type":"code","source":"max_input_length = 512\nmax_target_length = 512\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"question\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"answer\"], max_length=max_target_length, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"id":"vc0BSBLIIrJQ","execution":{"iopub.status.busy":"2023-10-25T07:08:07.414208Z","iopub.execute_input":"2023-10-25T07:08:07.414586Z","iopub.status.idle":"2023-10-25T07:08:07.423940Z","shell.execute_reply.started":"2023-10-25T07:08:07.414551Z","shell.execute_reply":"2023-10-25T07:08:07.422883Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"markdown","source":"This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:","metadata":{"id":"0lm8ozrJIrJR"}},{"cell_type":"code","source":"preprocess_function(raw_datasets['train'][:2])","metadata":{"id":"-b70jh26IrJS","outputId":"968a5a4c-296d-442c-c3f8-0901d095bc2a","execution":{"iopub.status.busy":"2023-10-25T07:08:07.425717Z","iopub.execute_input":"2023-10-25T07:08:07.426044Z","iopub.status.idle":"2023-10-25T07:08:07.440425Z","shell.execute_reply.started":"2023-10-25T07:08:07.426012Z","shell.execute_reply":"2023-10-25T07:08:07.439566Z"},"trusted":true},"execution_count":167,"outputs":[{"execution_count":167,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[0, 100, 33, 34126, 13162, 13, 130, 107, 4, 440, 2400, 479, 38, 74, 101, 7, 216, 596, 4, 2], [0, 876, 21284, 364, 611, 6457, 11483, 2407, 9, 12581, 117, 22628, 7427, 1499, 15715, 2340, 16698, 22090, 5395, 480, 7, 2178, 181, 9475, 611, 8307, 337, 12581, 2199, 7586, 2292, 17733, 4399, 571, 2368, 19, 1533, 9843, 5109, 1536, 4, 19869, 32108, 16, 30389, 7018, 9663, 7586, 33473, 6, 642, 3290, 241, 281, 6, 7049, 877, 70, 2340, 155, 377, 939, 33, 551, 1416, 7586, 127, 12079, 32, 1717, 417, 718, 1879, 6, 329, 3976, 1417, 405, 6, 462, 8538, 718, 13, 859, 7586, 78, 127, 2292, 17733, 21, 753, 4, 245, 13753, 2156, 485, 266, 924, 545, 13753, 31617, 853, 1792, 179, 12, 134, 4, 466, 2642, 179, 12, 306, 4, 245, 37544, 27377, 12, 176, 4, 245, 579, 22371, 12, 2518, 579, 571, 3320, 12, 1558, 38994, 38204, 34637, 41278, 12, 4156, 1437, 1437, 127, 12581, 1836, 21, 361, 4, 245, 13753, 4, 1437, 178, 130, 377, 41268, 338, 127, 2292, 17733, 2906, 7, 545, 4, 245, 13753, 8, 9843, 5109, 1536, 1836, 2906, 4528, 910, 127, 266, 21958, 11, 570, 1577, 48193, 18313, 1437, 11378, 162, 549, 24, 16, 33370, 890, 868, 50, 24, 8349, 2017, 7, 40441, 20378, 13310, 7586, 98, 939, 240, 14067, 734, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 939, 351, 75, 4076, 50, 4603, 7586, 8, 5171, 9, 45441, 741, 6, 438, 385, 7586, 479, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1892, 528, 7, 173, 939, 439, 7, 277, 1098, 7586, 89, 1437, 71, 65, 39750, 939, 362, 14194, 122, 456, 127, 2292, 17733, 1411, 7, 291, 13753, 29942, 259, 51, 2294, 1437, 1717, 417, 718, 1879, 9995, 1236, 620, 51, 1311, 129, 65, 9995, 8462, 38315, 13, 94, 130, 377, 29942, 277, 181, 3662, 119, 528, 7, 2292, 17733, 21, 5299, 10219, 2107, 151, 8, 885, 23219, 3788, 7586, 27882, 244, 162, 99, 939, 197, 109, 13, 42, 7586, 1219, 13, 127, 12581, 936, 21, 4727, 7586, 53, 122, 259, 51, 32, 584, 189, 71, 195, 50, 158, 1423, 4926, 47, 240, 14067, 734, 11, 78, 1098, 51, 174, 24, 16, 33370, 890, 5084, 47, 230, 16966, 120, 110, 2530, 1836, 12581, 53, 1717, 64, 24059, 110, 30223, 9, 12581, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[0, 534, 3892, 4791, 9027, 8, 29976, 5578, 1437, 38, 1346, 110, 6882, 59, 5, 696, 8, 24, 16, 5329, 1473, 8, 197, 50118, 3654, 28, 551, 14998, 1437, 2315, 10, 38346, 9027, 31, 10, 205, 821, 19577, 3204, 6393, 8, 41, 29976, 23, 5, 13342, 1437, 85, 64, 28, 50118, 21231, 98, 203, 98, 101, 1668, 50, 64, 95, 28, 1330, 7, 618, 2262, 42363, 20940, 30367, 1437, 2315, 5, 3457, 8, 172, 52, 50118, 7424, 2845, 617, 2], [0, 40181, 55, 335, 4557, 13, 6016, 110, 25860, 4, 38, 524, 1437, 925, 4, 248, 4, 229, 8, 1437, 38, 524, 4343, 7, 3991, 47, 4, 38, 240, 103, 55, 50118, 31480, 137, 1437, 38, 64, 492, 127, 2979, 4, 38, 74, 101, 7, 33, 103, 55, 335, 137, 1437, 38, 64, 492, 127, 2979, 4, 134, 4, 50118, 32112, 19961, 1001, 43511, 50, 10, 12581, 4003, 33716, 626, 116, 176, 4, 1437, 6871, 253, 17591, 16572, 626, 116, 246, 4, 1437, 3945, 47, 10, 33560, 116, 925, 4, 248, 4, 229, 4, 2]]}"},"metadata":{}}]},{"cell_type":"markdown","source":"To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command.","metadata":{"id":"zS-6iXTkIrJT"}},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)","metadata":{"id":"DDtsaJeVIrJT","outputId":"da49bd44-2747-4fef-c803-3354b5b66925","execution":{"iopub.status.busy":"2023-10-25T07:08:07.442138Z","iopub.execute_input":"2023-10-25T07:08:07.442610Z","iopub.status.idle":"2023-10-25T07:08:07.868377Z","shell.execute_reply.started":"2023-10-25T07:08:07.442573Z","shell.execute_reply":"2023-10-25T07:08:07.867192Z"},"trusted":true},"execution_count":168,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5477f713fa274bd68eb80f172cd09547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dddbd624bda43a394d00dbe9dbb29b6"}},"metadata":{}}]},{"cell_type":"markdown","source":"Even better, the results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n\nNote that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently.","metadata":{"id":"voWiw8C7IrJV"}},{"cell_type":"markdown","source":"## Fine-tuning the model","metadata":{"id":"545PP3o8IrJV"}},{"cell_type":"markdown","source":"Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us.","metadata":{"id":"FBiW8UpKIrJW"}},{"cell_type":"code","source":"# coding=utf-8\n# Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch BART model.\"\"\"\nimport copy\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn, einsum\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n    Seq2SeqQuestionAnsweringModelOutput,\n    Seq2SeqSequenceClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_code_sample_docstrings,\n    add_end_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.bart.configuration_bart import BartConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n_CONFIG_FOR_DOC = \"BartConfig\"\n\n# Base model docstring\n_EXPECTED_OUTPUT_SHAPE = [1, 8, 768]\n\n# SequenceClassification docstring\n_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"valhalla/bart-large-sst2\"\n_SEQ_CLASS_EXPECTED_LOSS = 0.0\n_SEQ_CLASS_EXPECTED_OUTPUT = \"'POSITIVE'\"\n\n# QuestionAsnwering docstring\n_CHECKPOINT_FOR_QA = \"valhalla/bart-large-finetuned-squadv1\"\n_QA_EXPECTED_LOSS = 0.59\n_QA_EXPECTED_OUTPUT = \"' nice puppet'\"\n\n\nBART_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/bart-large\",\n    # see all BART models at https://huggingface.co/models?filter=bart\n]\n\n\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\nclass BartLearnedPositionalEmbedding(nn.Embedding):\n    \"\"\"\n    This module learns positional embeddings up to a fixed maximum size.\n    \"\"\"\n\n    def __init__(self, num_embeddings: int, embedding_dim: int):\n        # Bart is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models don't have this hack\n        self.offset = 2\n        super().__init__(num_embeddings + self.offset, embedding_dim)\n\n    def forward(self, input_ids: torch.Tensor, past_key_values_length: int = 0):\n        \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n\n        bsz, seq_len = input_ids.shape[:2]\n        positions = torch.arange(\n            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n        ).expand(bsz, -1)\n\n        return super().forward(positions + self.offset)\n\n\nclass BartAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        # Re-attention\n        self.reatten_matrix = nn.Parameter(torch.randn(self.num_heads, self.num_heads))\n        self.var_norm = nn.BatchNorm2d(self.num_heads)\n   \n\n        if (self.head_dim * num_heads) != self.embed_dim:\n            raise ValueError(\n                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n                f\" and `num_heads`: {num_heads}).\"\n            )\n        self.scaling = self.head_dim**-0.5\n        self.reatten_scale = self.scaling\n        self.is_decoder = is_decoder\n\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.proj_drop = nn.Dropout(0.0)\n        self.attn_drop = nn.Dropout(0.0)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        re_attention = False\n        is_cross_attention = key_value_states is not None\n\n        bsz, tgt_len, _ = hidden_states.size()\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n        # is checking that the `sequence_length` of the `past_key_value` is the same as\n        # the provided `key_value_states` to support prefix tuning\n        if (\n            is_cross_attention\n            and past_key_value is not None\n            and past_key_value[0].shape[2] == key_value_states.shape[1]\n        ):\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            re_attention = True\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n        key_states = key_states.reshape(*proj_shape)\n        value_states = value_states.reshape(*proj_shape)\n\n        src_len = key_states.size(1)\n        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n\n        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n        # Re-attention\n        if re_attention:\n#         attn_weights = self.attn_drop(attn_weights)\n            attn_weights = attn_weights.reshape(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = einsum('b h i j, h g -> b g i j', attn_weights, self.reatten_matrix) * self.reatten_scale\n#             attn_weights = self.var_norm(attn_weights) * self.reatten_scale\n            attn_weights = attn_weights.reshape(bsz * self.num_heads, tgt_len, src_len)\n\n        if layer_head_mask is not None:\n            if layer_head_mask.size() != (self.num_heads,):\n                raise ValueError(\n                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n                    f\" {layer_head_mask.size()}\"\n                )\n            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        if output_attentions:\n            # this operation is a bit awkward, but it's required to\n            # make sure that attn_weights keeps its gradient.\n            # In order to do so, attn_weights have to be reshaped\n            # twice and have to be reused in the following\n            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n        else:\n            attn_weights_reshaped = None\n\n        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n\n        attn_output = torch.bmm(attn_probs, value_states)\n\n        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n        attn_output = attn_output.transpose(1, 2)\n\n        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n        # partitioned across GPUs when using tensor-parallelism.\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights_reshaped, past_key_value\n\n\nclass BartEncoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n        self.self_attn = BartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.encoder_attention_heads,\n            dropout=config.attention_dropout,\n        )\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.FloatTensor,\n        attention_mask: torch.FloatTensor,\n        layer_head_mask: torch.FloatTensor,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n        hidden_states, attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        residual = hidden_states\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\n        if hidden_states.dtype == torch.float16 and (\n            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n        ):\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n\nclass BartDecoderLayer(nn.Module):\n    def __init__(self, config: BartConfig):\n        super().__init__()\n        self.embed_dim = config.d_model\n\n        self.self_attn = BartAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True,\n        )\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = config.activation_dropout\n\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.encoder_attn = BartAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True,\n        )\n        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        layer_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = True,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            encoder_hidden_states (`torch.FloatTensor`):\n                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n                `(encoder_attention_heads,)`.\n            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n                size `(decoder_attention_heads,)`.\n            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n\n        # Self Attention\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        # add present self-attn cache to positions 1,2 of present_key_value tuple\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n            layer_head_mask=layer_head_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Cross-Attention Block\n        cross_attn_present_key_value = None\n        cross_attn_weights = None\n        if encoder_hidden_states is not None:\n            residual = hidden_states\n\n            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n                hidden_states=hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n                output_attentions=output_attentions,\n            )\n            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n            hidden_states = residual + hidden_states\n            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n\n            # add cross-attn to positions 3,4 of present_key_value tuple\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n        hidden_states = residual + hidden_states\n        hidden_states = self.final_layer_norm(hidden_states)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights, cross_attn_weights)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nclass BartClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        inner_dim: int,\n        num_classes: int,\n        pooler_dropout: float,\n    ):\n        super().__init__()\n        self.dense = nn.Linear(input_dim, inner_dim)\n        self.dropout = nn.Dropout(p=pooler_dropout)\n        self.out_proj = nn.Linear(inner_dim, num_classes)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.dense(hidden_states)\n        hidden_states = torch.tanh(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.out_proj(hidden_states)\n        return hidden_states\n\n\nclass BartPreTrainedModel(PreTrainedModel):\n    config_class = BartConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _keys_to_ignore_on_load_unexpected = [\"encoder.version\", \"decoder.version\"]\n    _no_split_modules = [r\"BartEncoderLayer\", r\"BartDecoderLayer\"]\n    _skip_keys_device_placement = \"past_key_values\"\n\n    def _init_weights(self, module):\n        std = self.config.init_std\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (BartDecoder, BartEncoder)):\n            module.gradient_checkpointing = value\n\n    @property\n    def dummy_inputs(self):\n        pad_token = self.config.pad_token_id\n        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n        dummy_inputs = {\n            \"attention_mask\": input_ids.ne(pad_token),\n            \"input_ids\": input_ids,\n        }\n        return dummy_inputs\n\n\nclass PretrainedBartModel(BartPreTrainedModel):\n    def __init_subclass__(self):\n        warnings.warn(\n            \"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\",\n            FutureWarning,\n        )\n\n\nclass BartPretrainedModel(BartPreTrainedModel):\n    def __init_subclass__(self):\n        warnings.warn(\n            \"The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\",\n            FutureWarning,\n        )\n\n\nBART_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`BartConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nBART_GENERATION_EXAMPLE = r\"\"\"\n    Summarization example:\n\n    ```python\n    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n\n    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n    >>> ARTICLE_TO_SUMMARIZE = (\n    ...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n    ...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n    ...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n    ... )\n    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n\n    >>> # Generate Summary\n    >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n    ```\n\n    Mask filling example:\n\n    ```python\n    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n\n    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n    >>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n    >>> logits = model(input_ids).logits\n\n    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n    >>> probs = logits[0, masked_index].softmax(dim=0)\n    >>> values, predictions = probs.topk(5)\n\n    >>> tokenizer.decode(predictions).split()\n    ['not', 'good', 'healthy', 'great', 'very']\n    ```\n\"\"\"\n\nBART_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n            For translation and summarization training, `decoder_input_ids` should be provided. If no\n            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n            for denoising pre-training following the paper.\n        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n\n            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n            1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of shape\n            `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing `input_ids` you\n            can choose to directly pass an embedded representation. This is useful if you want more control over how to\n            convert `input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n            input (see `past_key_values`). This is useful if you want more control over how to convert\n            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n\n            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n            of `inputs_embeds`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\nclass BartEncoder(BartPreTrainedModel):\n    \"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n    [`BartEncoderLayer`].\n\n    Args:\n        config: BartConfig\n        embed_tokens (nn.Embedding): output embedding\n    \"\"\"\n\n    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n        super().__init__(config)\n\n        self.dropout = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\n        embed_dim = config.d_model\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n\n        if embed_tokens is not None:\n            self.embed_tokens.weight = embed_tokens.weight\n\n        self.embed_positions = BartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            embed_dim,\n        )\n        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input = input_ids\n            input_ids = input_ids.view(-1, input_ids.shape[-1])\n        elif inputs_embeds is not None:\n            input = inputs_embeds[:, :, -1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n\n        embed_pos = self.embed_positions(input)\n        embed_pos = embed_pos.to(inputs_embeds.device)\n\n        hidden_states = inputs_embeds + embed_pos\n        hidden_states = self.layernorm_embedding(hidden_states)\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        # expand attention_mask\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        # check if head_mask has a correct number of layers specified if desired\n        if head_mask is not None:\n            if head_mask.size()[0] != (len(self.layers)):\n                raise ValueError(\n                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n                    f\" {head_mask.size()[0]}.\"\n                )\n\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            to_drop = False\n            if self.training:\n                dropout_probability = torch.rand([])\n                if dropout_probability < self.layerdrop:  # skip the layer\n                    to_drop = True\n\n            if to_drop:\n                layer_outputs = (None, None)\n            else:\n                if self.gradient_checkpointing and self.training:\n\n                    def create_custom_forward(module):\n                        def custom_forward(*inputs):\n                            return module(*inputs, output_attentions)\n\n                        return custom_forward\n\n                    layer_outputs = torch.utils.checkpoint.checkpoint(\n                        create_custom_forward(encoder_layer),\n                        hidden_states,\n                        attention_mask,\n                        (head_mask[idx] if head_mask is not None else None),\n                    )\n                else:\n                    layer_outputs = encoder_layer(\n                        hidden_states,\n                        attention_mask,\n                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                        output_attentions=output_attentions,\n                    )\n\n                hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n        )\n\n\nclass BartDecoder(BartPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`BartDecoderLayer`]\n\n    Args:\n        config: BartConfig\n        embed_tokens (nn.Embedding): output embedding\n    \"\"\"\n\n    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n        super().__init__(config)\n        self.dropout = config.dropout\n        self.layerdrop = config.decoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_target_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n\n        if embed_tokens is not None:\n            self.embed_tokens.weight = embed_tokens.weight\n\n        self.embed_positions = BartLearnedPositionalEmbedding(\n            config.max_position_embeddings,\n            config.d_model,\n        )\n        self.layers = nn.ModuleList([BartDecoderLayer(config) for _ in range(config.decoder_layers)])\n        self.layernorm_embedding = nn.LayerNorm(config.d_model)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                of the decoder.\n            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n                selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the cross-attention modules in the decoder to avoid performing\n                cross-attention on hidden heads. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`. inputs_embeds (`torch.FloatTensor` of\n                shape `(batch_size, sequence_length, hidden_size)`, *optional*): Optionally, instead of passing\n                `input_ids` you can choose to directly pass an embedded representation. This is useful if you want more\n                control over how to convert `input_ids` indices into associated vectors than the model's internal\n                embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input = input_ids\n            input_shape = input.shape\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            input = inputs_embeds[:, :, -1]\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input) * self.embed_scale\n\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, input_shape, inputs_embeds, past_key_values_length\n        )\n\n        # expand encoder attention mask\n        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n\n        # embed positions\n        positions = self.embed_positions(input, past_key_values_length)\n        positions = positions.to(inputs_embeds.device)\n\n        hidden_states = inputs_embeds + positions\n        hidden_states = self.layernorm_embedding(hidden_states)\n\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n        next_decoder_cache = () if use_cache else None\n\n        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n            if attn_mask is not None:\n                if attn_mask.size()[0] != (len(self.layers)):\n                    raise ValueError(\n                        f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n                        f\" {head_mask.size()[0]}.\"\n                    )\n\n        for idx, decoder_layer in enumerate(self.layers):\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            if self.training:\n                dropout_probability = torch.rand([])\n                if dropout_probability < self.layerdrop:\n                    continue\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, use_cache)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    head_mask[idx] if head_mask is not None else None,\n                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n                    None,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                    cross_attn_layer_head_mask=(\n                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n                    ),\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            cross_attentions=all_cross_attentions,\n        )\n\n\n@add_start_docstrings(\n    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n    BART_START_DOCSTRING,\n)\nclass BartModel(BartPreTrainedModel):\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n\n        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n\n        self.encoder = BartEncoder(config, self.shared)\n        self.decoder = BartDecoder(config, self.shared)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, value):\n        self.shared = value\n        self.encoder.embed_tokens = self.shared\n        self.decoder.embed_tokens = self.shared\n\n    def get_encoder(self):\n        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=Seq2SeqModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_EXPECTED_OUTPUT_SHAPE,\n    )\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqModelOutput]:\n        # different to other models, Bart automatically creates decoder_input_ids from\n        # input_ids if no decoder_input_ids are provided\n        if decoder_input_ids is None and decoder_inputs_embeds is None:\n            if input_ids is None:\n                raise ValueError(\n                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n                )\n\n            decoder_input_ids = shift_tokens_right(\n                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n            )\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                head_mask=head_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n)\nclass BartForConditionalGeneration(BartPreTrainedModel):\n    base_model_prefix = \"model\"\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n    _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n\n    def __init__(self, config: BartConfig):\n        super().__init__(config)\n        self.model = BartModel(config)\n        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_encoder(self):\n        return self.model.get_encoder()\n\n    def get_decoder(self):\n        return self.model.get_decoder()\n\n    def resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None) -> nn.Embedding:\n        new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n        self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n        return new_embeddings\n\n    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n        old_num_tokens = self.final_logits_bias.shape[-1]\n        if new_num_tokens <= old_num_tokens:\n            new_bias = self.final_logits_bias[:, :new_num_tokens]\n        else:\n            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n        self.register_buffer(\"final_logits_bias\", new_bias)\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqLMOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        lm_logits = self.lm_head(outputs[0])\n        lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n\n        masked_lm_loss = None\n        if labels is not None:\n            labels = labels.to(lm_logits.device)\n            loss_fct = CrossEntropyLoss()\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        decoder_attention_mask=None,\n        head_mask=None,\n        decoder_head_mask=None,\n        cross_attn_head_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past_key_values is used\n        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"decoder_attention_mask\": decoder_attention_mask,\n            \"head_mask\": head_mask,\n            \"decoder_head_mask\": decoder_head_mask,\n            \"cross_attn_head_mask\": cross_attn_head_mask,\n            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n        }\n\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past[:2])\n                + layer_past[2:],\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n    tasks.\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForSequenceClassification(BartPreTrainedModel):\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n\n    def __init__(self, config: BartConfig, **kwargs):\n        super().__init__(config, **kwargs)\n        self.model = BartModel(config)\n        self.classification_head = BartClassificationHead(\n            config.d_model,\n            config.d_model,\n            config.num_labels,\n            config.classifier_dropout,\n        )\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n        output_type=Seq2SeqSequenceClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n    )\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n\n        if input_ids is None and inputs_embeds is not None:\n            raise NotImplementedError(\n                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n            )\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            encoder_outputs=encoder_outputs,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]  # last hidden state\n\n        eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n\n        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n        sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n            :, -1, :\n        ]\n        logits = self.classification_head(sentence_representation)\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.config.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.config.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return Seq2SeqSequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\n    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForQuestionAnswering(BartPreTrainedModel):\n    _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        config.num_labels = 2\n        self.num_labels = config.num_labels\n\n        self.model = BartModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        checkpoint=_CHECKPOINT_FOR_QA,\n        output_type=Seq2SeqQuestionAnsweringModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_loss=_QA_EXPECTED_LOSS,\n        expected_output=_QA_EXPECTED_OUTPUT,\n    )\n    def forward(\n        self,\n        input_ids: torch.Tensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        decoder_head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if start_positions is not None and end_positions is not None:\n            use_cache = False\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            head_mask=head_mask,\n            decoder_head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            encoder_outputs=encoder_outputs,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (\n                start_logits,\n                end_logits,\n            ) + outputs[1:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return Seq2SeqQuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n\nclass BartDecoderWrapper(BartPreTrainedModel):\n    \"\"\"\n    This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is\n    used in combination with the [`EncoderDecoderModel`] framework.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.decoder = BartDecoder(config)\n\n    def forward(self, *args, **kwargs):\n        return self.decoder(*args, **kwargs)\n\n\n@add_start_docstrings(\n    \"\"\"\n    BART decoder with with a language modeling head on top (linear layer with weights tied to the input embeddings).\n    \"\"\",\n    BART_START_DOCSTRING,\n)\nclass BartForCausalLM(BartPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        config = copy.deepcopy(config)\n        config.is_decoder = True\n        config.is_encoder_decoder = False\n        super().__init__(config)\n        self.model = BartDecoderWrapper(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.decoder.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model.decoder = decoder\n\n    def get_decoder(self):\n        return self.model.decoder\n\n    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                if the model is configured as a decoder.\n            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, BartForCausalLM\n\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n        >>> model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)\n        >>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n        >>> outputs = model(**inputs)\n\n        >>> logits = outputs.logits\n        >>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n        >>> list(logits.shape) == expected_shape\n        True\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model.decoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            head_mask=head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        logits = self.lm_head(outputs[0])\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, **kwargs\n    ):\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_ids.shape)\n\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n        # first step, decoder_cached_states are empty\n        return {\n            \"input_ids\": input_ids,  # encoder_outputs is defined. input_ids not needed\n            \"attention_mask\": attention_mask,\n            \"past_key_values\": past_key_values,\n            \"use_cache\": use_cache,\n        }\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:08:07.870821Z","iopub.execute_input":"2023-10-25T07:08:07.871242Z","iopub.status.idle":"2023-10-25T07:08:08.120633Z","shell.execute_reply.started":"2023-10-25T07:08:07.871185Z","shell.execute_reply":"2023-10-25T07:08:08.119438Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = BartForConditionalGeneration.from_pretrained(\"GuysTrans/bart-base-re-attention-mini-seq-512\")","metadata":{"id":"TlqNaB8jIrJW","outputId":"21d6c406-445d-49de-dc8e-a0d324047c0d","execution":{"iopub.status.busy":"2023-10-25T07:08:08.122073Z","iopub.execute_input":"2023-10-25T07:08:08.122752Z","iopub.status.idle":"2023-10-25T07:08:11.390207Z","shell.execute_reply.started":"2023-10-25T07:08:08.122714Z","shell.execute_reply":"2023-10-25T07:08:11.389196Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"markdown","source":"Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case.","metadata":{"id":"CczA5lJlIrJX"}},{"cell_type":"markdown","source":"To instantiate a `Seq2SeqTrainer`, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:","metadata":{"id":"_N8urzhyIrJY"}},{"cell_type":"code","source":"batch_size = 8\nmodel_name = model_checkpoint.split(\"/\")[-1]\nargs = Seq2SeqTrainingArguments(\n    f\"{model_name}-re-attention-mini-seq-512\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    fp16=True,\n#     push_to_hub=True,\n)","metadata":{"id":"ie_CLhlEiIFq","execution":{"iopub.status.busy":"2023-10-25T07:08:11.391816Z","iopub.execute_input":"2023-10-25T07:08:11.392115Z","iopub.status.idle":"2023-10-25T07:08:11.406434Z","shell.execute_reply.started":"2023-10-25T07:08:11.392088Z","shell.execute_reply":"2023-10-25T07:08:11.405121Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"markdown","source":"Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the cell and customize the weight decay. Since the `Seq2SeqTrainer` will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the `predict_with_generate` option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n\nThe last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/t5-finetuned-xsum\"` or `\"huggingface/t5-finetuned-xsum\"`).\n\nThen, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:","metadata":{"id":"km3pGVdTIrJc"}},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"id":"lP7bL6M3iIFr","execution":{"iopub.status.busy":"2023-10-25T07:08:11.407639Z","iopub.execute_input":"2023-10-25T07:08:11.407959Z","iopub.status.idle":"2023-10-25T07:08:11.415572Z","shell.execute_reply.started":"2023-10-25T07:08:11.407934Z","shell.execute_reply":"2023-10-25T07:08:11.414633Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"markdown","source":"The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:","metadata":{"id":"7sZOdRlRIrJd"}},{"cell_type":"code","source":"import nltk\nimport numpy as np\nimport nltk\nnltk.download('punkt')\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    # Note that other metrics may not have a `use_aggregator` parameter\n    # and thus will return a list, computing a metric for each sentence.\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n    # Extract a few results\n    result = {key: value * 100 for key, value in result.items()}\n\n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"id":"UmvbnJ9JIrJd","outputId":"05a33966-2747-45f6-ee91-51e0fc7c8945","execution":{"iopub.status.busy":"2023-10-25T07:08:11.416803Z","iopub.execute_input":"2023-10-25T07:08:11.417525Z","iopub.status.idle":"2023-10-25T07:08:11.431551Z","shell.execute_reply.started":"2023-10-25T07:08:11.417486Z","shell.execute_reply":"2023-10-25T07:08:11.430537Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.python.sh | bash","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:08:11.432776Z","iopub.execute_input":"2023-10-25T07:08:11.433115Z","iopub.status.idle":"2023-10-25T07:08:13.065981Z","shell.execute_reply.started":"2023-10-25T07:08:11.433084Z","shell.execute_reply":"2023-10-25T07:08:13.064828Z"},"trusted":true},"execution_count":174,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\ncurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\nAlready configured pip for this repository, skipping\n","output_type":"stream"}]},{"cell_type":"code","source":"!apt-get install git-lfs","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:08:13.068003Z","iopub.execute_input":"2023-10-25T07:08:13.068515Z","iopub.status.idle":"2023-10-25T07:08:15.514938Z","shell.execute_reply.started":"2023-10-25T07:08:13.068461Z","shell.execute_reply":"2023-10-25T07:08:15.513777Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit-lfs is already the newest version (3.0.2-1ubuntu0.2).\n0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:","metadata":{"id":"rXuFTAzDIrJe"}},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"id":"imY1oC3SIrJf","outputId":"34ec5d0d-2a60-4e11-ac70-95d132e9ed01","execution":{"iopub.status.busy":"2023-10-25T07:08:15.516966Z","iopub.execute_input":"2023-10-25T07:08:15.517450Z","iopub.status.idle":"2023-10-25T07:08:15.688397Z","shell.execute_reply.started":"2023-10-25T07:08:15.517409Z","shell.execute_reply":"2023-10-25T07:08:15.687287Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"markdown","source":"We can now finetune our model by just calling the `train` method:","metadata":{"id":"CdzABDVcIrJg"}},{"cell_type":"code","source":"for i in range(100):\n    trainer.train()\n#     trainer.push_to_hub()","metadata":{"id":"uNx5pyRlIrJh","execution":{"iopub.status.busy":"2023-10-25T07:44:18.042489Z","iopub.execute_input":"2023-10-25T07:44:18.043566Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 11/125 00:08 < 01:48, 1.05 it/s, Epoch 0.08/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from tabulate import tabulate","metadata":{"id":"GNAvTDPSaW1H","execution":{"iopub.status.busy":"2023-10-25T07:31:24.630579Z","iopub.execute_input":"2023-10-25T07:31:24.630970Z","iopub.status.idle":"2023-10-25T07:31:24.658370Z","shell.execute_reply.started":"2023-10-25T07:31:24.630924Z","shell.execute_reply":"2023-10-25T07:31:24.657277Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"model_before_finetuned = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)","metadata":{"id":"qGwajSFvp13X","execution":{"iopub.status.busy":"2023-10-25T07:31:24.659817Z","iopub.execute_input":"2023-10-25T07:31:24.660183Z","iopub.status.idle":"2023-10-25T07:31:35.450059Z","shell.execute_reply.started":"2023-10-25T07:31:24.660148Z","shell.execute_reply":"2023-10-25T07:31:35.448767Z"},"trusted":true},"execution_count":179,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a88dca0b474b119a0101e970b289a8"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_summary(test_samples, model):\n    inputs = tokenizer(\n        test_samples[\"question\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n        return_tensors=\"pt\",\n    )\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=512)\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return outputs, output_str","metadata":{"id":"oSce-pEPZ0Jq","execution":{"iopub.status.busy":"2023-10-25T07:31:35.451907Z","iopub.execute_input":"2023-10-25T07:31:35.452641Z","iopub.status.idle":"2023-10-25T07:31:35.461715Z","shell.execute_reply.started":"2023-10-25T07:31:35.452599Z","shell.execute_reply":"2023-10-25T07:31:35.460635Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"test_samples = vals_ds.select(range(8))\n\n\nsummaries_before_tuning = generate_summary(test_samples, model_before_finetuned)[1]\nsummaries_after_tuning = generate_summary(test_samples, model)[1]","metadata":{"id":"OX4PJRRCZ5Wl","execution":{"iopub.status.busy":"2023-10-25T07:31:35.463320Z","iopub.execute_input":"2023-10-25T07:31:35.463653Z","iopub.status.idle":"2023-10-25T07:34:40.088950Z","shell.execute_reply.started":"2023-10-25T07:31:35.463621Z","shell.execute_reply":"2023-10-25T07:34:40.087878Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"print(\n    tabulate(\n        zip(\n            range(len(summaries_after_tuning)),\n            summaries_after_tuning,\n            summaries_before_tuning,\n        ),\n        headers=[\"Id\", \"Answer after\", \"Answer before\"],\n    )\n)\nprint(\"\\nTarget answers:\\n\")\nprint(\n    tabulate(list(enumerate(test_samples[\"answer\"])), headers=[\"Id\", \"Target answer\"])\n)\nprint(\"\\nSource questions:\\n\")\nprint(tabulate(list(enumerate(test_samples[\"question\"])), headers=[\"Id\", \"Question\"]))","metadata":{"id":"ghUn2o8jaRSm","execution":{"iopub.status.busy":"2023-10-25T07:34:40.090469Z","iopub.execute_input":"2023-10-25T07:34:40.090820Z","iopub.status.idle":"2023-10-25T07:34:40.138406Z","shell.execute_reply.started":"2023-10-25T07:34:40.090787Z","shell.execute_reply":"2023-10-25T07:34:40.137373Z"},"trusted":true},"execution_count":182,"outputs":[{"name":"stdout","text":"  Id  Answer after                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Answer before\n----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   0  Yes, you have vaginal bleeding. I have gone through the query and understand your concern. I would like to understand your concerns.  I have been through your concern                                                                                                                                                                                                                                                                                                                                                                                                                I have vaginal bleeding for three years. No pain. I would like to know why.\n      your doctor. I am\n      concerns. I can understand your question.  You are having vaginal bleeding? I am at  I would suggest you take a  C T T T S T T C T C G T C C T S P T C.  C- T C- C T- T T- C-\n      t C-T- T-  Healthcare Magic. Thanks for your\n      your concern.\n   1  Yes, you have liver disease. Thanks for your concern.  I have gone through your query. I have reviewed your query                                                                                                                                                                                                                                                                                                                                                                                                                                                                     coarse echotexture of liver no focal lesion CBD normal portal vein prominent -- to rule parenchymal liver disease.. spleenomegally with multiple collaterals. gall bladder is moderately distented.. kidneys,pancreas,postate all normal 3 months i have taken treatment.. my tablets are udiliv,zincovit,liveril forte.. first my spleen was 19.5cm, recent report shows 16cm bilirubin-1.9 albumin-4.5 globulin-2.5 sgot-27 sgpt-13 serum alkaline phosphate-59    my liver size was 9.5 cm.  And three months aftr my Spleen reduced to 16.5CM and collateral size reduced to 20cm.   This is the reason for my liver problem..   I need liver transplant...   My liver problem is controllable and it will progress to cirrhosis.. so i need hep b,c d.. and now i need transplant.. and i need liver problem to be cured.. and I need transplant... and I don't know what to do for this.. I was in first hospital for 5 or 10 yrs.. and there was no pblm due to spleen.. and absence of liver b, c d...   d   l l l d.... and then i went to another hospital.. there  after one yr i took scan now again I can't drink or smoke.. i can't smoke.. and u can postpone your degradation of liver.. and also i can postpone my degradation of spleen.\n      your doctor.  H C level is normal.  C liver disease is very low.   C R  R  C T R 7.  R 7 is 7.7.  D 7. Please take  C C T 7.6. Please attach a 7.5.  Healthcare Magic 7. D C T  D C is not required.  Thanks for the query.  Your liver is very high in liver disease Welcome to your doctor and  D D 7 days.  This is normal liver disease  C D 7  C 7.1. D 7 is  C6.  P 7.12  C 6.6  C is  D 6. D6. D7.6 is  Healthcare  D6  D7  D12. C C 7  D10 is  P12.  E 7.3.  V 7.8 is normal  C P12 is not a liver transplant.  You can take 7 days to 7.4. D12 is  R 6.7  C7. Please send a\n      suggested\n      d 7. 7. Healthcare  P6.1  C12 is normal\n      c 7. C12.6% is  H A 7.\n      6.6\n      c is 7%  C10.  L 7. P12  R6.12 is 7 days  C8.  T12 is 6.1 is 7 weeks.  Please take 7 day.  A 7 day  D8 is 7 months.  X X 7. Your liver can be 7. I can understand your concerns.  There is no need to take 7.2.  S6. I am 7. H C 7 Healthcare Magic 8.6 Healthcare  C level 7. R12.7 Healthcare Magic 6.5 is 7\n      12.1%  R12  P14 is 7+12.12. P14.1+12  D3 is 7, 7.14  D5.1,6.5% is 7-6.7% 7.10%  P10. 7 day is 7 month. 7 days 7. 6. 7 weeks  P7.1-12.3% 7 day 7.\n   2  Pregnancy can be due to miscarriage. I have gone through your question and understand your concern.  I have been through your concern                                                                                                                                                                                                                                                                                                                                                                                                                                                 After miscarriage of pregnancy bleeding is going on for last 20days in less quantity so suggest me how to stop the bleding\n      concerns\n      pregnancy. I would suggest you go for a proton pump for a  C M R  R  Healthcare Magic. Pregnancy is normal.  You can take a  R R R  P R  C R R 7. R 7  R 7 is 7 days.  R P R 7 weeks of pregnancy is 7 weeks. R  Thanks for your question.  Your question and your question is\n      your doctor is at 7 days of your doctor. R R C R 7 day is 7 months.   R 6 weeks 7 weeks 7 days 7 weeks  R A R 7 R 7\n      p 7. 7.6 weeks 7 months 7.\n      6.  C P R 6. R P 7. A 7.7. R A 7  Healthcare  R C P 7  P 6.6 is 7.1. R C C R  Dr  R 8.  P 7 day 7. P 7 weeks is 7 month 7. Thanks for writing in your question  R D R R. R. 7\n      your 7.5  R. 6.7  R 1.6  R for 7.8  R\n      7.6.6\n      p is 7-month 7.12  R is 7\n      8. 7-6.7 weeks 7. Please take  R E 7.3. 7 day  R N R 7 A R  A R R P 6 is 7 day.  A 7 day R 7 week is 7 week 7. C R P A R P C R. Please send a 7 day ultrasound  R- 7. Your doctor has gone through 7 days for a 7.4  R I R 7-7. 7 days  R O R 7 7\n      6\n      6-12.6-6\n      7am 7-12 is 7am 7.2am 7\n      1.6am 7 day 6. 7 weeks\n   3  Yes, you have lind.  I have gone through the query and understand your concern. I have been through your concern and understand the concern.  Lind is very concerned about the throat. This is a very severe throat upper upper lobe upper lobe throat upper lobe lobe lobe.  This is very severe.  C T R  C C C T C C is very low.  Dr  C  C P C is  Healthcare Magic  D C is                                                                                                                                                                                                        I am having some mouth ulcer lind of thing from yesterday. It happened due to drinking something very hot and some cut due to some cut while eating something.Can u plzz suggest some ointment please.\n      di  C G C C. Thanks for your concern  C S D C C  R  Healthcare  C R  R A C C S R A R A  R C C R A is not a severe throat\n      med.  R R A A R R C R C is a  R E R  A R C A R  E R R  D  R is  R N  C A C R is\n      n  C- C C P A C is A R N C C D C  D R  P C C A  C D  C.  Healthcare\n      and  C M R  S R D C P D C. C R E C C E R A D C R P C R N R C  Healthcare Healthcare  R P A R P  R Healthcare  P R R R P R  N  R\n        R 7  R S R R\n       R R is R   R I R R N A R\n\n      c  R D R R E  R. R  I R  H A R E N R R 7 Healthcare  Healthcare Dr  R V C  P A  D A  P  C is R A N R  U R R.  P D R C P R A Healthcare  D E R P N R A P R P S R  M R R S A R 7 is  C H C R R D  P P R N P R 7 A R is a R R Healthcare Magic 7  C N R 7 C R 7 N R is 7  Healthcare R R for  R H R R 1  R 2  R 1 R 7 R 7\n       R 7. R P is  P S  R 6  R O R R H C C O R  You R R V R  Dr R  X R R X R  Your R R 3  R for R R 2. R R U R A S R 7 day R R 6 R R\n   4  No need to worry about it.  I have gone through the query and understand your concern.  Hivivivitis is not an ice cream. I would suggest you take ice cream  ice cream for a day.  Ice cream is not a problem.  Thanks for your question.  You can take    I take                                                                                                                                                                                                                                                                                                                     For my information..I would like to know if you share an ice cream win someone can you get hiv or std since other person has licked the ice cream __________________ Also if I share ice cream with a person who is hiv, can I get haiv from sharing ice cream\n       ice cream\n      cream   H V  Healthcare Magic   Healthcare   Welcome to your doctor.\n   5  Please do not worry about it. I have gone through the query and understand your concern.  I have been through your concerns. I would suggest you take a vaginal x-ray.  E R E R  X-ray  C R  C is  C G C C C T C C  C T D C is a vaginal effusion.  D E C C D C C is not a vaginal discharge. I am at  C C G D C D E R is at  Healthcare Magic.  Thanks for your question and your question                                                                                                                                                                                           Doc my vagina is leaking white clumps of discharge is there any at home treatments that I can do? My vagina also swells when I masturbate what could be causing this?\n      your concern. Your concern is at\n      your doctor\n      your vaginal\n      vaginal\n      your vagina is a vagina.  C E R C C E D E D C  E D  C D  E C is\n      delges vaginal effray is  E S D C G E C E C D is  D C E E D is\n      dic  C S D E N C C S E D D E is  Healthcare  C P E D P E C G G D E  E N G C G V C C P C C R E C P D E E C  G C E N D E P E R D E\n      d E C R is  P E N E C N C E P is  G E N  C  Healthcare\n      del  C N E N P E\n      n is  V C  P C E  D  P  E P C is P E E P  P D C P is\n      se is  N E P P E P N C P N E E N V E P V E N N C  V E  P V C P  C V E C S P E V  E  V D E A C P V  P N  E A  Healthcare Healthcare Magic  Healthcare Doctor  P A  C O P E A P E  C A  E E R P N P  D P  V  Healthcare N E  A C C N  P P  A E P A N E R V  V V  A N  V A  V N  A A C E V V E V E A V  C H C P A C  A  P O  E V A C T E P D P V V V A N V  N V A E N A  N P V A A  A P A P  N  N A C G  E\n       A C R P  Healthcare P E S\n   6  Yes, it is. I have gone through the query and understand your concern.  I have been through your concern                                                                                                                                                                                                                                                                                                                                                                                                                                                                              A few days ago:Day after eating a rhubarb, raspberry granita, my stool has been pinkish red. I have not had blood in my stool before.  I'm wondering if I should be concerned.\n      your concern. I would suggest you go for a stool for a urine test.  R. R.  Dr. Dr.    R  R R. Dr   C.  C  C is  C C C is\n      and  C T C C. D C. is  Healthcare Magic.  Thanks for your concern  Dr  D.  D C  D  C R  C P C C  R is  R E R. D  D E R  Healthcare  D R  D is  D D  R\n        C D C C\n\n      your\n      del  C S D C is D C D  Healthcare\n      D  D N  D\n        D P D  E D C\n      D C C D E D E C is C D D E\n      D E D is\n      d C D. D E N D E  C E C D is D E E D. C D N D C E D   Healthcare D E is  E R E D\n      d E C  E C. E D N C is Healthcare  C G D E S D E.  E  E E C C E\n      d  E N  C\n      d\n      d Healthcare  E\n\n      d is  P E C E  D Healthcare  R D  P D E P E D D is Healthcare D  N D  A D E A  D A  E P D C P E E E R D E V E D P E  P is  N E D Healthcare D C O D E Healthcare  Healthcare Dr  C- D  V E   D O D C R E C P D is C E R P E N E C G E D A C D P  E S E D R E  N  E A D  H C is A C C P  C N E  R N  R A  C H C  A C E N C  P C  N C C T E D O C is a  C M D E T E C O C C O  C A C  I D E O D  I N C E P  Healthcare Doctor  C O R  A R E N P E R A D C N C D O R E E N R  P  R C C R D C A  R P D D C\n   7  Need more information.  I have gone through your query and understand your concern. I have been through your concern and understand the concern.  You have pneumonia.  C T R E R  C E R E C C C  C C is suffering from throat upper respiratory upper respiratory tract infection.   C R E  C is  C S E R C C E C is C T C C T E C  Healthcare Magic.  Thanks for your question and your question.  Your doctor has gone through the query.  Healthcare  C G C C S R C  R  E R A C C R  R E                                                                                           I am suffering from PATM, any cure for this. People are cough sneeze and clear throat when they around me. This makes me crazy\n      your doctor is at  C A  C  E C E E C R C is\n      medicine  C D C C. C C D E C.  E E R is  R C E A C E is  Healthcare\n      c  C P E C S C C P C C A C  D E R R C R is C C G E C T  C H C C for C C M  C. E C P  C M C is a  C- C C-  C N C C\n      med  C for throat\n      s  C+ C C N  C\n\n\n      med\n      med C C+  C 7 is  D C  P C is A C R 7 is C E N C  A C is Healthcare  E D C is P C E  E N  E P is  P E P C  S E C D is  E S E P E N E C G  C O C C V C  H C  I C P A C P is C P D C E P  E  Healthcare Healthcare Magic  C V E C N P E R P E  P  P A is  A  Healthcare Doctor  C Healthcare Magic C C O P C R P  R A  E A R  Healthcare C C H A C S P E\n        C level is  S P C S A C T A C A is C R R  A R E P P E E P A  R P C A A C M R  P is A P E A  P P C H E C A R C P R  S C   P N C E D E P R E A P C N R C S  C and C C Healthcare  P S C R A R P A P A N C is at C  N C\n\nTarget answers:\n\n  Id  Target answer\n----  ------------------------------------------------------------------------------------------------------------------------\n   0  Gynae examination and ultrasound advised  I understand your anxiety about the issue and it is indeed serious and should\n      not be taken lightly  Get a pelvic examination from a good gynaecologist and an ultrasound at the earliest  It can be\n      serious so much so like cancer or can just be related to postmenopausal hormone deficiency  Get the tests and then we\n      can decide further\n   1  Need more information Thanks for posting your query. I am  Dr. R. K and  I am pleased to assist you. I need some more\n      information before  I can give my opinion. I would like to have some more information before  I can give my opinion.1.\n      Was fibroscan or a liver biopsy done?2.  Was endoscopy done?3.  Are you a diabetic? Dr. R. K.\n   2  Retained products of conception can be there I have gone through your question and understand the concerns. Continuous\n      and prolonged bleeding after miscarriage can be due to retained products of conception in the uterine cavity. I will\n      suggest you to get an ultrasound done to assess if any retained products are there. If present, then you may require\n      surgical evacuation to remove the products. You can get back to me with the ultrasound report. Hope you found the answer\n      helpful. Dr  Deepti  Verma O B G Y N\n   3  Dologel  C T or zytee ointment... I would suggest  Dologel  C T on the affected areas.  Alternatively you may try  Zytee\n      ointment or even liquid glycerine.  All these are available at any nearest pharmacy / chemist stores. In addition, you\n      may also start tab zincovit once a day for a week for added benefits. Usually apthous ulcers such as those should heal\n      within a week's time.  Avoid hot and spicy food. Hope this helps.\n   4  Please don't worry. There is no risk to get  H I V  infection. I have been through your question. Related your concern\n      you shouldn't worry because you cannot get  H I V or  S T D from sharing  any kind of foods(even ice cream) with a\n      person infected with  H I V or  S T D. I hope my answer helps you.\n   5  It indicates infection I have gone through the details.  So a white discharge which is either excessive or foul smelling\n      usually indicates infection,  can be either fungal candida or bacterial vaginosis.   You will require antibiotics or\n      antifungal drugs depending upon the clinical examination.  Swelling can be due to either infection or vigorous efforts.\n      Please visit your gynecologist for a prescription for the medicines as soon as possible.\n   6  Rhubarb can be the cause Dear sir,  Rhubarb can cause pinkish red stools but this should only persist for another day or\n      two. If it persists longer than that then you need to exclude the presence of blood in stool by a stool analysis. I hope\n      I answered your question, let me know if  I can assist you further.\n   7  Please provide me more details  Healthcare Magic..  I can understand your concern.  It's very difficult for you as\n      people around you develop allergy symptoms and this is resulting in distress.  What are your main presenting symptoms?\n      Is there any specific situation during which maximum distress occurs? What is the opinion of other people around you?\n      Thanks and  Take care.  Please provide me more details in follow up.\n\nSource questions:\n\n  Id  Question\n----  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   0  I have vaginal bleeding for three years. No pain . I would like to know why.\n   1  coarse echotexture of liver no focal lesion CBD normal portal vein prominent -- to rule parenchymal liver disease.. spleenomegally with multiple collaterals. gall bladder is moderately distented.. kidneys,pancreas,postate all normal 3 months i have taken treatment.. my tablets are udiliv,zincovit,liveril forte.. first my spleen was 19.5cm , recent report shows 16cm bilirubin-1.9 albumin-4.5 globulin-2.5 sgot-27 sgpt-13 serum alkaline phosphate-59   my liver size was 9.5cm.  And three months aftr my spleen reduced to 16.5cm and collaterals size reducedThese r my report sir in 2015 XXXXXXX  Tell me whether it is controllable or it surely progress to cirrhosis.. so i need transplant...               i won't drink or smoke.. and absence of hep b,c d.. .               Then due to work i went to another hospital.. there  after one yr i took scan now again my spleen goes to 20cm .. here they stopped  udiliv tablet jst they giving only one tablet propanol for last three months .. another pblm due to spleen was platelets 32000 and wbc 2000.. kindly help me what i should do for this.. reason for my liver problem was unknown.. but now here they are saying may after 5 or 10 yrs you need transplant... in first hospital they told it is controllabe you CNT get your older size liver but u can postpone your degradation of liver.\n   2  After miscarriage of pregnancy bleeding is going on for last 20days in less quantity so suggest me how to stop the bleding\n   3  I am having some mouth ulcer lind of thing from yesterday. It happened due to drinking something very hot and some cut due to some cut while eating something.Can u plzz suggest some ointment please.\n   4  For my information ..I would like to know if you share an ice cream win someone can you get hiv or std since other person has licked the ice cream\n       Also if I share ice cream with a person who is hiv, can I get hiv from sharing the ice cream\n   5  Doc my vagina is leaking white clumps of discharge is there any at home treatments that I can do? My vagina also swells when I masturbate what could be causing this?\n   6  A\n      Day after eating a rhubarb, raspberry granita, my stool has been pinkish red. I have not had blood in my stool before.  I'm wondering if I should be concerned.\n   7  I am suffering from PATM, any cure for this. People are cough sneeze and clear throat when they around me. This makes me crazy\n","output_type":"stream"}]},{"cell_type":"code","source":"summaries_after_tuning","metadata":{"id":"iVEDsRW6sc_j","execution":{"iopub.status.busy":"2023-10-25T07:34:40.140075Z","iopub.execute_input":"2023-10-25T07:34:40.140490Z","iopub.status.idle":"2023-10-25T07:34:40.149924Z","shell.execute_reply.started":"2023-10-25T07:34:40.140456Z","shell.execute_reply":"2023-10-25T07:34:40.148961Z"},"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"['Yes, you have vaginal bleeding. I have gone through the query and understand your concern. I would like to understand your concerns.  I have been through your concern\\nyour doctor. I am\\nconcerns. I can understand your question.  You are having vaginal bleeding? I am at  I would suggest you take a  C T T T S T T C T C G T C C T S P T C.  C- T C- C T- T T- C-\\nt C-T- T-  Healthcare Magic. Thanks for your\\nyour concern.',\n 'Yes, you have liver disease. Thanks for your concern.  I have gone through your query. I have reviewed your query\\nyour doctor.  H C level is normal.  C liver disease is very low.   C R  R  C T R 7.  R 7 is 7.7.  D 7. Please take  C C T 7.6. Please attach a 7.5.  Healthcare Magic 7. D C T  D C is not required.  Thanks for the query.  Your liver is very high in liver disease Welcome to your doctor and  D D 7 days.  This is normal liver disease  C D 7  C 7.1. D 7 is  C6.  P 7.12  C 6.6  C is  D 6. D6. D7.6 is  Healthcare  D6  D7  D12. C C 7  D10 is  P12.  E 7.3.  V 7.8 is normal  C P12 is not a liver transplant.  You can take 7 days to 7.4. D12 is  R 6.7  C7. Please send a\\nsuggested\\nd 7. 7. Healthcare  P6.1  C12 is normal\\nc 7. C12.6% is  H A 7.\\n6.6\\nc is 7%  C10.  L 7. P12  R6.12 is 7 days  C8.  T12 is 6.1 is 7 weeks.  Please take 7 day.  A 7 day  D8 is 7 months.  X X 7. Your liver can be 7. I can understand your concerns.  There is no need to take 7.2.  S6. I am 7. H C 7 Healthcare Magic 8.6 Healthcare  C level 7. R12.7 Healthcare Magic 6.5 is 7\\n12.1%  R12  P14 is 7+12.12. P14.1+12  D3 is 7, 7.14  D5.1,6.5% is 7-6.7% 7.10%  P10. 7 day is 7 month. 7 days 7. 6. 7 weeks  P7.1-12.3% 7 day 7.',\n 'Pregnancy can be due to miscarriage. I have gone through your question and understand your concern.  I have been through your concern\\nconcerns\\npregnancy. I would suggest you go for a proton pump for a  C M R  R  Healthcare Magic. Pregnancy is normal.  You can take a  R R R  P R  C R R 7. R 7  R 7 is 7 days.  R P R 7 weeks of pregnancy is 7 weeks. R  Thanks for your question.  Your question and your question is\\nyour doctor is at 7 days of your doctor. R R C R 7 day is 7 months.   R 6 weeks 7 weeks 7 days 7 weeks  R A R 7 R 7\\np 7. 7.6 weeks 7 months 7.\\n6.  C P R 6. R P 7. A 7.7. R A 7  Healthcare  R C P 7  P 6.6 is 7.1. R C C R  Dr  R 8.  P 7 day 7. P 7 weeks is 7 month 7. Thanks for writing in your question  R D R R. R. 7\\nyour 7.5  R. 6.7  R 1.6  R for 7.8  R\\n7.6.6\\np is 7-month 7.12  R is 7\\n8. 7-6.7 weeks 7. Please take  R E 7.3. 7 day  R N R 7 A R  A R R P 6 is 7 day.  A 7 day R 7 week is 7 week 7. C R P A R P C R. Please send a 7 day ultrasound  R- 7. Your doctor has gone through 7 days for a 7.4  R I R 7-7. 7 days  R O R 7 7\\n6\\n6-12.6-6\\n7am 7-12 is 7am 7.2am 7\\n1.6am 7 day 6. 7 weeks',\n 'Yes, you have lind.  I have gone through the query and understand your concern. I have been through your concern and understand the concern.  Lind is very concerned about the throat. This is a very severe throat upper upper lobe upper lobe throat upper lobe lobe lobe.  This is very severe.  C T R  C C C T C C is very low.  Dr  C  C P C is  Healthcare Magic  D C is\\ndi  C G C C. Thanks for your concern  C S D C C  R  Healthcare  C R  R A C C S R A R A  R C C R A is not a severe throat\\nmed.  R R A A R R C R C is a  R E R  A R C A R  E R R  D  R is  R N  C A C R is\\nn  C- C C P A C is A R N C C D C  D R  P C C A  C D  C.  Healthcare\\nand  C M R  S R D C P D C. C R E C C E R A D C R P C R N R C  Healthcare Healthcare  R P A R P  R Healthcare  P R R R P R  N  R\\n  R 7  R S R R\\n R R is R   R I R R N A R\\n\\nc  R D R R E  R. R  I R  H A R E N R R 7 Healthcare  Healthcare Dr  R V C  P A  D A  P  C is R A N R  U R R.  P D R C P R A Healthcare  D E R P N R A P R P S R  M R R S A R 7 is  C H C R R D  P P R N P R 7 A R is a R R Healthcare Magic 7  C N R 7 C R 7 N R is 7  Healthcare R R for  R H R R 1  R 2  R 1 R 7 R 7\\n R 7. R P is  P S  R 6  R O R R H C C O R  You R R V R  Dr R  X R R X R  Your R R 3  R for R R 2. R R U R A S R 7 day R R 6 R R',\n 'No need to worry about it.  I have gone through the query and understand your concern.  Hivivivitis is not an ice cream. I would suggest you take ice cream  ice cream for a day.  Ice cream is not a problem.  Thanks for your question.  You can take    I take \\n ice cream\\ncream   H V  Healthcare Magic   Healthcare   Welcome to your doctor.',\n 'Please do not worry about it. I have gone through the query and understand your concern.  I have been through your concerns. I would suggest you take a vaginal x-ray.  E R E R  X-ray  C R  C is  C G C C C T C C  C T D C is a vaginal effusion.  D E C C D C C is not a vaginal discharge. I am at  C C G D C D E R is at  Healthcare Magic.  Thanks for your question and your question\\nyour concern. Your concern is at\\nyour doctor\\nyour vaginal\\nvaginal\\nyour vagina is a vagina.  C E R C C E D E D C  E D  C D  E C is\\ndelges vaginal effray is  E S D C G E C E C D is  D C E E D is\\ndic  C S D E N C C S E D D E is  Healthcare  C P E D P E C G G D E  E N G C G V C C P C C R E C P D E E C  G C E N D E P E R D E\\nd E C R is  P E N E C N C E P is  G E N  C  Healthcare\\ndel  C N E N P E\\nn is  V C  P C E  D  P  E P C is P E E P  P D C P is\\nse is  N E P P E P N C P N E E N V E P V E N N C  V E  P V C P  C V E C S P E V  E  V D E A C P V  P N  E A  Healthcare Healthcare Magic  Healthcare Doctor  P A  C O P E A P E  C A  E E R P N P  D P  V  Healthcare N E  A C C N  P P  A E P A N E R V  V V  A N  V A  V N  A A C E V V E V E A V  C H C P A C  A  P O  E V A C T E P D P V V V A N V  N V A E N A  N P V A A  A P A P  N  N A C G  E\\n A C R P  Healthcare P E S',\n 'Yes, it is. I have gone through the query and understand your concern.  I have been through your concern\\nyour concern. I would suggest you go for a stool for a urine test.  R. R.  Dr. Dr.    R  R R. Dr   C.  C  C is  C C C is\\nand  C T C C. D C. is  Healthcare Magic.  Thanks for your concern  Dr  D.  D C  D  C R  C P C C  R is  R E R. D  D E R  Healthcare  D R  D is  D D  R\\n  C D C C\\n\\nyour\\ndel  C S D C is D C D  Healthcare\\nD  D N  D\\n  D P D  E D C\\nD C C D E D E C is C D D E\\nD E D is\\nd C D. D E N D E  C E C D is D E E D. C D N D C E D   Healthcare D E is  E R E D\\nd E C  E C. E D N C is Healthcare  C G D E S D E.  E  E E C C E\\nd  E N  C\\nd\\nd Healthcare  E\\n\\nd is  P E C E  D Healthcare  R D  P D E P E D D is Healthcare D  N D  A D E A  D A  E P D C P E E E R D E V E D P E  P is  N E D Healthcare D C O D E Healthcare  Healthcare Dr  C- D  V E   D O D C R E C P D is C E R P E N E C G E D A C D P  E S E D R E  N  E A D  H C is A C C P  C N E  R N  R A  C H C  A C E N C  P C  N C C T E D O C is a  C M D E T E C O C C O  C A C  I D E O D  I N C E P  Healthcare Doctor  C O R  A R E N P E R A D C N C D O R E E N R  P  R C C R D C A  R P D D C',\n 'Need more information.  I have gone through your query and understand your concern. I have been through your concern and understand the concern.  You have pneumonia.  C T R E R  C E R E C C C  C C is suffering from throat upper respiratory upper respiratory tract infection.   C R E  C is  C S E R C C E C is C T C C T E C  Healthcare Magic.  Thanks for your question and your question.  Your doctor has gone through the query.  Healthcare  C G C C S R C  R  E R A C C R  R E\\nyour doctor is at  C A  C  E C E E C R C is\\nmedicine  C D C C. C C D E C.  E E R is  R C E A C E is  Healthcare\\nc  C P E C S C C P C C A C  D E R R C R is C C G E C T  C H C C for C C M  C. E C P  C M C is a  C- C C-  C N C C\\nmed  C for throat\\ns  C+ C C N  C\\n\\n\\nmed\\nmed C C+  C 7 is  D C  P C is A C R 7 is C E N C  A C is Healthcare  E D C is P C E  E N  E P is  P E P C  S E C D is  E S E P E N E C G  C O C C V C  H C  I C P A C P is C P D C E P  E  Healthcare Healthcare Magic  C V E C N P E R P E  P  P A is  A  Healthcare Doctor  C Healthcare Magic C C O P C R P  R A  E A R  Healthcare C C H A C S P E\\n  C level is  S P C S A C T A C A is C R R  A R E P P E E P A  R P C A A C M R  P is A P E A  P P C H E C A R C P R  S C   P N C E D E P R E A P C N R C S  C and C C Healthcare  P S C R A R P A P A N C is at C  N C']"},"metadata":{}}]},{"cell_type":"code","source":"list(enumerate(test_samples[\"answer\"]))","metadata":{"id":"NJVLQqcvuZBx","execution":{"iopub.status.busy":"2023-10-25T07:34:40.151145Z","iopub.execute_input":"2023-10-25T07:34:40.151509Z","iopub.status.idle":"2023-10-25T07:34:40.161783Z","shell.execute_reply.started":"2023-10-25T07:34:40.151484Z","shell.execute_reply":"2023-10-25T07:34:40.160690Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"[(0,\n  'Gynae examination and ultrasound advised  I understand your anxiety about the issue and it is indeed serious and should\\nnot be taken lightly  Get a pelvic examination from a good gynaecologist and an ultrasound at the earliest  It can be\\nserious so much so like cancer or can just be related to postmenopausal hormone deficiency  Get the tests and then we\\ncan decide further'),\n (1,\n  'Need more information Thanks for posting your query. I am  Dr. R. K and  I am pleased to assist you. I need some more\\ninformation before  I can give my opinion. I would like to have some more information before  I can give my opinion.1.\\nWas fibroscan or a liver biopsy done?2.  Was endoscopy done?3.  Are you a diabetic? Dr. R. K.'),\n (2,\n  'Retained products of conception can be there I have gone through your question and understand the concerns. Continuous\\nand prolonged bleeding after miscarriage can be due to retained products of conception in the uterine cavity. I will\\nsuggest you to get an ultrasound done to assess if any retained products are there. If present, then you may require\\nsurgical evacuation to remove the products. You can get back to me with the ultrasound report. Hope you found the answer\\nhelpful. Dr  Deepti  Verma O B G Y N'),\n (3,\n  \"Dologel  C T or zytee ointment... I would suggest  Dologel  C T on the affected areas.  Alternatively you may try  Zytee\\nointment or even liquid glycerine.  All these are available at any nearest pharmacy / chemist stores. In addition, you\\nmay also start tab zincovit once a day for a week for added benefits. Usually apthous ulcers such as those should heal\\nwithin a week's time.  Avoid hot and spicy food. Hope this helps.\"),\n (4,\n  \"Please don't worry. There is no risk to get  H I V  infection. I have been through your question. Related your concern\\nyou shouldn't worry because you cannot get  H I V or  S T D from sharing  any kind of foods(even ice cream) with a\\nperson infected with  H I V or  S T D. I hope my answer helps you.\"),\n (5,\n  'It indicates infection I have gone through the details.  So a white discharge which is either excessive or foul smelling\\nusually indicates infection,  can be either fungal candida or bacterial vaginosis.   You will require antibiotics or\\nantifungal drugs depending upon the clinical examination.  Swelling can be due to either infection or vigorous efforts.\\nPlease visit your gynecologist for a prescription for the medicines as soon as possible.'),\n (6,\n  'Rhubarb can be the cause Dear sir,  Rhubarb can cause pinkish red stools but this should only persist for another day or\\ntwo. If it persists longer than that then you need to exclude the presence of blood in stool by a stool analysis. I hope\\nI answered your question, let me know if  I can assist you further.'),\n (7,\n  \"Please provide me more details  Healthcare Magic..  I can understand your concern.  It's very difficult for you as\\npeople around you develop allergy symptoms and this is resulting in distress.  What are your main presenting symptoms?\\nIs there any specific situation during which maximum distress occurs? What is the opinion of other people around you?\\nThanks and  Take care.  Please provide me more details in follow up.\")]"},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import pipeline, Conversation\n\n# chatbot = pipeline(\"conversational\", model=model, tokenizer=tokenizer)\n# conversation = Conversation(\"hello am 22 years old and i have type 2diabet i wanted to sign for a new gym which use electric vibes to improve heart pulses and fasten the process of loosing weight i wanted to know if it is dangerous for me knowing that 20min of this sports equals 4 h of normal one thanks\t\")\n# conversation = chatbot(conversation)\n# conversation.generated_responses[-1]","metadata":{"id":"ImT5drFr5fzw","execution":{"iopub.status.busy":"2023-10-25T07:34:40.163938Z","iopub.execute_input":"2023-10-25T07:34:40.164616Z","iopub.status.idle":"2023-10-25T07:34:40.171299Z","shell.execute_reply.started":"2023-10-25T07:34:40.164579Z","shell.execute_reply":"2023-10-25T07:34:40.170154Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"markdown","source":"You can now upload the result of the training to the Hub, just execute this instruction:","metadata":{"id":"b5rItmHxiIFs"}},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"id":"NPyW6RDjiIFs","execution":{"iopub.status.busy":"2023-10-25T07:34:40.172517Z","iopub.execute_input":"2023-10-25T07:34:40.172766Z","iopub.status.idle":"2023-10-25T07:35:16.475983Z","shell.execute_reply.started":"2023-10-25T07:34:40.172743Z","shell.execute_reply":"2023-10-25T07:35:16.474757Z"},"trusted":true},"execution_count":186,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db7fd72af0c14103b044c22735ddbb65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"163d2ee419704bc8a11f8a7d9938f7c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d33c2917f73430fb2809509fa7b9d81"}},"metadata":{}},{"execution_count":186,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/GuysTrans/bart-base-re-attention-mini-seq-512/tree/main/'"},"metadata":{}}]},{"cell_type":"markdown","source":"You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"sgugger/my-awesome-model\")\n```","metadata":{"id":"xqle58ISiIFs"}},{"cell_type":"code","source":"","metadata":{"id":"OnvVbSnOiIFt"},"execution_count":null,"outputs":[]}]}