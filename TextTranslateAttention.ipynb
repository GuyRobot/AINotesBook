{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"TextTranslateAttention.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"id":"60NlBzUY0p4v","executionInfo":{"status":"error","timestamp":1615606561014,"user_tz":-420,"elapsed":426484,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}},"outputId":"5eb9a1cf-8134-4fbd-85e2-347983b699b1"},"source":["from google.colab import drive\r\n","drive.mount(\"/content/data\")"],"execution_count":1,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-574d46744fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"FLVd7dMX0Juy","executionInfo":{"status":"aborted","timestamp":1615606560996,"user_tz":-420,"elapsed":426456,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["import os\n","import numpy as np\n","import unicodedata\n","import tensorflow as tf\n","import re\n","import io\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"75tqDrq10Ju3","executionInfo":{"status":"aborted","timestamp":1615606560999,"user_tz":-420,"elapsed":426454,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["path_data_file = \"/content/data/MyDrive/data/rus-eng/rus.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"WSaFZmpj0Ju4","executionInfo":{"status":"aborted","timestamp":1615606561001,"user_tz":-420,"elapsed":426450,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["def unicode_to_ascii(w):\n","    return ''.join(unicodedata.normalize(\"NFD\", c) for c in w if unicodedata.category(c) != 'Mn')\n","\n","word = \"Оно там?\"\n","unicode_to_ascii(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"LAnnLl-M0Ju5","executionInfo":{"status":"aborted","timestamp":1615606561002,"user_tz":-420,"elapsed":426443,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["def preprocess_word(w):\n","    w = unicode_to_ascii(w)\n","\n","    w = re.sub(r\"([.!?,])\", r\" \\1\", w)\n","    w = re.sub(r'[\" ]', \" \", w)\n","\n","    w = w.strip()\n","    return \"<start> %s <end>\" % w\n","\n","preprocess_word(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"EkQaxG1S0Ju5","executionInfo":{"status":"aborted","timestamp":1615606561004,"user_tz":-420,"elapsed":426438,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["def create_dataset(path, num_instance):\n","    lines = io.open(path, encoding=\"utf-8\").read().strip().split(\"\\n\")\n","    return zip(*[[preprocess_word(w) for w in line.split(\"\\t\")[:2]] for line in lines[:num_instance]])\n","\n","a, b = create_dataset(path_data_file, 3)\n","a, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"kjMePHWa0Ju6","executionInfo":{"status":"aborted","timestamp":1615606561005,"user_tz":-420,"elapsed":426434,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["def tokenize(texts):\n","    \"\"\"\n","\n","    :param texts: the text to tokenize\n","    :return: the tensors and tokenizer of the texts\n","    \"\"\"\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","    tokenizer.fit_on_texts(texts)\n","\n","    tensor = tokenizer.texts_to_sequences(texts)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n","\n","    return tensor, tokenizer\n","\n","\n","def load_dataset(path, num_instance):\n","    tar, inp = create_dataset(path, num_instance)\n","\n","    tar_tensor, tar_tokenizer = tokenize(tar)\n","    inp_tensor, inp_tokenizer = tokenize(inp)\n","\n","    return tar_tensor, inp_tensor, tar_tokenizer, inp_tokenizer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"T9ogCsc30Ju6","executionInfo":{"status":"aborted","timestamp":1615606561006,"user_tz":-420,"elapsed":426431,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["NUM_EXAMPLES = None\n","\n","tar_tensor, inp_tensor, tar_tokenizer, inp_tokenizer = load_dataset(path_data_file, NUM_EXAMPLES)\n","\n","max_len_tar = tar_tensor.shape[1]\n","max_len_inp = inp_tensor.shape[1]\n","\n","max_len_tar, max_len_inp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"NqZbiAjg0Ju6","executionInfo":{"status":"aborted","timestamp":1615606561007,"user_tz":-420,"elapsed":426426,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["inp_tensor_train, inp_tensor_val, tar_tensor_train, tar_tensor_val = train_test_split(inp_tensor, tar_tensor)\n","\n","len(inp_tensor_train), len(inp_tensor_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"Cyv7lBVQ0Ju7","executionInfo":{"status":"aborted","timestamp":1615606561008,"user_tz":-420,"elapsed":426422,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["def print_convert(tokenizer, tensor):\n","    for t in tensor:\n","        if t != 0:\n","            print(\"%d\\t--->\\t%s\" % (t, tokenizer.index_word[t]))\n","\n","print_convert(inp_tokenizer, inp_tensor_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"cXfjRzOt0Ju7","executionInfo":{"status":"aborted","timestamp":1615606561009,"user_tz":-420,"elapsed":426418,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["BUFFER_SIZE = 10000\n","BATCH_SIZE = 64\n","\n","dataset = tf.data.Dataset.from_tensor_slices((inp_tensor_train, tar_tensor_train))\n","dataset = dataset.shuffle(BUFFER_SIZE).cache().batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"VtpFWiDb0Ju7","executionInfo":{"status":"aborted","timestamp":1615606561009,"user_tz":-420,"elapsed":426413,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["EMBEDDING_DIM = 256\n","ENC_UNITS = 1024\n","DEC_UNITS = 1024\n","\n","vocab_inp_size = len(inp_tokenizer.word_index) + 1\n","vocab_tar_size = len(tar_tokenizer.word_index) + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"E5c4PNmP0Ju8","executionInfo":{"status":"aborted","timestamp":1615606561010,"user_tz":-420,"elapsed":426410,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["for exam_inp, exam_tar in dataset.take(1):\n","    print(exam_inp.shape, exam_tar.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"aR2RZrnZ0Ju8","executionInfo":{"status":"aborted","timestamp":1615606561011,"user_tz":-420,"elapsed":426405,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["class Encoder(tf.keras.Model):\n","    def get_config(self):\n","        pass\n","    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n","        self.batch_size = batch_size\n","        self.encoder_units = encoder_units\n","        super(Encoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n","        self.gru = tf.keras.layers.GRU(encoder_units, return_sequences=True, return_state=True,\n","                                       recurrent_initializer=\"glorot_uniform\")\n","\n","    def call(self, x, hidden, *args):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state=hidden)\n","        return output, state\n","\n","    def initialize_hidden(self):\n","        return tf.zeros((self.batch_size, self.encoder_units))\n","\n","\n","exam_inp, exam_tar = next(iter(dataset))\n","encoder_test = Encoder(vocab_inp_size, EMBEDDING_DIM, ENC_UNITS, batch_size=BATCH_SIZE)\n","\n","init_hidden = encoder_test.initialize_hidden()\n","sample_output, sample_hidden = encoder_test(exam_inp, init_hidden)\n","\n","\"Encoder Output Shape: \", sample_output.shape, \"Encoder Hidden shape: \", sample_hidden.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"FS7wiM7F0Ju8","executionInfo":{"status":"aborted","timestamp":1615606561011,"user_tz":-420,"elapsed":426400,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["class BahdanauAttention(tf.keras.Model):\n","    def __init__(self, fc_units):\n","        super(BahdanauAttention, self).__init__()\n","        self.fc1 = tf.keras.layers.Dense(fc_units)\n","        self.fc2 = tf.keras.layers.Dense(fc_units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values, *args):\n","        \"\"\"\n","\n","        :param query: The hidden from encoder (batch_size, enc_hidden)\n","        :param values: encoder output (batch_size, seq_len, enc_hidden)\n","        :param args:\n","        :return:\n","        \"\"\"\n","        # query hidden state shape == (batch_size, hidden size)\n","        # query_with_time_axis shape == (batch_size, 1, hidden size)\n","        # values shape == (batch_size, max_len, hidden size)\n","        # expand dim to broadcast addition along the time axis to calculate the score\n","        query_time_with_axis = tf.expand_dims(query, axis=1)\n","        # fc2 --> (batch_size, ..., units)\n","        # fc1 --> (batch_size, ..., units)\n","        # fc1 + fc2 --> (batch_size, ..., units)\n","        # V --> (batch_size, ..., 1) (score shape)\n","        score = self.V(tf.tanh(self.fc2(query_time_with_axis) + self.fc1(values)))\n","        # attention_weights = tf.keras.layers.Softmax(axis=1)(score)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        # Point wise element multi (not dot product)\n","        context_vector = tf.reduce_sum(attention_weights * values, axis=1)\n","        # context_vector = tf.reduce_sum(tf.matmul(attention_weights, encoder_out), axis=1)\n","\n","        return context_vector, attention_weights\n","\n","    # def call(self, inputs, training=None, mask=None):\n","    #     encoder_out, hidden = inputs\n","    #     score = self.V(tf.tanh(self.fc2(encoder_out) + self.fc1(hidden)))\n","    #     # attention_weights = tf.keras.layers.Softmax(axis=1)(score)\n","    #     attention_weights = tf.nn.softmax(score, axis=1)\n","    #     context_vector = tf.reduce_sum(attention_weights * encoder_out, axis=1)\n","    #     # context_vector = tf.reduce_sum(tf.matmul(attention_weights, encoder_out), axis=1)\n","    #\n","    #     return context_vector, attention_weights\n","\n","attention = BahdanauAttention(10)\n","attention_context, attention_weights = attention(sample_hidden, sample_output)\n","\n","\"Attention context shape: \", attention_context.shape, \"attention weights shape: \", attention_weights.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"IOXqDioW0Ju9","executionInfo":{"status":"aborted","timestamp":1615606561012,"user_tz":-420,"elapsed":426395,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units):\n","        super(Decoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n","        self.gru = tf.keras.layers.GRU(units=dec_units, return_state=True, return_sequences=True,\n","                                       recurrent_initializer=\"glorot_uniform\")\n","\n","        self.final_fc = tf.keras.layers.Dense(vocab_size)\n","\n","        self.attention = BahdanauAttention(dec_units)\n","\n","    def call(self, x, hidden, enc_output):\n","        \"\"\"\n","        :param x: inputs\n","        :param hidden: hidden from encoder\n","        :param enc_output: output of encoder\n","        :return:\n","        \"\"\"\n","\n","        # (batch_size, hidden_units), (batch_size, seq_len, hidden_units)\n","        # embedded (batch_size, ..., embedding_dim)\n","        # concat (batch_size, ..., embedding_dim + units)\n","        # gru --> out (batch_size, ..., units), state (batch_size, units)\n","        # reshape --> merge batch_size and ... --> (batch_size, units)\n","        # fc --> (batch_size, vocab_size)\n","        context_vector, context_weights = self.attention(hidden, enc_output)\n","        x = self.embedding(x)\n","        x = tf.concat([x, tf.expand_dims(context_vector, axis=1)], axis=-1)\n","\n","        out, state = self.gru(x)\n","\n","        out = tf.reshape(out, shape=(-1, out.shape[2]))\n","\n","        x = self.final_fc(out)\n","\n","        return x, state, attention_weights\n","\n","\n","decoder = Decoder(vocab_tar_size, embedding_dim=EMBEDDING_DIM, dec_units=DEC_UNITS)\n","\n","sample_decode_output, _, _ = decoder(tf.random.uniform(shape=(BATCH_SIZE, 1)), sample_hidden, sample_output)\n","\n","sample_decode_output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"RHyvfgwf0Ju-","executionInfo":{"status":"aborted","timestamp":1615606561012,"user_tz":-420,"elapsed":426390,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["optimizer = tf.keras.optimizers.Adam()\n","sparse_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_func(real, pred):\n","    # Mask for target (0 is mask, 1 is real target)\n","    mask = tf.math.logical_not(tf.equal(real, 0))\n","    loss = sparse_loss(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss.dtype)\n","    # ignore for mask loss\n","    loss *= mask\n","\n","    return tf.reduce_mean(loss)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"xYUl3V5p0Ju-","executionInfo":{"status":"aborted","timestamp":1615606561013,"user_tz":-420,"elapsed":426387,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["encoder = Encoder(vocab_size=vocab_inp_size, embedding_dim=EMBEDDING_DIM, encoder_units=ENC_UNITS, batch_size=BATCH_SIZE)\n","decoder = Decoder(vocab_size=vocab_tar_size, embedding_dim=EMBEDDING_DIM, dec_units=DEC_UNITS)\n","check_point_dir = \"/content/data/MyDrive/checkpoints/text_translate\"\n","prefix = \"ckpt\"\n","check_point_prefix = os.path.join(check_point_dir, prefix)\n","checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder)\n","checkpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"06rDLtN_0Ju-","executionInfo":{"status":"aborted","timestamp":1615606561014,"user_tz":-420,"elapsed":426382,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["@tf.function\n","def train_step(inp, tar, enc_hidden):\n","    \"\"\"\n","    Pass the input through the encoder which return encoder output and the encoder hidden state.\n","    The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n","    The decoder returns the predictions and the decoder hidden state.\n","    The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n","    Use teacher forcing to decide the next input to the decoder.\n","    Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n","    The final step is to calculate the gradients and apply it to the optimizer and backpropagate\n","    :param inp:\n","    :param tar:\n","    :param enc_hidden:\n","    :return:\n","    \"\"\"\n","    loss = 0.0\n","\n","    with tf.GradientTape() as g:\n","\n","        enc_output, enc_hidden = encoder(inp, enc_hidden)\n","        dec_input = tf.expand_dims([tar_tokenizer.word_index['<start>']] * BATCH_SIZE, axis=1)\n","        dec_hidden = enc_hidden\n","        for t in range(1, tar.shape[1]):\n","            predictions, state, _ = decoder(dec_input, dec_hidden, enc_output)\n","            loss += loss_func(tar[:, t], predictions)\n","\n","            dec_input = tf.expand_dims(tar[:, t], axis=1)\n","\n","    batch_loss = (loss / int(tar.shape[1]))\n","\n","    train_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    gradients = g.gradient(loss, train_variables)\n","\n","    optimizer.apply_gradients(zip(gradients, train_variables))\n","\n","    return batch_loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"OLf2pmJq0Ju_","executionInfo":{"status":"aborted","timestamp":1615606561014,"user_tz":-420,"elapsed":426378,"user":{"displayName":"Guys Users","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPDLTE2xq9CHYxzYfOVoXbFT7HwhIJa5bkbk16=s64","userId":"03786715776420508747"}}},"source":["EPOCHS = 20\n","steps_per_epoch = len(inp_tensor_train) // BATCH_SIZE\n","\n","for epoch in range(EPOCHS):\n","    total_loss = 0\n","    enc_hidden = encoder.initialize_hidden()\n","\n","    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n","        loss = train_step(inp, tar, enc_hidden)\n","        total_loss += loss\n","\n","        if batch % 100 == 0:\n","            print('Epoch %d Batch %d Loss %.4f' % (epoch + 1, batch, loss.numpy()))"],"execution_count":null,"outputs":[]}]}